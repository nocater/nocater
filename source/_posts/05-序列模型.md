---
title: 05-序列模型
date: 2018-10-25 11:10:43
mathjax: true
tags:
- Andrew Ng
- deep learning
categories: 深度学习[吴恩达]
description: 吴恩达深度学习课程第五课笔记
---

# 05.序列模型 (Sequence Models)
## 第一周 循环序列模型 (Recurrent Neural networks)
### 1.1 为什么选择序列模型？ (Why Sequence Models)
循环神经网络在语音识别、自然语言处理等领域有很大应用。
- 语音识别
- 音乐生成
- 情感分类
- DNA序列
- 机器翻译
- 视频行为识别
- 命名实体识别

### 1.2  数学符号  (Notation)
给出下面语句：
> Harry Potter and Herminoe Granger invented a new spell.
>  - $x^{<t\>}$ 来索引序列中的元素
>  - $y^{<t\>}$ 来表示输出数据中的序列元素
>  - $T_{x}$来表示输入序列的长度，使用 $T_{y}$来表示输出序列的长度
>  - 在以前，用$x^{(i)}$来表示第$i$个训练样本，训练样本i的序列中第$t$个元素用$x^{\left(i \right) <t\> }$这个符号来表示，$T_{x}^{(i)}$就代表第$i$个训练样本的输入序列长度。

在NLP中，建立**Vocabulary**，一般规模的商业应用来说30000到50000单词大小的词典比较常见。每个单词可以使用**one-hot**表示。

还有一点是 如果遇到不在词表中的单词时候，使用一个新的标记**Unknow word**，用**<UNK\>**来作为标记

### 1.3 循环神经网络模型 (Recurrent Neural Network Model)
使用标准的神经网络结构来构建学习序列$x$到序列$y$的映射，会遇到下面问题：
- 不同的样本的输入和输出长度是不相同的
- 并没有共享从文本的不同位置上学到的特征(如CNN在图像小块学习到的内容可以应用到整个图像)

循环神经网络模型如图：
![](http://img.nocater.com/18-9-1/54183946.jpg)
首先构造激活值$a^{<0>}$，这通常是零向量。$x^{<1>}$经过神经网络得到预测值$\hat{y}^{<1>}$。而当第二个单词$x^{<2>}$输入时候，来自时间步(Time-step)1的信息也会输入。这样一直到最后一个时间步，输入$x^{<T_{x}>}$，然后输出${\hat{y}}^{< T_{y} >}$。至少在这个例子中$T_{x} =T_{y}$，同时如果$T_{x}$和$T_{y}$不相同，这个结构会需要作出一些改变。可以发现，RNN是使用了当前输入的以前信息来综合预测。

> He said, "Teddy Roosevelt was a great President."
> He said, "Teddy bears are on sale!"

这个例子中，仅仅使用前置单词信息，Teddy并不能被很好的识别为是否为人名，因此出现了**BRNN**，即双向RNN
公式化： 
$a^{<1\>} = g_{1}(W_{{aa}}a^{< 0 \>} + W_{{ax}}x^{< 1 \>} + b_{a})$
$\hat y^{< 1 \>} = g_{2}(W_{{ya}}a^{< 1 \>} + b_{y})$
经过整理后：
$a^{<t\>} =g(W_{a}\left\lbrack a^{< t-1 \>},x^{t} \right\rbrack +b_{a})$
$\hat y^{<t\>} = g(W_{ya}a^{<t\>} +b_{y})$

### 1.4 通过时间的反向传播 (Backpropagation through time)
讲的不够清楚，仅提示是前向传播的反过程。
损失函数：
$L^{<t\>}( \hat y^{<t\>},y^{<t\>}) = - y^{<t\>}\log\hat y^{<t\>}-( 1-\hat y^{<t\>})log(1-\hat y^{<t\>})$
将每一个时间步的的损失函数加起来，得到整体损失函数。
$L(\hat y,y) = \ \sum_{t = 1}^{T_{x}}{L^{< t \>}(\hat y^{< t \>},y^{< t \>})}$

![enter image description here](http://img.nocater.com/18-9-1/12350616.jpg)
详细需要细看反向传播实现

### 1.5 不同类型的循环神经网络 (Different types of RNNs)

![@RNN的类型](http://img.nocater.com/18-9-1/14599121.jpg)
> $T_{x}=T_{y}$不适用的情况是存在的。
> many-to-one的例子如电影情感分类
> one-to-many的例子如音乐生成
> many-to-many的例子如机器翻译，输入与输出的序列是不等长的。

### 1.6 语言模型和序列生成 (Language model and sequence generation)
在自然语言处理中，构建语言模型是最基础的也是最重要的工作之一，并且能用RNN很好地实现。
语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少。语言模型做的最基本工作就是输入一个句子，准确地说是一个文本序列，$y^{<1\>}​$，$y^{<2\>}​$一直到$y^{<T_{y}>}​$。对于语言模型来说，用$y​$来表示这些序列比用$x​$来表示要更好，然后语言模型会估计某个句子序列中各个单词出现的可能性。
如何建立一个语言模型呢？首先需要语料库**corpus**，然后将语句进行*标记化*，如使用one-hot形式。一般句尾会增加一个额外标记**EOS**，表示句尾。再添加一个**UNK**标记表示不在字典中的字。
![](http://img.nocater.com/18-9-1/79718075.jpg)
> $x^{<1\>}$设为0向量，这一步其实就是通过一个softmax层来预测字典中的任意单词会是第一个词的概率，
> $y^{<1\>} = x^{<2\>}$（上图编号2所示）
> 损失函数： $L\left( \hat y^{<t\>},y^{<t\>}>\right) = - \sum_{i}^{}{y_{i}^{<t\>}\log\hat y_{i}^{<t\>}}$ 可记为 $L = \sum_{t}^{}{L^{< t \>}\left( \hat y^{<t\>},y^{<t\>} \right)}$

### 1.7 对新序列采样 (Sampling novel sequences)
**论文：**
（Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling[J]. Eprint Arxiv, 2014.

Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches[J]. Computer Science, 2014.）

在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。

![](http://img.nocater.com/18-9-2/43977616.jpg)
首先输入$x^{<1\>} =0$，$a^{<0\>} =0$，然后第一个时间步会通过softmax概率采样，得到第一个单词$\hat y^{<1>}$，然后将其输入到第二个时间步。一直采样得到**EOS**标识。有时候采样到**UNK**，可以忽略或重采样。

实际中可能用到基于字符的RNN结构。优点是不会遇到**UNK**标识。但缺点序列太多太长，计算成本高。
![](http://img.nocater.com/18-9-2/35767802.jpg)

### 1.8 循环神经网络的梯度消失 (Vanishing gradients with RNNs)
> “The cat, which already ate ……, was full.”
> “The cats, which ate ……, were full.”

这些句子中，cat/cats和was/were有长期依赖性质，但中间的词可以无限长。神经网络可能会遇到梯度消失，所以RNN很难学习这种依赖。梯度爆炸虽然也会出现在RNN中，但梯度消失更难处理。以后课程会详细讲解。

### 1.9 GRU单元 (Gated Recurrent Unit(GRU))
![@RNN unit](http://img.nocater.com/18-9-2/99489443.jpg)
$a^{< t \>} = g(W_{a}\left\lbrack a^{< t - 1 \>},x^{< t \>}\right\rbrack +b_{a})$，通过上图可以发现，首先将上一个时间步的$a^{<t-1\>}$输入，然后再输入$x^{<t\>}$，并在一起后经过激活函数*tanh*计算得到$a^{<t\>}$，然后$a^{<t\>}$经过**softmax**单元可以产生输出$y^{<t\>}$。这是最简单的RNN单元。其规则图像如下：
![@RNN unit](http://img.nocater.com/18-9-2/59703678.jpg)

> *The cat, which already ate..., was full.*

许多GRU的想法都来分别自于Yu Young Chang, Kagawa，Gaza Hera, Chang Hung Chu和 Jose Banjo的两篇论文。

![GRU(simplified)](http://img.nocater.com/18-9-2/11203795.jpg)
首先，**GRU**添加了新的变量$c$(cell)，代表记忆细胞(图中编号1)。**GRU**实际上输出了激活值$a^{<t\>}​$，$c^{<t\>} = a^{<t\>}​$（图中编号2所示）。仍使用$c^{<t\>} $是因为**LSTMs**会是两个不同值，**GRU**是一样的。
然后经过计算可以得到候选值${\tilde{c}}^{<t\>}$，使用*tanh*函数计算。重点来了，**GRU**中的**门**，用$\Gamma_{u}$表示：其计算公式为：
$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{},x^{} \right\rbrack +b_{u})$
$\Gamma_{u}$的值在大多数情况家非常接近0或1。$\Gamma_{u}$的作用就是决定什么时候你会更新这个值。更新公式为：
$c^{<t\>} = \Gamma_{u}\*{\tilde{c}}^{<t\>} +\left( 1- \Gamma_{u} \right)\*c^{<t\>}$  (* 为元素相乘)
当$\Gamma_{u}$=1时候，就会更新，为0时候会保持原来的值。

![](http://img.nocater.com/18-9-2/69268923.jpg)
以例句为例子，当到单词*cat*时候，**门**值更新，后面一直不需要更新，到了*was*单词的时候更新。这时候**GRU**记住了*cat*是单数。
同样的，$\Gamma_{u}$在多个步骤下都=0，即经过几层后，$c^{<t\>}$几乎就等于$c^{<t\>}$，这就是**GRU**缓解梯度消失问题的关键。

细节：$c^{<t-1\>}$是一个向量，比如100维，那么$c^{<t\>}$也是100维。${\tilde{c}}^{<t\>}$也是相同的维度。$\Gamma_{u}$也就100维的向量，里面的值几乎都是0或者1，就是说这100维的记忆细胞$c^{}$（$c^{}=a^{}$上图编号1所示）就是你要更新的比特。

当然在实际应用中$\Gamma_{u}$不会真的等于0或者1，有时候它是0到1的一个中间值（上图编号5所示），但是这对于直观思考是很方便的，就把它当成确切的0，完全确切的0或者就是确切的1。元素对应的乘积做的就是告诉GRU单元哪个记忆细胞的向量维度在每个时间步要做更新，所以你可以选择保存一些比特不变，而去更新其他的比特。比如说你可能需要一个比特来记忆猫是单数还是复数，其他比特来理解你正在谈论食物，因为你在谈论吃饭或者食物，然后你稍后可能就会谈论“The cat was full.”，你可以每个时间点只改变一些比特。

**完整的GRU**：
![@Full GRU](http://img.nocater.com/18-9-2/13563868.jpg)
完整的添加了一个新的门$\Gamma_{r}$，$r$代表相关性（relevance）。$\Gamma_{r}$门告诉你计算出的下一个$c^{}$的候选值${\tilde{c}}^{<t\>}$跟$c^{<t\>}$有多大的相关性。计算这个门$\Gamma_{r}$需要参数，正如你看到的这个，一个新的参数矩阵$W_{r}$，$\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{<t\>},x^{<t\>} \right\rbrack + b_{r})$。
$\Gamma_{r}$是研究生试验后选择的，非常健硕和实用。

GRU，即门控循环单元，这是RNN的其中之一。这个结构可以更好捕捉非常长范围的依赖，让RNN更加有效。然后我简单提一下其他常用的神经网络，比较经典的是这个叫做LSTM，即长短时记忆网络，我们在下节视频中讲解。

### 1.10 长短期记忆(LSTM (long short term memory) unit)
论文：Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780. (比较难)

![@GRU](http://img.nocater.com/18-9-2/32350232.jpg)
> **GRU**中，$a^{<t\>} = x^{<t\>}$，并且有两个门：
> - 更新门$\Gamma_{u}$（the update gate）
> - 相关门$\Gamma_{r}$（the relevance gate）

![@LSTM 右图为示意图](http://img.nocater.com/18-9-2/72695871.jpg)
> **LSTM**中，有三个门：更新(u)、遗忘(f)、输出(o)
> **LSTM**中不再有$a^{<t\>} = c^{<t\>}$的情况
> 更新与遗忘不再是二选一操作

![](http://img.nocater.com/18-9-2/98900858.jpg)
> 由红色10号线可以看出，只要设置了正确的遗忘门和更新门，LSTM是相当容易把$c^{<0>}$的值（上图编号11所示）一直往下传递到右边，比如$c^{<3\>} = c^{<0>}$（上图编号12所示）。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。

“**偷窥孔连接**”其实意思就是门值不仅取决于$a^{}$和$x^{}$，也取决于上一个记忆细胞的值（$c^{}$），然后“偷窥孔连接”就可以结合这三个门（$\Gamma_{u}$、$\Gamma_{f}$、$\Gamma_{o}$）来计算了。如果你读过论文，见人讨论“偷窥孔连接”，那就是在说$c^{<t-1\>}$也能影响门值。

> LSTM反向传播计算：
> 门求偏导：
> $d \Gamma_o^{\langle t \rangle} = da_{next}\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}(1-\Gamma_o^{\langle t \rangle})\tag{1}$
>
> $d\tilde c^{\langle t \rangle} = dc_{next}\*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2)\*i_t\*da_{next}\*\tilde c^{\langle t \rangle}\*(1-\tanh(\tilde c)^2) \tag{2}​$
>
> $d\Gamma_u^{\langle t \rangle} = dc_{next}\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) \* \tilde c^{\langle t \rangle} \* da_{next}\Gamma_u^{\langle t \rangle}\*(1-\Gamma_u^{\langle t \rangle})\tag{3}$
>
> $$d\Gamma_f^{\langle t \rangle} = dc_{next}\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) \* c_{prev} \* da_{next}\Gamma_f^{\langle t \rangle}\*(1-\Gamma_f^{\langle t \rangle})\tag{4}$$
>
> 参数求偏导 ：
>
> $ dW_f = d\Gamma_f^{\langle t \rangle} \* \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{5} $ $ dW_u = d\Gamma_u^{\langle t \rangle} \* \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{6} $ $ dW_c = d\tilde c^{\langle t \rangle} \* \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{7} $ $ dW_o = d\Gamma_o^{\langle t \rangle} \* \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{8}$
>
> 为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。
>
> 最后，计算隐藏状态、记忆状态和输入的偏导数：
>
> $ da_{prev} = W_f^T\*d\Gamma_f^{\langle t \rangle} + W_u^T \* d\Gamma_u^{\langle t \rangle}+ W_c^T \* d\tilde c^{\langle t \rangle} + W_o^T \* d\Gamma_o^{\langle t \rangle} \tag{9}​$
>
> $ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} \* (1- \tanh(c_{next})^2)\*\Gamma_f^{\langle t \rangle}da_{next} \tag{10}$ 
>
> $ dx^{\langle t \rangle} = W_f^Td\Gamma_f^{\langle t \rangle} + W_u^T \* d\Gamma_u^{\langle t \rangle}+ W_c^T \* d\tilde c_t + W_o^T \* d\Gamma_o^{\langle t \rangle}\tag{11} $


**LSTM**和**GRU**的选择没有准则，**GRU**模型更加简单，容易创建更大网络，只有两个门，计算也快。**LSTM**更强大和灵活，拥有三个门。

### 1.11 双向循环神经网络 (Bidirectional RNN)
![](http://img.nocater.com/18-9-3/81050603.jpg)
在这个示例中，仅使用单向RNN不能很好的识别*Teddy*是否为人名。
以四个单词为例子，使用${\overrightarrow{a}}^{<1>}$，${\overrightarrow{a}}^{<2>}$，${\overrightarrow{a}}^{<3>}$还有${\overrightarrow{a}}^{<4>}$，表示前向循环单元，然后添加了${\overleftarrow{a}}^{<1>}$，左箭头代表反向连接，${\overleftarrow{a}}^{<2>}$反向连接，${\overleftarrow{a}}^{<3>}$反向连接，${\overleftarrow{a}}^{<4>}$反向连接，这里的左箭头代表反向连接。
![@BRNN示意图](http://img.nocater.com/18-9-3/94017880.jpg)
最后，这各网络构成了无环图。举个例子，以时间步3来说，$x^{<1\>}$经过前向的${\overrightarrow{a}}^{<1\>}$到前向的${\overrightarrow{a}}^{<2\>}$到前向的${\overrightarrow{a}}^{<3\>}$再到$\hat y^{<3\>}$。同理$x^{<2\>}$可以到达。而对于$x^{<4\>}$，可以经过反向的${\overleftarrow{a}}^{<4\>}$，到反向的${\overleftarrow{a}}^{<3\>}$再到$\hat y^{<3\>}$。所以时间3，不仅使用了过去的信息，还有现在的信息。

这就是双向循环神经网络，并且这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，对于大量有自然语言处理问题的文本，有LSTM单元的双向RNN模型是用的最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，一个有LSTM单元的双向RNN模型，有前向和反向过程是一个不错的首选。

如果你总是可以获取整个句子，这个标准的双向RNN算法实际上很高效。

### 1.12 深层循环神经网络 (Deep RNNs)
Deep RNN有两种类型：**网状**、**串状**

**网状**：
![@网状Deep RNN](http://img.nocater.com/18-9-3/20547164.jpg)
以$a^{\lbrack 2\rbrack <3>}$，上图编号5所示为例子，激活值$a^{\lbrack 2\rbrack <3>}$有两个输入，一个是从下面过来的输入（上图编号6所示），还有一个是从左边过来的输入（上图编号7所示），$a^{\lbrack 2\rbrack < 3 \>} = g(W_{a}^{\left\lbrack 2 \right\rbrack}\left\lbrack a^{\left\lbrack 2 \right\rbrack < 2 >},a^{\left\lbrack 1 \right\rbrack < 3 \>} \right\rbrack + b_{a}^{\left\lbrack 2 \right\rbrack})$，这就是这个激活值的计算方法。参数$W_{a}^{\left\lbrack 2 \right\rbrack}$和$b_{a}^{\left\lbrack 2 \right\rbrack}$在这一层的计算里都一样，相对应地第一层也有自己的参数$W_{a}^{\left\lbrack 1 \right\rbrack}$和$b_{a}^{\left\lbrack 1 \right\rbrack}$。

**串状**：
![](http://img.nocater.com/18-9-3/94935397.jpg)
没有横向的连接，只有纵向的连接。可以使用简单的RNN单元，也可以使用**GRU**，**LSTM**，甚至双向RNN。

## 第二周 自然语言处理与词嵌入 (Natural Language Processing and Word Embeddings)

### 2.1 词汇表征 (Word Representation)
使用**one-hot**形式表示词汇，每个单词都是独立没有联系的(任何两个单词的内积为零)。因此可以使用*词嵌入*。这种高维表示，词向量间可以通过计算内积来表示相似度。为了可视化，可以使用**t-SNE**算法将词向量高维空间映射到二维空间来观察。

### 2.2 使用词嵌入 (Using Word Embeddings)
如何将词嵌入表示应用到NLP中？这是如何用词嵌入做迁移学习的步骤。

第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。

第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量。尽管one-hot向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。

第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。

词嵌入技术广泛应用于NLP中的命名实体识别，文本摘要，指代消除等，在语言模型和机器翻译领域使用的少一些。

人脸识别中的术语编码（encoding）和嵌入（embedding）可以互换，差别不是因为术语不一样，这个差别就是，人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，而像一些没有出现过的单词我们就记为未知单词。

### 2.3 词嵌入的特性 (Properties of Word Embeddings) 
**论文**：Mikolov T, Yih W T, Zweig G. Linguistic regularities in continuous space word representations[J]. In HLT-NAACL, 2013.

词嵌入能够实现**类比推理**，准确率在30%~75%左右。
![](http://img.nocater.com/18-9-8/26063925.jpg)
对于Man-Woman这种关系，King对应的是那个单词？这时候我们使用Man的词嵌入向量$e_{man}$减去Woman的词嵌入向量$e_{woman}$，在第一位度表示Gender项可以得到大概-2值，其余项为0。同样，对于King来说，Queen与King的差值向量与之相似。

在向量$u$和$v$之间定义相似度:$\text{sim}\left( u,v \right) = \frac{u^{T}v}{\left| \left| u \right| \right|{2}\left| \left| v \right| \right|{2}}$
距离用平方距离或者欧氏距离来表示:$\left| \left| u - v \right|\right|^{2}$
测量两个词的两个嵌入向量之间的相似程度。 给定两个向量$u$和$v$，余弦相似度定义如下： ${CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$ 其中 $u.v$ 是两个向量的点积（或内积），$||u||_2$是向量$u$的范数（或长度），并且 $\theta$ 是向量$u$和$v$之间的角度。角度越小，两个向量越相似。 

### 2.4 嵌入矩阵 (Embedding Matrix)
应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵。

例如，假如词汇表长度为1000，词嵌入维度为300。则所有词汇组成的矩阵为300*1000。假如6527个单词代表**orange**，使用符号$O_{6527}$来表示这个**one-hot**向量，这个向量除了6527位置为1，其余全为0。假设嵌入矩阵为$E$，它的第6527列为$e_{6527}$表示单词**orange**的嵌入向量。这时候把矩阵$E$和**one-hot**向量相乘，则可以得到$e_{6527}$。
在实际中，通常使用一个专门的函数来单独查找矩阵$E$的某列，而不是使用通常的矩阵乘法来做。

###2.5 学习词嵌入 (Learning Word Embeddings)
假如构建一个语言模型，并且使用神经网络来实现。如预测“**I wang a glass of range —**”,然后预测下一个词。对于每一个单词使用嵌入向量，然后输入到神经网络中，则可以很好实现词嵌入。如每个单词是300维度，使用前四个单词则是1200的维度输入到神经网络，再经过softmax预测下一个单词。同时你还可以使用6个单词，所以是6x300的维度。这个固定的窗口是算法的超参数。这中算法能很好的学习词嵌入。

其它方式的更加简单的方法是使用不用类型的上下文，
使用 目标词的前后各两个词作为上下文，如果目标不是学习语言模型本身的话，可以选择其它上下文
也可以使用上下文一个单词，这就是一种**Skip-Gram**模型的思想，也能得到很好的效果。

### 2.6 Word2Vec
论文：Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.

在**Skip-gram**模型中，上下文不一定总是目标最近的$n$个单词，而是随机选择。比如在上下文词前后5个词内或10个词内选择目标词。构建监督学习问题，给定上下文词，预测正负10个词距内随机选择的目标词。我们的目标不是解决监督学习问题本身，而是想要使用它来学习一个好的词嵌入模型。
对于一个单词，先构建**one-hont**向量，然后使用嵌入矩阵得到词向量，$e_{c}=EO_{c}$。然后将向量$e_{c}$喂入一个softmax。
$Softmax:p\left( t \middle| c \right) = \frac{e^{\theta_{t}^{T}e_{c}}}{\sum_{j = 1}^{10,000}e^{\theta_{j}^{T}e_{c}}}$
$\theta_{t}$是一个与输出$t$有关的参数，即某个词$t$和标签相符的概率是多少。
损失函数：
$L\left( \hat y,y \right) = - \sum_{i = 1}^{10,000}{y_{i}\log \hat y_{i}}$

对于词汇过大，softmax计算过慢有两种解决办法，1.分层softmax分类器2.负采样。

模型中对于上下文单词的采样并不是随机抽取，否则 the、of、a、and等词出现相当频繁，而是采用不同的分级平衡。
论文中提到了两种模型：**Skip-Gram**和**CBOW**，**Skip-Gram**使用更多些。

### 2.7 负采样 (Negative Sampling)
论文： Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119.

不使用分层softmax，而是使用负采样方式。即构建新的监督学习，给定一堆单词，比如**orange**和**juice**，预测是否为一对上下文词-目标词(context-target)。在上个例子中，**orange**和**juice**就是一个正样本。再选择k个负样本，即使抽取的词再目标词的范围内也无所谓。k的取值：数据集越大k越小，选择5-20比较好。
![](http://img.nocater.com/18-9-12/71331665.jpg)

当输入词**orange**，即词6527，得到嵌入向量$e_{6527}$，就得到了10000个可能的逻辑回归分类任务。但我们不在使用上面提到的10000类分类器，而是选择训练其中的$k+1$个，这样计算成本更低。	

还有一个细节是负样本的采样过程，既不选择随机抽取，也不通过词频才采样。而是选择下面方式：
$P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}}}{\sum_{j = 1}^{10,000}{f\left( w_{j} \right)^{\frac{3}{4}}}}$

### 2.8 GloVe 词向量 (GloVe Word Vectors)
**NLP**领域的**Glove**算法，使用不多，但研究的人页不少。
GloVe代表用词表示的全局变量（global vectors for word representation），如果使用$X_{\\{ij\\}}$来表示单词$i$在单词$j$的上下文出现的次数，那么这里的$i$和$j$功能是和$t$ $c$是一样的。如果将上下文定义为左右范围的话，很明显会得到对称关系，但如果规定是单侧方向，就不对称了。不过对于GloVe算法，我们可以定义上下文和目标词为任意两个位置相近的单词，假设是左右各10词的距离，那么$X_{\\{ij\\}}$就是一个能够获取单词$i$和单词$j$出现位置相近时或是彼此接近的频率的计数器。

GloVe模型做的就是进行优化，我们将他们之间的差距进行最小化处理：
$\text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}{f\left( X_{\\{ij\\}} \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - logX_{\\{ij\\}} \right)^{2}}}$

最后，一件有关这个算法有趣的事是$\theta$和$e$现在是完全对称的，所以那里的$\theta_{i}$和$e_{j}$就是对称的。如果你只看数学式的话，他们（$\theta_{i}$和$e_{j}$）的功能其实很相近，你可以将它们颠倒或者将它们进行排序，实际上他们都输出了最佳结果。因此一种训练算法的方法是一致地初始化$\theta$和$e$，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值，所以，给定一个词$w$，你就会有$e_{w}^{(final)}= \frac{e_{w} +\theta_{w}}{2}$。因为$\theta$和$e$在这个特定的公式里是对称的，而不像之前视频里我们了解的模型，$\theta$和$e$功能不一样，因此也不能像那样取平均。

**最后，词嵌入维度的可解释性很差**


### 2.9 情感分类 (Sentiment Classification)
情感分类是NLP的重要模块之一，最大的挑战可能是标记的训练集可能没有那么多。
比如对一个餐馆的评价，输入$x$是一段文本，输出$y$是要预测的相应情感，分为五颗星。对于情感分类任务，训练集从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集时。

算法一：
将语句用**one-hot**表示，并转换成词向量，然后将语句的词向量进行求和或求平均，最后将这些特征输入到一个softmax分类器，谭厚输出$\hat{y}$。根据输出结果来进行星级的确定。
但是这个算法有个一缺点："Completely lacking in good taste, good service, and good ambiance."，但是good这个词出现了很多次，有3个good，如果你用的算法跟这个一样，忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，你最后的特征向量会有很多good的表示，你的分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价。

算法二：
![@RNN 情感分类](http://img.nocater.com/18-9-23/41291199.jpg)
我们首先取这条评论，"Completely lacking in good taste, good service, and good ambiance."。找到每一个**one-hot**，向量，乘以词嵌入矩阵$E$，得到嵌入表达$e$，然后输入到RNN。RNN能够考虑词序问题。
这样的算法，会得到一个很合适的情感分类算法。如果词嵌入是一个在更大数据集训练的，这样效果会更好。

### 2.10 词嵌入除偏 (Debiasing Word Embeddings)
**论文**：Man is to computer programmer as woman is to homemaker?Debiasing word embeddings(Bolukbasi et.al, 2016)
这里的**bias**指的是学习到的知识上的偏见，比如性别歧视，种族歧视等。比如下面这种情况：
Man : Woman as King : Queen
Man : Computer_programmer as Woman : Homemaker ***Wrong!***
Father : Doctor as Mother : Nurse ***Wrong!***

以性别歧视为例子，首先看如何辨别与这个偏见相似的趋势：
1. 
![](http://img.nocater.com/18-9-25/33476036.jpg)对于性别歧视，首先得到$e_{\text{he}}-e_{\text{she}}$，然后将$e_{\text{male}}-e_{\text{female}}$，然后将这些值取平均（上图编号2所示），将这些差简单地求平均。这个趋势（上图编号3所示）看起来就是性别趋势或说是偏见趋势，然后这个趋势（上图编号4所示）与我们想要尝试处理的特定偏见并不相关，因此这就是个无偏见趋势。然后偏见趋势看成1D子空间，无偏见趋势就会使299D子空间。当然可以使用更复杂的算法**SUV(奇异值分解)**来描述偏见趋势的维度大于1.
2. 中和步骤。
![](http://img.nocater.com/18-9-25/33476036.jpg)像grandmother、grandfather、girl、boy、she、he，他们的定义中本就含有性别的内容，不过也有一些词像doctor和babysitter我们想使之在性别方面是中立的。像doctor和babysitter这种单词我们就可以将它们在这个轴（上图编号1所示）上进行处理，来减少或是消除他们的性别歧视趋势的成分，也就是说减少他们在这个水平方向上的距离（上图编号2方框内所示的投影），所以这就是第二个中和步。
3. 均衡步。![](http://img.nocater.com/18-9-25/68280416.jpg)我们想要确保的是像grandmother和grandfather这样的词都能够有一致的相似度，或者说是相等的距离，和babysitter或是doctor这样性别中立的词一样。

参考资料：针对性别特定词汇的均衡算法

## 第三周 序列模型和注意力机制
### 3.1 基础模型 (Basic Models)
在这一周，你将会学习**seq2seq**（sequence to sequence）模型，从机器翻译到语音识别，它们都能起到很大的作用，从最基本的模型开始。之后你还会学习**集束搜索**（Beam search）和**注意力模型**（Attention Model），一直到最后的音频模型，比如语音。

如何训练一个网络来输入序列$x$和输出序列$y$呢？
![@机器翻译(法英)](http://img.nocater.com/18-9-27/47811277.jpg)
首先建立一个编码网络(encoder network)，是可以是GRU或LSTM组成的RNN网络。每次只向网络输入一个法语单词，序列接收完成后，得到一个表示序列X的向量。然后建立解码网络(decoder network)，用来输出翻译后的英文单词，一直输出到序列结尾或句子结尾标记。类似语言模型合成文本。

同样的可以应用到图片描述问题，将一张图片输入到AlexNet网络，去掉最后的softmax层，接一个RNN网络输出文本序列。

这两种模型都非常有效，细节之后的课程继续讲。

### 3.2 选择最可能的句子(Picking the most likely sentence)
**seq2seq**机器翻译模型和之前的语言模型有很多相似之处，但也有重要的区别。

可以把机器翻译模型想象成建立一个**条件语言模型**。语言模型是估计一个句子的可能性，可以理解$x^{<1>}$是一个全为0的向量，然后$x^{<2>}$、$x^{<3>}$等都等于之前所生成的输出，这就是所说的语言模型。

![](http://img.nocater.com/18-9-27/99670289.jpg)
机器翻译模型中，用绿色标识**encoder**，紫色标识**decoder**。与语言模型不同在于，**encoder**网络会计算一系列向量，并输入到**decoder**中。**decoder**并不是以零向量开始，所以把它叫做**条件语言模型(conditional language model)**。换句话说，你将估计一个英文翻译的概率，所以它是一个语言模型。

![](http://img.nocater.com/18-9-27/85697941.jpg)
现在，假如通过模型来讲法语翻译成英文,通过模型会得到各中英文翻译的可能性。显然，你不想得到随机的输出，所以需要一个算法来找到合适的输出$y$，使得条件概率最大。常用的算法便是**束搜索(Beam Search)**。

这里需要主义的是和贪心算法作比较，当使用贪心算法的时候，是每次选择概率最大的单词，以下面翻译句子为例：
Jane is visiting Africa in September.
Jane is gong to be visiting Africa in September.
英语中 *going*更常见， 但第二个句子并没有第一个句子翻译的好。所以$p(y|x)$模型并不是最好的选择。应该以句子为整体来考虑。

另外一点，假设你的字典有10000个单词，翻译的句子假如有10个单词长度。那么组合10,000的10次方这么多，数量非常大。所以近似的算法是尽力去挑选出句子$y$使得条件概率最大化，不一定成功，但已经足够了。

最后总结一下，在本视频中，你看到了机器翻译是如何用来解决条件语言模型问题的，这个模型和之前的语言模型一个主要的区别就是，相比之前的模型随机地生成句子，在该模型中你要找到最有可能的英语句子，最可能的英语翻译，但是可能的句子组合数量过于巨大，无法一一列举，所以我们需要一种合适的搜索算法，让我们在下节课中学习集束搜索。

### 3.3  集束搜索 (Beam Search)
语音识别，机器翻译等，我们想要的都是最好的而不是随机选择一个结果。集束搜索就是为了解决这个问题。

“Jane visite l'Afrique en Septembre.”（法语句子），我们希望翻译成英语，"Jane is visiting Africa in September".（英语句子）。假如词汇表有10000个单词，然后利用编码网络(绿色)和解码网络(蓝色)来评估di第一个单词的概率。即给定$x$，第一个输出$y$的概率是多少$P(y^{<1>}|x)$
![](http://img.nocater.com/18-9-27/98430334.jpg)
贪婪算法只选取最可能的一个，而集束算法有一个参数**B**，叫做集束宽(**beam width**)。以3为例，选取最大概率的三个单词并记录概率，假如是*in, jane, september*。

第二步:针对每一个单词，继续求解词汇中每个单词出现在第二个位置的概率。
![](http://img.nocater.com/18-9-27/89245036.jpg)
以*in*为例，绿色编码，蓝色解码。同时$y^{<1>}$设为单词*in*，来计算$y^{<2>}$。我们在第二步更关心的出找到最可能的第一个单词和第二个单词对，而不是仅仅第二个单词最大的概率。编号8表示第一个单词的概率，乘以第二个单词的概率(编号9，可以从网络编号10中得到)，最后就得到了第一个和第二个单词对的概率(编号7)。
![](http://img.nocater.com/18-9-27/40125209.jpg)
接下来对第二个单词*jane*继续求解与字典中各单词组成单词对的概率。同样对第三个三次*september*进行相同的操作。
![](http://img.nocater.com/18-9-27/58152241.jpg)
针对第二个单词有10000中不同的选择，集束宽度为3，所以最终会有30000个可能的结果，然后对这30000个结果进行评估，选取前三个结果。这时候可能选取的是*in september、jane is、jane visits*，这时候可以看到，第一个单词的选择只有两种可能，去掉了*september* 。
![](http://img.nocater.com/18-9-27/58292062.jpg)
接下的步骤与第二步一样，重复这个过程最终得到*Jane visits africa in september*这个句子，终止在句尾符号(编号8)。集束算法每次只考虑3个可能的结果，当集束宽为1时候就成了贪婪算法了。集束算法会比贪婪算法搜索更好的输出结果。

### 3.4 改进集束搜索(Refinements to Beam Search)
![](http://img.nocater.com/18-9-27/45318120.jpg)
前面讲到束搜索就是最大化这个概率，这个乘积就是$P(y^{< 1 >}\ldots y^{< T_{y}}|X)$，可以表示成:$P(y^{<1>}|X)$ $P(y^{< 2 >}|X,y^{< 1 >})$ $P(y^{< 3 >}|X,y^{< 1 >},y^{< 2>})$…$P(y^{< T_{y} >}|X,y^{<1 >},y^{<2 >}\ldots y^{< T_{y} - 1 >})$
这些概率都小于1，很多乘在一起就可能造成数值下溢(**numerical underflow**)，因此，我们在实践中会取$log$值，这样数值会更加稳定，所以在实际工作中，我们记录的是概率的对数和，而不是概率的乘积。

还有一点就是，哪怕采取$log$操作，每一项都是小于1的，因此模型会对短句有偏好，因为越长相乘目标函数的值越小。因此要选择长度归一化：![](http://img.nocater.com/18-9-27/78369669.jpg)
$\alpha$是一个超参数，为0时候完全没有归一化，为1的时候完全归一化，在实践中通常选取0.7，虽然没有理论支持，但效果还不错，你自己以后的项目可以来调节这一参数。有时候也叫这个目标函数为**归一化的对数似然目标函数（a normalized log likelihood objective）**。

其次，超参数集束宽**B**也需要调节，同样，**B**越大，计算成本越大，而且收益由大到小。对于不同的应用和特定的领域来说，10-3000都是可能的。

### 3.5 集束搜索的误差分析 (Error analysis in beam search)
集束搜索是一种近似搜索算法(an approximate algorithm )，也叫启发式搜索(a heuristic search algorithm)。将误差分析和集束搜索结合起来。
法语：*Jane visite l'Afrique en septembre*
人工翻译：*Jane visits Africa in September* 记为$(y^\*)$ 
算法翻译：*Jane visited Africa last September* 记为$(\hat{y})$
这个例子中，算法翻译很差，偏离了含义。
你的模型有两个主要部分，一个是神经网络或序列到序列模型(seq2seq)，它实际是编码器和解码器。另一个是技术算法，以某个集束宽**B**运行。如何判断是那个部分出错呢？
**RNN**是计算$P(y|x)$。
![](http://img.nocater.com/18-9-28/91934392.jpg)
我们来计算$P(y^*|x)$和$P(\hat y|x)$，然后比较这两个哪个更大，所以就会有两种情况。
1. $P(y^|x)$ 大于$P(\hat y|x)$。这说明集束算法并没有搜索到正确的结果，所以是集束算法出错了。
2. $P(y^|x)$ 大于$P(\hat y|x)$。是RNN出错了，这里没有涉及长度归一化(length normalization)，如果使用需要比较的是归一化的最优目标函数。

![](http://img.nocater.com/18-9-28/72126631.jpg)
所以误差分析过程是，首先遍历开发集，然后找出错误并标记各个错误是属于RNN还是集束搜索问题。最后根据分析结果对模型算法进行改进。

### 3.6 Bleu得分(选修)
论文(看)：**Bleu:A method for automatic evaluation of machine translation. Papineni et.al., 2002**

机器翻译(machine translation)的一个难题是一个法语句子可以有多种英文翻译，并且同样好。这样改如何评价一个机器翻译系统？常用的解决办法就是**BLEU**得分。	
**BLEU**代表**bilingual evaluation understudy (双语评估替补)**。它背后的理念是观察机器生成的翻译，然后看生成的词是否出现再至少一个人工翻译参考中。

> 例子：
法语：Le hcat est sur le tapis.
参考(Reference)1: The cat is on the mat.
参考(Reference)2: There is a cat on the mat.
机器翻译(MT): the the the the the the the.

这时候，精度(**Precision**)为$\frac{7}{7}$，因为MT的输出共有7个单词(分母)，其中7个单词出现在两个参考(Reference)中。而修改后的精度(**Modified precision**)为$\frac{2}{7}$，对于单词*the*,在两个参考(Reference)中出现的次数分别为2，1。而MT中出现次数为7，超过了最大参考中出现，因此进行截断。所以**改良后的精确度评估（the modified precision measure）**得分为$\frac{2}{7}$。

目前为止，我们关注的都是单独的单词。我们定义以下二元词组(**bigrams**)，是指相近的两个单词。同样，可能会考虑一元词组(unigram)，三元词组(trigrams)等。我们首先列出所有的二元词组，然后统计次数，以及截断统计次数可以得到下图：
![](http://img.nocater.com/18-9-28/16789452.jpg)
所以二元词组的改良精度为$\frac{4}{6} = \frac{2}{3}$.

现在我们将他们公式化：
$$P_1 = \frac{\sum_{unigram\in y} count_{clip}(unigram)}{\sum_{unigram\in \hat y} count(unigram)}$$

$$P_n = \frac{\sum_{ngram\in y} count_{clip}(ngram)}{\sum_{ngram\in \hat y} count(ngram)}$$

如果翻译完全一致的话，那么$P_1$、$P_n$的值为1。

细节：
我们来组合以下构成最终的BLUE得分。$P_n$就是$n$元词组这一项的**BLEU**得分，也是计算出的$n$元词组改良后的精确度。首先计算得到$P_1$，$P_2$， $P_3$，$P_4$。然后求和取平均。BLEU定义为$exp (\frac{1}{4}\sum\limits_{n=1}^{4}{P_n})$，它是严格单调递增的。我们还会使用一个额外的**BP**(brevity penalty)惩罚项，用于调整输出偏向短句翻译。
![](http://img.nocater.com/18-9-28/44522540.jpg)

拥有单一实数评估指标(**a single real number evaluation metric**)是非常重要的。**BLEU**对机器翻译来说具有革命性的意义。它的开源算法有很多。BLEU得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似。不过它并没有用于**语音识别**（speech recognition）。因为在语音识别当中，通常只有一个答案。

### 3.7 注意力模型直观理解 (Attention Model Intuition)
论文：**Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate[J]. Computer Science,2014.**
本节主要讲述Attention model的直观理解，细节在下节。注意力思想已经成为深度学习中最重要的思想之一。

![](http://img.nocater.com/18-9-29/27017481.jpg)
假如翻译的句子非常长，绿色编码器的任务是读取整个句子，然后记忆。紫色神经网络(解码器)将生成英文。而人工翻译是看一点，翻译一点。这个编码器中，它对短语句非常好，**BLUE**得分会很高，但当句子长度达到30，40时候，表现会非常差。神经网络记忆非常长的句子也是很困难的。所以采用注意力模型，每次只翻译一部分。不会出现长句子得分严重下滑情况。
![](http://img.nocater.com/18-9-29/79928189.jpg)
以"Jane visite l'Afrique en septembre"为例子，使用双向RNN可以得到$\hat y^{<1\>},\hat y^{<1\>},\hat y^{<1\>}$等。现在我们构建一个新的注意力机制的单向RNN，首先去掉所有的$Y$输出。然后为了区分感知器(activations)$a^{<n\>}$，我们使用$S^{<n\>}$来标识RNN的隐藏状态。来看生成第一个单词，我们需要看句子的那一部分呢？应该先看第一个或它附近的单词，因此，用$a{<1,1\>}$来标识生成第一个单词时，原句中第一个单词的注意力权重。$a^{<1,2\>}$它告诉我们当你尝试去计算第一个词Jane时，我们应该花多少注意力在输入的第二个词上面。同理这里是$a^{<1,3\>}$，接下去也同理。然后RNN向前进一次生成一个词，知道最终生成**EOS**。*注意力权重，即$a^{<t,t\>}$告诉你，当你尝试生成第$t$个英文词，它应该花多少注意力在第$t$个法语词上面。当生成一个特定的英文词时，这允许它在每个时间步去看周围词距内的法语词要花多少注意力。*

> 个人理解：双向RNN用来对法语单词进行特征标识，新的RNN中，$S$与$A$效果一致，但输入处的$X$不再是一个单词，而是上下文$C$，$C$是注意力权重与双向RNN的输出(不同时间步的特征值)结合的。细节见下一节

### 3.8 注意力模型 (Attention Model)
上节提到**注意力模型如何让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译**。

![](http://img.nocater.com/18-9-29/27713774.jpg)
首先有一个双向RNN来进行计算每个词的特征。每个单元既可以是GRU，也可以是LSTM，LSTM可能会更常见。一共有五个时间步，前向激活值和后向激活值都有六个。
我们使用$t^\prime$来索引法语单词。用$a^{<t^\prime\>}$来标识时间步上的特征向量。然后设置注意力权重$a{<1,t'>}$。注意力权值和为1。之后计算上下位$C$，其中$C^{<1\>} = \sum_{t'}\alpha^{<1,t'>} a^{t'}$。每一个时间步的输入为上下文$C$与上个时间步的输出。

接下来的问题就是该如何定义注意力权重了。
![](http://img.nocater.com/18-9-29/37361513.jpg)
用一个小的神经网络来输出各项权重，softmax可以保证权重和为1。相信反向传播，相信梯度下降。

这个算法有一个缺点是它需要花费三次方时间，即$O(n^3)$。如果你有$T_x$个输入单词和$T_y$个输出单词，于是注意力参数的总数就会是$T_x\times T_y$，所以这个算法有着三次方的消耗。但是在机器翻译的应用上，输入和输出的句子一般不会太长，可能三次方的消耗是可以接受，但也有很多研究工作，尝试去减少这样的消耗。

### 3.9 语音识别 (Speech recognition)
将seq2seq应用到语音识别，了解就好。
- Attention Model for recognition
- CTC cost for speech recognition

### 3.10 触发字检测 (Trigger word detection)
![](http://img.nocater.com/18-9-29/74509672.jpg)
可以将触发后的多个label 0改为1，缓解不平衡。

### 3.11 Thank you! Andrew Ng

### 参考
[深度学习笔记-黄海广](nocater/deeplearning_ai_books)