<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[马尔可夫-蒙特卡洛(MCMC)方法]]></title>
    <url>%2F2019%2F07%2F26%2F%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B-MCMC-%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markov Chain Monte Carlo Without all the Bullshit文章翻译于Markov Chain Monte Carlo Without all the Bullshit 我有一个特点：我才不喜欢统计学中的术语，符号和写作风格。因为它复杂而没有必要。比如，当我想要了解马尔可夫链-蒙特卡洛(Markov Chain Monte Carlo)方法时候，Encyclopedia of Biostatistics.中摘要是这么说的。 马尔可夫-蒙特卡洛(MCMC)是一种通过模拟估计复杂模型中统计量的期望的技术。连续随机选择形成马尔可夫链，这个固定分布就是目标分布。他对贝叶斯模型的后验分布的评估非常有效。…. 我只能模糊地理解作者在这里所说的内容（因为我提前知道MCMC是啥）。它肯定提到了很多比这篇博客要介绍的更高级的东西。但很难找到MCMC的多余解释。作者这些不明不白的“废话”是行话需要。这么做可能是为了解释更加高级的应用程度，但显然没有必要来定义和分析基本思想。 所以，这是我个人的MCMC描述，是受到John Hopcroft and Ravi Kannan的处理启发。 从分布中提取的问题马尔可夫蒙特卡洛是用来解决从一个复杂分布中采样问题的技术。让我构建以下场景来解释。我有一个魔法箱，他能很好地估计婴儿名字地概率。我给他一个类似于“李二狗(Malcolm)”地字符串，它能告诉我这个名字准确的概率$P_{李二狗}$，然后你就可以给你下一个孩子取这个名字。所以，这里有一个在所有名字上的分布$D$，它非常了解你的偏好，为了继续研究，这个分布式固定的。 那么问题来了：假如我想有效描述(efficiently draw)在分布$D$下的名字。这个问题就是MCMC想要解决的。为什么是这个问题？因为我不知道你选取名字的步骤，所以我不能模仿你的步骤。你可能想到另一种方式：随时生成一个名字$x$，然后从魔法箱得到$P_x$，然后掷一枚概率为$P_x$硬币来决定要不要。这个方式问题是你有爆炸级别数量$n=|x|$的名字！所以概率$P_x$会非常非常的小，你需要非常长的时间才能得到同一个名字。或者只有几个名字概率非零，然后需要我指数级别的重复才能找到它们。效率低的我选择死亡。 所以这是一个非常严重的问题！我们重新规范下描述以表达清楚。 定义(采样问题)：$D$为一个有限集$X$上的分布。你有一个可以得到概率分布函数$p(x)$的黑盒，它的输出为根据$D$得到的$x \in X$的概率。请设计一个有效随机算法$A$，它能输出$X$中元素，使得输出的$x$概率近似$p(x)$。更一般的，根据$p(x)$输出$X$元素的采样。 假设算法$A$只能使用公平的随机硬币，尽管这允许我们有效地模拟任何所需概率的硬币。 需要注意，像这种算法我们能做类似估计随机变量$f:X\rightarrow \mathbb{R}$的期望。我们可以通过解决采样的问题得到一个大采样$S \in X$，然后在采样上计算$f$的平均。当采样很容易的时候， 这就是蒙特卡洛。事实上，马尔可夫是用来解决采样问题的，它允许我们在一次采样上估计$\mathbb{E}(f)$。 但核心问题实际上是一个采样问题，而“马尔可夫链蒙特卡罗”更准确地称为“马尔可夫链采样方法”。所以让我们看看为什么马尔可夫链可能会帮助我们。 随机游走，MCMC的’马尔可夫链’部分马尔可夫链是本质上是在图上随机游走的术语。 给你一个图$G={V,E}$,和每一条边$e=(u,v) \in E$。给你一个数字$P{u,v} \in [0,1]$。为了使随机游走有效，$p{u,v}$的和必须为1。 如果满足我们就可以在$G$上根据概率进行随机游走：从顶点$x_0$开始，然后根据出度的边概率随机选择，然后走到顶点$v_1$。然后尽可能重复。 ‘尽可能’是因为任意一个图并不是每个顶点都有出向边。我们需要添加一些限定条件，来在MCMC种应用随机游走。但无论如何，随机游走是明确定义的，我们将$(V,E,{Pe}{e\in E})$整体对象称为马尔可夫链。 这是一个示例，其中图中的顶点对应于情绪状态。 An example Markov chain; image source http://www.mathcs.emory.edu/~cheung/ 在统计学，它们非常重视随机游走的’状态’含义。它们呈边概率为’状态到状态转移’。 马尔可夫链的最有用的主要理论是静态分布定理(有时称为’马尔可夫链的基本定理’)。从直觉上说，对于很长的随机游走，你在顶点V结束的概率与你开始时候的地方无关！所有这些概率一起称为随机游走的静态分布，它由马尔可夫链唯一确定。 然而，由于以上原因，静态分布定理并不是对所有马尔可夫链都适用。重点是图G是强连接的。当你忽略图中边的方向时候，每个顶点都能连接到其它顶点的路径，这称为连通有向图。如果考虑方向，则称为强连接。如果边没有零概率，那么强连接等同于以下属性： 对于每一个顶点$v \in V(G)$，一个从顶点$v$无限的随机游走都会以1的概率返回顶点$v$。 事实上它经常会无限返回。这个属性称为统计下的状态$v$的持久性。我不喜欢这个术语，因为它似乎描述的是一个顶点的属性，对我来说，它描述的是包含该顶点的连通组件的属性。在任何情况下，由于马尔可夫蒙特卡洛，我们会通过设计来确保图是强连接的。 我们使用线性代数来描述静态分布，将转移概率矩阵记为$A$，其中$a_{j,i}=p(i,j)$，边$(i,j)\in E$。矩阵的行列和图$G$的顶点相关，每列$i$表示在随机游走中，从状态$i$到其它状态的概率分布。注意$A$是有向加权图$G$的加权邻接矩阵的转置，其中权重是转移概率(我这么做是因为矩阵向量乘法矩阵在左而不是右，见下)。 这个矩阵允许我用线性代数来很好描述一些东西。在实例中，如果给定一个基本顶点$e_i$作为’’在当前顶点$i$的随机游走’’，$Ae_i$表示了一个顶点在一步随机游走后能到达的$j$个顶点对应的概率。此外，如果有在所有顶点的概率分布$q$，那么$Aq$概率向量表示为： 如果一个随机游走以概率$q_i$在状态$i$,然后$Aq$的第$j$项是指在得到向量$j$后一步随机游走后到达顶点$j$的概率。 以这种方式解释，静态分布就是一个概率分布$\pi$，使得$A\pi=\pi$。换句话说，$\pi$就是具有特征值1的$A$的特征向量。 对于博客的读者来说，这是个一个快速的注意事项：对随机游走的分析正是我们在本博客早期研究用于排名的PageRank算法所作的。在PageRank中，我们称$A$为一个”网络矩阵”，通过随机游走，发现了一个特殊的特征值，其特征向量是我们用来对网页进行排名的的“静态分布”(这用了Perron-Frobenius定理，它说随机游走矩阵具有特殊的特征向量)。我们描述了一种通过迭代乘以$A$来真正找到特征向量的算法。以下定理本质上是该算法的变体，但条件限定更弱一些。对于网络矩阵，在给定需要更强的条件下添加额外的’假’边。 定理：图$G$表示强连接图，其中相关的概率为${pe}_e \in E$边形成了马尔可夫链。对每个概率向量$x_0$，定义$x{t+1}=Axt$，$t \geq 1$，并且$v_t$表示平均$v_t = \frac{1}{t}\sum{s=1}^t x_s$。 然后： 有唯一确定概率向量$\pi$使得$A_\pi = \pi$ 对所有的$x0$，$\lim{t\rightarrow \infin v_t = \pi}$ 证明。因为$v_t$是概率向量，我们希望随着$|t\rightarrow \infin|$而$|Av_t-v_t|\rightarrow 0$。事实上，我们能做以下展开： \begin{aligned} A v_{t}-v_{t} &=\frac{1}{t}\left(A x_{0}+A x_{1}+\cdots+A x_{t-1}\right)-\frac{1}{t}\left(x_{0}+\cdots+x_{t-1}\right) \\ &=\frac{1}{t}\left(x_{t}-x_{0}\right) \end{aligned}但是$x_t,x_0$是单位向量，所以它们的不同主要在2，$|Av_t-v_t|\leq \frac{2}{t} \rightarrow 0$。现在这很清晰表明它并不依赖$v_0$。对于唯一性，我们使用Perron-Frobenius定义，它表明这种形式的任何矩阵都具有唯一的(归一化)的特征向量。 另外一点，除了通过实际计算这个平均值或使用本征结算期来计算静态分布之外，还可以通过解析方法将其解析为特定矩阵的逆矩阵。定义$B=A-I_n$，$I_n$是一个$n\times n$的单位阵。假设$C$为删除第一行并添加最后一行的$B$。然后可以明白最后一列$C^{-1}$是$\pi$。此处练习留给读者，因为实践中没有人使用这种方法。 最后一句关于为什么需要上面定理中的所有的$x_t$取平均值。有一个额外的技术条件添加到强连接，称为非周期性。这允许我们加强定理来使$x_t$收敛到静态分布。严格说，非周期性使这样的属性，无论你在哪里开始随机游走，在经过一些足够多的步骤后，随机游走都有可能在每个后续步骤处于每个顶点。作为非周期性失败的图的样例：偶数个顶点上的五项循环。在那种情况下，每隔一个步骤只存在某些顶点的正概率，并且对这两个长期序列求平均给出时间的静态分布。 保证马尔可夫链使非周期性的一种方法是确保在任何顶点有正概率。即，图具有自循环。 构建一个图来游走回想一下，我们的问题是从有限集$X$的分布得到概率函数$p(x)$。MCMC方法是构建一个马尔可夫链，它静态分布正好是$p$，即使你只能通过黑盒访问。也就是说，你(隐式的)选择了一个图$G$,并(隐式地)选择了边地转移概率来构成静态分布$p$。然后你在图$G$上进行足够长地随机游走，输出你所走过的状态对应的$x$。 如果一个图$G$有正确的竞态分布，这是很简单的(事实上，‘大部分’图都会有效)。难得是，给你一个图，你需要证明随机游走到静态分布的收敛速度要比$x$的增加要快。这超出了本文范围，但如何选择’正确’的图并不难理解。 本文将介绍Metropolis-Hastings算法。输入是对p(x)的黑盒访问，输出是一组隐式定义图上随机游走的规则。它的工作原理如下：选择一些方法来放置$X$格子上，这样每个状态对应${0,1,2,\cdots n}^d$中的一些顶点。然后你给所有的相邻的格子添加顶点(两种直接方式)。比如$n=5,d=2$，和$d=3,n\in {2,3}$为例： 你需要注意保证为$X$选择的顶点没有断开，但在很多应用中，$X$自然已经是一个格子。 然后我们必须描述转换概率。令$r$为格子中顶点的最大度数$r=2d$。假设我们在顶点$i$，想知道下一个步骤，可以按以下操作： 以$1/r$的概率选择一个邻居$j$。(有可能留在原地) 如果选择的邻居$j$概率大于$i$,$p(j)\geq p(i)$，就决定去$j$ 其它情况，$p(j)&lt;p(i)$，以$p(j)/p(i)$概率去$j$ 我们能更简洁保存边$(i,j)$的概率权重$p_{i,j}$： \begin{aligned} p_{i, j} &=\frac{1}{r} \min (1, p(j) / p(i)) \\ p_{i, i} &=1-\sum_{(i, j) \in E(G) ; j \neq i} p_{i, j} \end{aligned}对每个顶点$i$都很容易检查其实际上是一个概率分布。所以我们展示了$p(x)$是当前随机游走的静态分布。 需要注意的是，如果每个$x \in X$都有概率分布$v(x)$，对所有的$(x,y\in X)$都有$v(x)P{x,y} = v(y)P{y,x}$性质,那么$v$就是静态分布。证明它，固定$x$然后对两边在所有$y$求和。结果就是公式$v(x) = \sumy v(y)P{y,x}$，这等于$v=Av$。由于静态分布是满足这个公式的唯一向量，所以$v$是静态分布。 使用$p(i)$来选择是很简单的，因为$p(i)p{i,j}$和$p(i)p{j,i}$都等于$\frac{1}{r} \min(p(i),p(j))$。所以完成了，可以根据这些概率随机游走来获得样本。 最后关于MCMC最后我想说的是，在Metropolis-Hastings图随机游走的过程中，你可以估计函数的期望值$\mathbb{E}(f)$(或者任意竞态分布为$p(x)$的图)。通过定义期望为$\sum_xf(x)p(x)$。 现在需要做的就是计算在随机游走中访问到的状态的$f(x)$的平均值。通过一些其它工作，可以证明这个数量会在随机游走中收敛到静态分布。论证懒得写，但重点是它有效。 我没有从估计函数的期望来描述MCMC是因为核心问题是一个抽样问题。此外MCMC的需要应用都只需要一个样本。比如，MCMC可以用于估计任意凸集的体积。可参考 these lecture notes]]></content>
      <tags>
        <tag>马尔可夫</tag>
        <tag>蒙特卡洛</tag>
        <tag>MCMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformer详解]]></title>
    <url>%2F2019%2F05%2F05%2Ftransformer%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[[TOC] Abstract主流的序列翻译模型一般是基于复杂的循环或卷积神经网络，他们包括encoder和decoder。最好的模型效果是通过Attention机制将encoder和decoder连接的。我们提出了一个简单的网络结构:Transformer，完全基于Attention机制，而不是整体的循环或卷积网络。在两个翻译任务中表明，模型效果更好，更容易并行化和训练。 1. Introduction在实践中，循环神经网络LSTM和GRU稳居序列模型和翻译(如语言模型，机器翻译)第一。许多研究者推进了循环语言模型和encoder-decoder结构工作。RNN通常考虑沿输入和输出序列的符号位置的计算。将位置与time step对齐(Aligning)，从而生成隐层状态序列$ht$, t表示位置，$h{t-1}$是前一个状态。最近的工作已经通过因子化和条件计算实现了计算效率的显着改善，同时在后者的情况下也提高了模型性能。然而，顺序计算的基本约束仍然存在。 Attention机制已成为序列建模和翻译模型等任务中的重要部分，它能忽略输入和输出中的距离从而对依赖建模。绝大多数情况下，Attention机制都是和RNN相连的。 本文提出了Transformer模型架构，其完全依赖于Attention机制来描述输出与输出间的全局依赖。Transformer更易实现并行化，在8个P100GPU上训练12个小时就可以达到最好效果。 3. Model Architecture大部分高效的神经序列翻译模型都是encoder-decoder结构。encoder将输入的符号序列$(x1,…,x_n)$表示成连续表示序列$\mathbf{z}=\left(z{1}, \dots, z{n}\right)$. 在给定$\mathbf{z}$后，decoder生成输出序列$\left(y{1}, \dots, y_{m}\right)$,每次生成一个。每一step，模型是自回归(auto-regressive)， 将上一个输出作为当前额外的输入。 Transformer模型使用stacked self-attention和point-wise来遵循这样整体架构,全连接层连接encoder和decoder。如下图所示： 3.1 Encoder and Decoder StacksEncoder：encoder是由一个$N=6$的栈层。每一层包含两个子层。第一个是multi-head self-attention机制，第二个是一个简单的position-wise全连接的前向神经网络。我们使用残差来连接每个两个子层，并使用layer normalization。换句话说，每个子层的输出为$\text{LayerNorm}(x+\text{Sublayer}(x))$，$\text{Sublayer}(x)$是由其子层自己实现的。为了方便残差的连接，模型中的子层以及嵌入层，其输出维度都是$d_{\text{model}}=512$。 Decoder: decoder同样是由一个$N=6$的栈层构成的。此外，encoder中每层的两个子层，添加了第三个子层：在encoder stack输出上的multi-head attention。与encoder一样，我们使用残差连接每个子层，并使用Layer Normalization。我们还修改了了decoder stack中self-attention子层来保存从attending到后面位置的位置信息。masking结合了嵌入偏移一个位置的情况，保证了对于位置$i$的预测仅依赖于小于$i$的位置的已知输出。 3.2 AttentionAttention功能可以看成是将query和一系列key-value对映射成输出，query，keys，values和output都是向量。oputput是values的加权和，对每个value设置的权重是通过计算query和其对应的key的相关函数(compatibility function)得到的。 3.2.1 Scaled Dot-Product Attention我们使用的attention记为”Scaled Dot-Product Attention“，如图所示。输入是由queries、$d_k$维度的keys、$d_v$维度的values组成。首先使用query与所有的keys进行点积(dot products)运算，并除以每一个$\sqrt{d_k}$，然后使用softmax函数来得到values上的权重。 在实现中，我们同时在一组queries上计算attention函数，并记为矩阵$Q$。kyes和values也组成矩阵$K$和$V$。我们计算的输出矩阵为： (Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \tag{1}两种最常用的attention函数是 additive attention 和 dot-product(multiplicative) attention。除了除数$\sqrt{d_k}$外，点积attention与我们的算法是一样的。加法attention是使用单隐层前向网络来计算相关函数。两种方式在理论上复杂度是相似的，由于dot-product可以使用高度优化的矩阵代码实现，所以它在应用中更快，空间使用也更加有效。 尽管当$d_k$值比较小的时候，两种机制性能相似，但$d_k$变大时候，加法attention不需要缩放，比dot product attention表现更好。我们认为，当$d_k$很大的时候，点积大幅增大，将softmax函数推向具有极小梯度的区域。为了避免这个影响，我们使用$\sqrt{d_k}$来缩放点积。 3.2.2 Multi-Head Attention相比于使用单一attention的$d_{model}$维的keys，values和queries，我们发现，经过$h$次学习的线性映射将query,keys,values分别映成$d_k,d_k,d_v$是有效的。 Multi-head attention允许模型抽取不同表示子层空间的任意位置的信息。相比于单个attentin head，平均值抑制了这一点。 \begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}此处，映射是参数矩阵$W{i}^{Q} \in \mathbb{R}^{d{\text { model }} \times d{k}}$，$W{i}^{K} \in \mathbb{R}^{d{\text { model }} \times d{k}}$，$W{i}^{V} \in \mathbb{R}^{d{\mathrm{model}} \times d{v}}$，和$W^{O} \in \mathbb{R}^{h d{v} \times d_{\mathrm{mode}}}$。 在本文中，我们设置$h=8$来表示attention的 层或头。对每个attention我们使用$d{k}=d{v}=d_{\text { model }} / h=64$。由于对每个attention降维，整体计算的损失与全维度的单头(single-head)attention是接近的。 3.2.3 Applications of Attention in our ModelTransformer使用multi-head attention有三种方式： 在”encoder-decoder attention”层，queries来自于之前的decoder层，然后记忆的keys和values来自于encoder的输出。这允许decoder中的每个位置再能在输入序列的所有位置上进行attention。这于seq2seq中的attention机制相似。 encoder包含self-attention层。在每个self-attention层，所有的keys,values,queries都来自前一encoder层的输出。encoder中的每个位置都能对encdoer所有的历史输出进行attention 同样，self-attention层应用于decoder，这允许decoder的每个位置能对decoder的位置(包括当前位置)进行attention。我们需要防止decoder中的左向信息流来保持自回归属性(auto-regressive property)。我们通过masking(设置为$-\infty$)softmax输入中与非法连接相对应的所有值来实现对点积attention的内部缩放。 3.3 Position-wise Feed-Forward Networks在attention的子层，encoder和decoder中的每一层都是由一个全连接的前向神经网络组成。它是由两个线性变换和ReLU激活函数组成的： \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2} \tag{2} 线性变换在所有不同的位置上是一样的，不同层间的参数是不一样的。另一种理解是看成两个kernel size为1的卷积核。输入和输出维度是$d{\text { model }}=512$，内层网络维度为$d{f f}=2048$。 3.4 Embedding and Softmax与其它序列转换模型类似，我们使用embedding来将输入tokens和输出tokens转换成$d{model}$维向量。我们同样使用学习的新型变化和softmax函数来将decoder输出转换成预测的下一个token概率。在我们的模型，我们在两个embedding layers 和pre-softmax 线性转换中共享同样的权重参数，这与 Using the output embedding to improve language models相似。在embedding层，我们将权重与$\sqrt{d{\text { model }}}$相乘。 3.5 Positional Encoding由于我们的模型没有循环和卷积层，为了使模型能更好的利用序列的排列信息，我们必须加入输入序列的相对或绝对的位置信息。处于此点考虑，我们在encoder stacks和decoder stacks底部添加了”positional encodings”到输入embedding。positional encodings与embedding维度一样，都是$d_{model}$，因此二者可以相加。还有很多其它位置信息的编码方式 Convolutional sequence to sequence learning。 在本文中，我们使用sine和cosine函数来计算频率： P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\ P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)其中$pos$是位置，$i$是维度。这是说，positional encoding的每个维度都对影成了sin值。波长形成几何级数($2\pi-10000\cdot2\pi$).我们选择这个函数是假设它允许模型能容易学习到相对位置，因为对任意固定的偏移项$k$，$PE{pos+k}$能由$PE{pos}$的线性函数表示。 我们还实验了其它positional embedding，结果相似。 4. Why Self-Attention在这个章节，我们对比了self-attention与循环和卷积层各个方面。循环和卷积通常用来将符号序列$\left(x{1}, \dots, x{n}\right)$表示成另一个等长序列$\left(z{1}, \dots, z{n}\right)$，$x{i}, z{i} \in \mathbb{R}^{d}$。就像电影的序列模型encoder或decoder中的隐层。我们采用了三个必要条件。 每层的整体计算复杂度， 能否并行化，通过所需的最小操作数来衡量 网络中远距离依赖的路径长度 Layer Type Complexity per Layer Sequential Operations Maximum Path Length Self-Attention $O(n^2\cdot d)$ $O(1)$ $O(1)$ Recurrent $(n\cdot d^2)$ $O(n)$ $O(n)$ Convolutional $O(k\cdot n \cdot d^2)$ $O(1)$ $O(log_k(n))$ Self-Attentoin(restricted) $O(r\cdot n\cdot d)$ $O(1)$ $O(n/r)$ 如上表所示，一个self-attention层以恒定数量的顺序执行操作来连接所有的位置，而循环层需要$O(n)$序列操作。在计算复杂度上，当序列长度$n$比表示维度$d$小的时候，self-attention层比循环层要快。为了提高在长序列任务中的计算效果，self-attention能后限制成只考虑相应输出位置为中心的输入序列中size为$r$的位置。这能最大化path 长度为$O(n/r)$。我们计划以后进一步研究这个方法。 一层核维度为$k&lt;n$的卷积层没有连接所有的输入和输出位置对。这样做需要在连续内核佛如情况下堆叠O(n/k)$卷积层，或者在扩展卷积的情况下需要$$o(log_k(n))$，增加网络中任何两个位置之间的最长路径的长度。在因子$k$下，卷积层通常比循环层代价更高。系数卷积能将复杂度降低至$O(k \cdot n \cdot d + n \cdot d^2)$，当$k = n$时，稀疏卷积的复杂度与self-attention层核point-wise层相结合(我们模型采用的方法)相同。 另一方面，self-attention能够生成可解性更强的模型。我们对比了我们模型中的attention分布核当前讨论的例子，放在附录中。不仅每个attention head能独立学习不同的任务，许多还表现出与句法核语义结构相关的行为。 备注分析 Complexity per Layer: 每层计算复杂度 Sequential Operations: 论文使用最小的序列来衡量并行化计算。对于传统的RNN，$x_1,x_2,…,x_n$序列需要逐步计算，而self-attention可以使用矩阵操作实现一步到位。 Path length between long-range dependencies: Path length的含义表示计算一个序列长度为n的信息所经过的路径长度。RNN需要从1-n逐次计算，CNN需要增加卷积层来扩大感受野，而self-attention只需要一步矩阵计算。所以self-attention可以比RNN更好解决Long Term Dependency问题。当然，如果序列长度n&gt;序列维度d，可以使用限制(restricted)Attention。 此外，论文附录中表明Attention有更好的可解释性，能学习到一些语法和语义信息。 5 Traning本届来介绍训练细节 5.1 Traning Data and Batching我们使用斯坦福WMT2014English-German数据，包含4.5m语句。词汇表大约37000token。English-French使用著名的WMT2014English-French数据，包含36M语句核32000词汇。句子按照近似的长度来进行批处理。每个batch训练数据包含25000个源token和25000目标token组成的句子。 5.2 Hardward and Schedule8个NVIDIA P100 GPUs.每步需要0.4秒。我们训练100000步或12个小时。对于较大版本的模型，每步需要1秒，训练300000步（3.5天） 5.3 Optimizer使用Adam优化，$\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-9}$, 学习率： \text{lrate} = d_{\text { model }}^{-0.5} \cdot \min \left(\operatorname{step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup_oteps }^{-1.5}\right)这对应于对于第一个warmup_steps训练步骤线性地增加学习速率，并且此后与步数的反平方根成比例地减小它。 我们使用了warmup_steps = 4000。 5.4 Regularization我们使用三种正则化手段： Residual Dropout 在每个子层的输出添加到子层的输入和归一化前，使用dropout。此外我们使用dropout来对encoder及decoder中的embedding和positional encoding求和。基本模型，$P_{d r o p}=0.1$。 Label Smoothing 训练过程中我们使用类别平滑，$\epsilon_{l s}=0.1$。这降低了困惑度，模型学习更不确定，单BLEU得分增加了 ? 6 Result6.1 Machine Translation6.2 Model Variations7 Conclusion在本文中，我们提出了Transformer，第一个完全基于attention的序列转换模型，用multi-head self-attention来替代encoder-decoder结构中常用的卷积层。 对于翻译任务，Transformer训练明显快于基于卷积核循环层的结构。在WMT2014英-德 英-法翻译任务上，达到了state of the art。在前一项任务中，打败了所有以前报道的。 balalal~ 代码： https://github.com/ tensorflow/tensor2tensor 8 个人笔记Attention is all you need这篇论文对Transformer模型的一些细节讲的并不是很清楚，multi-head attention是Transformer模型的核心，讲解的也不是非常多，以及训练的细节描述也不够：比如decoder中的中间子层如何理解？encoder与decoder中的KV有何不同？模型的loss是什么等等。在查阅一部分博客之后，稍微有了一些了解然后先记录下来。 8.1 Attention基本Attention 首先来解释下以前的attention，一般是指seq2seq中的attention机制，当然以前的attention也有很多类型，但区别不是非常大。Attention的思想简单的说，是在decoder的每个time-step时候，添加一个context向量，这个向量是在每一个encoder隐层状态上使用不同的权重计算得到。不同的time-step下，对encoder隐层状态上的权重(注意力)不同，从而帮助更好模型进行预测。那么这个权重是如何计算的呢？我们可以记为score函数，其输入为最后的encoder隐层状态$h_t$和当前decoder状态$\overline{h}_t$(有的用的前一个隐层状态)，score的实现有多种方式： \operatorname{score}\left(h_{t}, \overline{h}_{s}\right)=\left\{\begin{array}{ll}{h_{t}^{T} \overline{h}_{s}} & {\text { Dot }} \\ {h_{t}^{T} W_{a} \overline{h}_{s}} & {\text { General }} \\ {v_{a}^{T} \tanh \left(W_{a} \cdot \operatorname{concat}\left(h_{t}, \overline{h}_{s}\right)\right)} & {\text { Concat }}\end{array}\right.可以记为加法，乘法和MLP三种类型的attention。当然权重应该和为1，所以再使用softmax归一化一下。此外，如果是对所有的encoder的隐层状态进行权重计算，便是global attention，如果是对一部分位置进行attention则成为local attention。所以attention的实现是多样化的，但这些attention都是single-head，既不是multi-head也不是self-attention。详细的传统的Attention可以参考Attention模型详解)。 Transformer 中的Attention Transformer中的attention初步上手看到QKV会非常的懵，其实整理一下并不难。将QKV代入到以往的Attention中对应概念就很好理解了。在基本Attention中，随着decoder端的time-step，我们将decoder的隐层状态$\overline{h}_t$与encoder隐层状态$h_t$进行权重(attention)的计算，每个encoder的隐层状态都会得到一个权重值，二者的这种对应关系与k,v是一样的，所以K,V的理解就比较简单了。在基础Attention中，Q表示decoder端隐层状态，K=V都表示encoder的隐层状态。 我们来看Transformer中的Attention: 首先QK的计算是使用MatMul，即使用的是乘法Attention(论文中称为Scaled Dot-product Attention)，然后经过了缩放，以及Mask(可选)，再经过Softmax得到归一化的权重，然后与V相乘得到最后的输出。 (Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V在公式中，QK的维度为$d_k$，V的维度为$d_v$。$\sqrt{d_k}$是为了对数值进行缩放，否则softmax计算后的效果不够’soft’。公式比较好理解，但QKV在模型中对应还没有说到，往下看。 8.2 Multi-Head Attention顾名思义，multi-head attention是将Attention添加了一个维度，使用多个attention机制去计算。但是根据上一节中如果，QKV不变的话，得到的结果也是一样的，该如果实现multi呢？Transformer是添加了一层对QKV的线性映射。论文中使用了$h=8$个head(attention layer)，所以添加了三个学习权重：$W{i}^{Q} \in \mathbb{R}^{d{\text { model }} \times d{k}}$，$W{i}^{K} \in \mathbb{R}^{d{\text { model }} \times d{k}}$，$W{i}^{V} \in \mathbb{R}^{d{\mathrm{model}} \times d{v}}$，和$W^{O} \in \mathbb{R}^{h d{v} \times d_{\mathrm{mode}}}$，其中$i \in [0, h-1]$。最后将所有single head attention的结果拼接在一起，乘以一个权重后作为最后的输出。 \begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}如果使用代码去实现这里，很直接的思想是使用for head in heads:，但更好的实现方式是使用矩阵，详细的图示可以参考The Illustrated Transformer[译]。multi-head attention是作为Transformer一个子层模块的，论文中提到为了方便，模型的嵌入维度及子层维度是一样的，$d{model} = 512$，而QKV的维度设置为$d{k}=d{v}=d{\mathrm{model}} / h=64$。这样最后拼接后的维度与512一致。 8.3 Self-Attention在基础Attention中，Q来自decoder，KV一样来自encoder。而Self-Attention是指Q=K=V。:laughing: 8.4 Position Extraction Transformer模型包含两类子层，Multi-Head Attention已经讲完，还有一个细节在于，在Multi-Head Attention添加了残差连接(Add)和Layer Norm，同样另一个子层Feed Forward也有相同处理。 Attention is all you need这篇论文将’抛弃RNN及CNN的，完全基于Attention机制的序列转换模型’作为最大的创新点说明。multi-head attention能使序列任意位置的信息进行抽取，这很明显丢失了序列的排列信息。因此，Transformer模型采用了两种方式来进行处理： Positional Encoding 对词序位置信息的编码方式有很多，Google发现下面这种公式处理和其他方式效果一样，所以选择公式处理： P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)$pos$是位置，$i$是维度，也就是说每个维度都使用Positional Encoding编码成了一个的sin值，这样得到的维度与输入序列的嵌入维度一致，就可以将两个嵌入加在一起。由sin的性质可以发现，$PE{pos+k}$可以由$PE{POS}$表示，这表示记录了词序的相对位置信息。 Position-wiseFeed-ForwardNetworks 虽然标题中带了Position，但和序列的直接关系好像并不强。这一个子层是使用ReLU和线性映射组成的： \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}子层的输入和输出维度都是$d_{model}=512$。$W_1$维度为2048。 8.4 The Final Linear and Softmax Layer模型预测单词是使用线性映射和softmax两层来实现的。 8.5 MaskMask顾名思义就是掩码，是为了对某些位置进行遮罩，使其不产生效果。 Transformer模型涉及了两种mask:padding mask和sequence mask。 Padding Mask 在数据的输入中，每个序列的长度是一样的，因此要对序列进行对齐，对较短的序列进行填充。这些填充位置不应该被attention注意，所以需要进行mask。具体的做法是，将这些位置加上一个非常大的负数，这样经过softmax，其值接近于0。 Sequence Mask sequence mask 是为了让decoder不能看见未来的信息。也就是对应序列，在time_step为t的时刻，decoder应该只依赖于t时刻前的输出。具体的做法是：产生一个上三角矩阵，上三角的值全为1，下三角的值权威0，对角线也是0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。 所以： 在decoder中，两种mask都使用，具体实现是两种mask相加 其他情况都是padding mask 8.6 其它细节论文中还提到了使用了Dropout、变化的学习率、Embedding层权重。 在encoder中，每层的输出一次传递到下一层，最后得到K,V是传输到decoder中的每个’encoder-decoder attention’中 在decoder中，是使用上一层的输出作为Q，而K和V是来自encoder的输出。 decoder中使用了mask，将后续的位置设置为$-\infin$，避免后面的翻译结果对当前学习的影响。 算法整理： P\left(w^{1}, \ldots, w^{n}\right)=\prod_{j=1}^{n} p\left(w^{j} | w^{1}, \ldots, w^{j-1}\right)模型结构： h_{0}=U W_{\mathrm{embed}}+W_{\mathrm{position}} \\ h_{l}=\text { transformer-block }\left(h_{l-1}\right) \forall l \in[1, n] \\ P(u)=\operatorname{softmax}\left(h_{n} W_{\text { embed }}^{T}\right)$U$是输入序列的one-hot形式，维度为$dn\times d{vocab}$，$W{embed}$是嵌入矩阵，维度$d{vocab} \times d_{model}$。$n$表示stack size，论文中为6。 Transformer block: transformer-block: input: $h_{in}$ output: $h_{out}$ $h{m i d}=\text { LayerNorm }\left(h{i n}+\text { MultiHead }\left(h_{i n}\right)\right)$ $h{o u t}=\text { LayerNorm }\left(h{m i d}+\mathrm{FFN}\left(h_{m i d}\right)\right)$ $h{in},h{out} \in \mathbb{R}^{d{n} \times d{\mathrm{model}}} $，$d_n$是输入序列维度 MultiHead Attention: $\text{MultiHead}(h)=\text { Concat }\left[\text {head}{1}, \ldots, h e a d{m}\right] W^{O}$ $\text{where} head_i = \text{Attention}(Q,K,V)$ $\text{where} Q,K,V = hW_i^Q,hW_i^K,hW_i^V$ $m$是head数量，输入$h$维度是$dn \times d{model}$，输出维度也是$dn \times d{model}$。 Self-Attention： $\text{Attention}(Q,K,V) = \text{softmax} (\frac{QK^T}{\sqrt{D_k}})V$ 其中Q,K维度为$d_n\times d_k$，V的维度为$d_n \times d_v$ Position-wise Feed Forward Neural Network: $\operatorname{FFN}(h)=\operatorname{ReLU}\left(h W{1}+b{1}\right) W{2}+b{2}$ $W1 \in \mathbb{R}^{d{model}\times d{ff}}, W_2 \in \mathbb{R}^{d{ff}\times d{model}}$，$h$的维度为$d_n \times d{model}$。 9 参考&amp;引用 The Illustrated Transformer The Illustrated Transformer[译] Transformer 模型的 PyTorch 实现(Mask理解)]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的正则化]]></title>
    <url>%2F2019%2F04%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文来自知乎@管他叫大靖 首先，机器学习的一般结构是： \hat{f}=\arg \min_{f \in \mathcal{H}} \sum_{i=1}^{n} l\left(y_{i}, f\left(x_{i}\right)\right) \tag{1}等号右边的叫做损失项。其中 $(x_i,y_i)$ 是训练样本，$l$是损失函数，$\mathcal{H}$ 是假设空间。更多时候我们看到的是如下表达： \hat{f}=\arg \min_{f \in \mathcal{H}} \sum_{i=1}^{n} l\left(y_{i}, f\left(x_{i}\right)\right)+\lambda \Omega(f) \tag{2}多了的一项 $\lambda\Omega(f)$ 叫做正则项。其中， $\Omega(f)$ 是函数的复杂度， $\lambda$ 是超参数。 1. 直观理解基于奥卡姆剃刀原则：如无必要，勿增实体。我们希望再假设空间 $mathcal{H}$ 中找到一个函数 $f$ ，这个函数再训练样本上的误差 $\sum_{i=1}^{n}l(y_i,f(x_i))$ 尽可能地小，同时这个函数也尽可能简单，即 $\Omega(f)$ 尽可能地小。比如当 $\mathcal{H}$ 是线性空间时候，函数维度小。 当 $\mathcal{H}$ 是决策树空间时候， $\Omega(f)$ 可以用叶子节点的数量来刻画。我们希望决策树的损失项尽可能小，但同时叶子又不要太多，$\lambda$ 在其中起到了权衡作用。 2. 从优化角度理解常见的机器学习模型，都可以看成某个特征空间中的线性模型。 比如，一维空间上的多项式回归， $f(x)=\alpha_0+\alpha_1 x+\alpha_2 x^{2}+\alpha_3 x^{3}$ , 可以看成三位空间 $(x_1,x_2,x_3) = (x,x^2,x^3)$ 上的线性回归。 比如，kernel regression，等价于把原空间中的 $p$ 维样本点 $x$ 映射未新空间的 $n$ 维样本 $\left(z_1, \cdots, z_n\right)=\left(k\left(x, x_1\right), \cdots, k\left(x, x_n\right)\right)$ ,再进行普通的线性回归。常见的实现就是SMV中的核函数。 因此，为了表达方便，下面直接拿线性回归举例。 如果 $\Omega(f)=|\omega|_{2}^{2}$ , 这就是ridge回归： \hat{f}=\arg \min_{\omega} \sum_{i=1}^{n} l\left(y_{i}, x_{i}^{T} \omega\right)+\lambda\|\omega\|_{2}^{2} \tag{3}这是一个无约束问题， 需要求解参数 $\omega$，等价于如下带约束条件的优化问题： \begin{array}{c}{\operatorname{minimi} z e_{\beta} \sum_{i=1}^{n} l\left(y_{i}, x_{i}^{T} \beta\right)} \\\\ {\text { s.t. }\|\beta\|_{2}^{2} \leq \tau}\end{array} \tag{4}将(4)写成拉格朗日乘子形式， \operatorname{minimize}_{\beta} \sum_{i=1}^{n} l\left(y_{i}, x_{i}^{T} \beta\right)+\gamma\left(\|\beta\|_2^2-\tau\right) \tag{5}其中， $\gamma \geq 0$ ，不难看出(5)和(3)是等价的，从而(4)和(3)是等价的。因此，加了正则化的优化问题(2)等价于将参数限定再一个区域内(4)。又因参数空间和函数空间是对应的，从而原假设空间加正则项，等价于再子假设空间中上求解(1)。 如果 $\Omega(f)=|\omega|_{1}^{1}$ ，这就是lasso回归： \hat{f}=\arg \min_{\omega} \sum_{i=1}^{n} l\left(y_{i}, x_{i}^{T} \omega\right)+\lambda\|\omega\|_{1}^{1} \tag{6}等价如下带约束问题： \begin{array}{c}{\text { minimize }_{\omega} \sum_{i=1}^{n} l\left(y_{i}, x_{i}^{T} \omega\right)} \\ {\text { s.t. }\|\omega\|_{1}^{1} \leq \tau}\end{array} \tag{7}可以看到，(7)的约束域是一个有尖点的区域(二维的话就是矩形)，有稀疏解。 从方差、偏差角度理解机器学习的最终目的还是做预测，我们希望模型 $f$ 在未知数据上表现好，即泛化能力强，所以在理论上应该优化如下问题： \arg \min_{f \in \mathcal{H}} E_{(x, y)} l(y, f(x)) \tag{8}实际操作时，优化的是(1)或者(2)。假设 $l$ 是平方损失，则： \begin{aligned} M S E &=E l(y, f(x)) \\ \\\\&=E(y-f(x))^{2} \\ \\\\&=E\left(y-f_{\rho}(x)+f_{\rho}(x)-\overline{f}(x)+\overline{f}(x)-f(x)\right)^{2} \\ \\\\&=E\left(y-f_{\rho}(x)\right)^{2}+E\left(f_{\rho}(x)-\overline{f}(x)\right)^{2}+E(\overline{f}(x)-f(x))^{2} \\ \\\\&=\sigma^{2}+\text { bias }+\text {variance} \end{aligned}其中,bias反应了假设空间 $\mathcal{H}$ 的表达能力，variance反应了模型 $f$ 的波动性， $f(x)$ 本质上是一个依赖于训练样本的随机变量。关于Bias(偏差)和Variance(方差)更生动(深刻)的理解可以参考原作者的另一个回答机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？。 通常，假设空间 $\mathcal{H}$ 越大，对潜在函数 $f_p$ 的逼近效果越好(bias越小)，但模型的波动性越大(variance越大)。加载正则化，其实就是在约束假设空间，从(4)和(7)可知，假设空间变小了，表达能力就会变弱，因此bias会变大。但是，variance会变小。最后的MSE可能就减小了。具体操作中，对涉假空间的约束强度用(3)和(6)中的$\lambda$来表示， $\lambda$ 是需要调节的超参数。 为什么假设空间越小，方差越小?举个例子，如果用一个常熟来训练模型，对于(8)就是用均值来预测，那么给定训练集，预测值的方差为0，达到最小，此时模型也就是最简单的。 4. 从贝叶斯的角度理解从极大似然的角度，我们需要优化如下极大似然问题： \arg \max_{\theta} P(Y, X | \theta)其中，$\theta$是待求解的参数，如果误差服从正态分布，那么极大似然可以写成： P(Y, X | \theta)=\Pi_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left\\{-\frac{1}{2 \sigma^{2}}(y-f(x; \theta))^{2}\right\\}求对数，再取符号，等价于如下极小化问题： \arg \min_{\theta} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i} ; \theta\right)\right)^{2} \tag{9}这就是一般的最小二乘问题。 贝叶斯认为参数不是一个给定的值，而是一个随机变量，服从一个分布。因此，我们需要极大化如下全概率： \begin{aligned} & \arg \max _{\theta} P(Y, X | \theta) f(\theta) \\=& \arg \min _{\theta}-\log (P(Y, X | \theta))-\log (f(\theta)) \\=& \arg \min _{\theta} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i} ; \theta\right)\right)^{2}-\log (f(\theta)) \end{aligned} \tag{10}其中$f(\theta)$ 是参数 $\theta$ 的先验分布。(10)和(9)相比，可以看到多了一项 $-\log(f(\theta))$ ， 这其实就是正则项。 当 $f(\theta)$ 服从(标准)正太分布的时候， $-\log(f(\theta)) = C||\theta||_2^2$， (10)就是Ridge回归，对应了L2正则化。 当 $f(\theta)$ 服从拉普拉斯分布的时候， $-\log(f(\theta)) = C||\theta||_1^1$, (10)就是Lasso回归，对应了L1正则化。 因此，贝叶斯先验概率和正则化是对应起来的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>机器学习</tag>
        <tag>惩罚项</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习题目小结]]></title>
    <url>%2F2019%2F04%2F04%2F%E5%AE%9E%E4%B9%A0%E9%A2%98%E7%9B%AE%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[百度数据结构-堆排序大顶堆:子节点比根节点小。小顶堆：子节点比根节点大。参考) 构造初始堆。将给定无序序列构造成一个大顶堆（一般升序采用大顶堆，降序采用小顶堆)。 从最后一个非叶子结点开始，将两个节点信息进行互换，调整子树为正确顺序。继续调整交换节点后的子树结构。 将堆顶元素与末尾元素进行交换，重复以上步骤。 数据结构-普利姆算法&amp;克鲁斯卡尔算法普利姆算法: 选点法，每次选最近的点构成连通图。克鲁斯克尔算法: 选边法，每次选择最短边，避免回环，平均时间复杂度为 $O(|E\log |V|)$ 机器学习-朴素贝叶斯贝叶斯计算 编程 给定一个字符串abab，不断将首字母添加到尾部，可以看出这个过程不同的字符串数量是有限的，请编程实现此问题。对时空复杂度有要求。1234567s = input()res = []for i in range(len(s)): s_new = s[i:] + s[:i] if hash(s_new) not in res: res.append(hash(s_new))print(len(res)) 用例: abab 输出 2 给定源字符串，目标字符串，两个索引值。求源字符串索引值片段内，目标字符串的出现次数。对时空复杂度有要求。12345678910111213141516s = input()t = input()n = input()for i in range(int(n)): nums = input().split() start = int(nums[0]) end = int(nums[1]) sub = s[start-1:end] count = 0 for i in range(len(sub)-len(t)+1): print(sub) if sub[i:i+len(t)] == t: count+=1 print(count) 用例：12345678comeonmandontconconnecton51 51 61 2311 1611 23 输出0 1 4 2 3 面试一面自我介绍，比较详细。 请介绍xgboost 随机森林 GBDT？ 编程题1：求一个特别大的数字的质数分解 编程题2：有一个数组，里面有Articial、Guard、Wall。如何分配警卫使其到艺术品的距离最短。(广度优先搜索算法问题Google)这些编程题更像是LeetCode风格的，有时候偏向难，代码也不用完成很多，面试官也会提示一两点，比如第一题的循环次数可以减少到根号n。最后个人Github成为了加分项。 二面自我介绍(简短)，依次介绍简历上的项目，论文(当前项目使用了RL，把RL介绍了很多)。编程题1：最大正向匹配二面的编程题侧重点与一面的时候不一样，更像是给你算法及思路让你完全把代码一步步写出来。由于没有调整好思路，与面试官来回讨论了多次才对上思路(面试官都说难道还要我给你写出来么~)，最后完成了，不过应该是没一面编程题结果好点，但面试官说看手写的代码还是比较完整的。 机器学习题目： 解释过拟合、欠拟合 常用的激活函数 梯度消失的原因及解决办法 介绍下SVM、核函数的作用及类型 三面自我介绍，介绍项目，论文(同样针对当前所作的项目论文进行了很多的沟通)机器学习题目： 对数据添加个相同特征，会对LR有什么影响(权重，拟合效果) 相同特征的权重会减小，但两个权重和会比之前的权重大。如果在当前维度是线性可分，则扩展到更高维度，更容易可分，即间隔变大，权重变大。 单机如何处理数百亿级特征数量的数百亿级数据(基本没有回答上来，仅提了GPU并行，数据并行，模型并行概念) Sigmoid怎么来的？(两个人思路没有对上，面试官放弃继续追问这个问题了) 信息论解释下LR？ 讲解最近看的一篇论文(机器翻译里的Position Network)，讲解下Bert，Transformer。其它问题： 进程、线程的区别 进程是cpu资源分配的最小单位，线程是cpu调度的最小单位 虚拟内存与物理内存的区别 三门与羊问题(概率论) 同事不完成任务，你该如何应对 上级给了你不能完成的任务，你如何应对 你与同事的目标不一致如何 总结百度实习生的招聘还是很满意的，面试是在食堂，和面试官在餐桌一对一，隔一个座位就是另一个面试官和面试人。面试官很亲和，压力不是很大，面试官会对你进行思维引导，交流过程很亲和。一面问题偏技术向，二面偏学术向，详细的介绍了在校项目和论文。三面面试官以你的经历入手，问你一些问题，自己基本就是见招拆招。 阿里1234567891011121314151617181920212223242526272829303132333435363738394041424344s = '&lt;[播]放|来&gt;[一|几]&lt;首|曲|个&gt;@&#123;singer&#125;的&lt;歌[曲]|[流行]音乐&gt;'target = '来几首@&#123;singer&#125;的流行歌曲'def foo(s, target): while s: seg = '' if s[0] in '&lt;[': if s[0] == '&lt;': seg = s[s.index('&lt;'):s.index('&gt;')+1] s = s[s.index('&gt;')+1:] seg = seg[1:-1] else: seg = s[s.index('['):s.index(']')+1] s = s[s.index(']')+1:] seg = seg[1:-1] seg = seg.split('|') lens = len(target) for i in seg: i.replace('[','') i.replace(']','') j = 0 while target[:j+1] in i: j+=1 if j: target = target[j:] break if len(target) == lens: return 0 else: i = 0 while s[i] == target[i]: i+=1 if i: s = s[i:] target = target[i:] else: return 0 return 1foo(s, target)]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>实习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2019%2F04%2F03%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[特征工程是什么？有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面： 数据预处理未处理的特征可能有以下问题： 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。 信息冗余 定性特征不能直接使用 存在缺失值 信息利用率低 无量纲化无量纲化能使不同规格(范围)的数据转换到同一规格(范围)。常用的无量纲化方法包括标准化和区间缩放法。标准化的前提使特征值服从正态分布，标准化后，其转化成正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特征的范围，例如[0,1]。 常用的方法及划分，网络上的资料负责多样。维基百科中Feature scaling(特征缩放)包含四种方式：Rescaling (min-max normalization)、Mean normalization、Standardization、Scaling to unit length。 Standardization翻译问题：Normalization可以翻译成标准化或归一化，但是根据不同的用途(公式)其理解不同。 通常使用方法：Z-Score标准化 x^\prime = \frac{x-\mu}{\sigma}这会将特征数据转换成均值为0，方差为1。其方法可以使用sklearn.preprocessing.StandardScaler NormalizationMin-Max Normalization x^\prime = \frac{x-mean(x)}{max(x)-min(x)}这将特征数据缩放到[0,1]范围，可以使用sklearn.preprocessing。 MinMaxScaler 对定量特征二值化设置阈值，大于阈值用1表示，相反用0表示。 对定性特征哑编码(one-hot)对类别特征可以使用One-hot编码。 处理缺失值特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方法大于阈值的特征。 相关系数(pearsonr)使用相关系数(pearsonr)，先要计算各个特征值对目标值的相关系数和相关系数的P值。其定义为两个变量之间的协方差和标准差的商： P_{X,Y} = \frac{\text{cov}(X,Y)}{\sigma_X \sigma_Y}其能描述特征与目标值的线性相关性，绝对值越接近1相关性越强，&lt;0.3其意义不大。可以使用scipy.stats.pearsonr，其返回相关系数及p值，The p-value is a number between zero and one that represents the probability that your data would have arisen if the null hypothesis were true. 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： x^2 = \sum \frac{(A-E)^2}{E} = \sum_{i=1}^k \frac{(A_i - E_i)^2}{E_i} = \sum_{i=1}^{k} \frac{(A_i - np_i)}{np_i}其中，Ai为i水平的观察频数，Ei为i水平的期望频数，n为总频数，pi为i水平的期望频率。 可以使用sklearn.feature_selection.chi2 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的： I(X;Y)=\sum_{x\in X} \sum_{y\in Y} p(x,y)\log \frac{p(x,y)}{p(x)p(y)} 在机器学习领域，互信息与信息增益是等价的。 待重新确认。 Wrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征。 Embedded基于惩罚项的特征选择法(L1,L2)L1,L2以及Droup等归于深度学习中的正则化技术进行总结。 信息增益L1,L2以及Droup等归于深度学习中的正则化技术进行总结。 降维当特征维度特别高的时候可以选择对特征进行降维处理。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA） 和 线性判别分析（LDA），线性判别分析本身也是一个分类模型，其也称作 Fisher判别分析。 PCA无监督降维方法。 计算出n个特征的协方差矩阵 $C^{n\times n}, c_{i,j}=COV(X_i,X_j)$ 根据 $Ax = cx$ ，求得特征向量$x$和特征根 $c$ 。 对A的协方差矩阵 $A_{cov}$ 的n个特征根进行排序：$\lambda_1,\lambda_2,…\lambda_n$，及其对应的特征向量：$\xi_1, \xi_2, …\xi_n$。这些特征根就是主成分量 $P=[\xi_1, \xi_2, …\xi_n]$ 将矩阵A与P相乘，就可以得到降维结果。 $A_h^P = A_h \times P$ LDALDA是有监督降维方法，可以参考西瓜书60页。其目标是数据投影到更低维空间，优化目标：最大化类间散度矩阵，最小化类内散度矩阵。 TODO 完成LDA，公式上表。 完成正则化 Reference 特征工程到底是什么？ 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文:Get To The Point]]></title>
    <url>%2F2019%2F04%2F01%2F%E8%AE%BA%E6%96%87-Get%20To%20The%20Point_Summarization%20with%20Pointer-Generator%20Networks%2F</url>
    <content type="text"><![CDATA[Get To The Point: Summarization with Pointer-Generator NetworksAbigail See, Peter J. Liu, Christopher D. Manning Abstract本文提出了一个Sequence-to-Sequence attentional 模型，用于生成文本摘要，数据集使用的 CNN/Daily Mail。此模型思想可用于解决OOV(out of vocabulary)问题。 Our ModelsBaseline 模型与 Nallapati et al. (2016)相似，包含Encoder及Decoder，前者为单层双向LSTM，后者为单向LSTM。 2.1 Sequence-to-sequence attentional model首先使用Attention机制(Bahdanau et al. (2015))， e_i^t = v^T \text{tanh}(W_h h_i + W_s s_t + b_{attn})a^t = softmax(e^t)其中, $V, Wh, W_s, b{attn}$ 都是学习参数，然后可以计算 上下文向量(context vector) $h^*$ ： h_t^* = \sum_i a_i^t h_i然后是模型新添加的模块：将上下文向量 $h^{*}$ 与decoder隐层状态 $s_t$ 拼接，并经过两层线性层，再使用softmax进行归一化，得到 vocabulary distribution $Pvocab$： P_{vocab} = \text{softmax} (V^\prime (V [s_t, h_t^*]+b)+b^\prime)这时候可以得到预测单词 $w$ 最终的分布： $P(w) = P_{vocab}(w)$ 。在训练时候，每个time-step的 $t$ ，目标单词的 $w_t^*$ 的负对数似然估计为： \text{loss}_t = -\log P(w_t^*)整个sequence的损失为: \text{loss} = \frac{1}{T}\sum_{t=0}^T \text{loss}_t2.2 Pointer-generator Network模型的Pointer-generator网络是baseline模型与pointer 模型的结合。其中time-step t 的generation probability $P_{gen} \in [0,1]$ 是由context vector ， decoder state, decoder input计算得到的： P_{gen} = \sigma (w_{h^*}^T h_t^* + w_s^Ts_t + w_x^Tx_t + b)可以看出， $P{gen}$是一个软开关，表示通过从 $P{vocab}$ 中采样来从词汇表中生成(generate)一个单词的概率，或者从输入序列中采样attention $a^t$ 来复制一个单词。我们将原始文档中的所有单词记为 拓展词汇表(extended vocabulary)，则其对应的分布(distribution)为： P(w) = p_{gen}P_{vocab}(w) + (1-p_{gen}) \sum_{i:w_i=w} a_i^t如果w是OOV，那么 $P_{vocab}(w)$ 为0，如果w没有出现在源文档中，则公式后者为0。这使得模型能够解决OOV问题。 生成一个概率值，将两个distribution进行叠加，从而既可以从源序列中提取单词，也可以从词汇表中选择单词。 2.3 Coverage mechanism重复是Seq2Seq的常见问题。使用 converage model 来解决这个问题。首先添加一个 converage vector $c^t$ ，它是decoder的之前的time-step的attention distribution和： c^t = \sum_{t\prime = 0} ^{t-1} a^{t^\prime}直觉上，$c^t$ 是源文档单词的分布，表示这些单词受到attention机制覆盖的长远。其中 $c^0$ 是一个零向量。 converage vector是作为attention机制额外的输入： e_i^t = v^T \text{tanh} (W_h h_i + W_s s_t + W_c c_i^t + b)这使Attention能简单的不在重复输出内容。 注意，添加 coverage loss 来惩罚相同位置重复attention 是非常必要的： \text{covloss}_t = \sum_i \text{min}(a_i^t, c_i^t)最后，整体loss为(添加了超参数 $\lambda$ )： \text{loss}_t = -\log P(w_t^*) + \lambda \sum_i min(a_i^t, c_i^t)]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>自然语言处理</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS_EC2使用教程]]></title>
    <url>%2F2019%2F03%2F21%2FAWS-EC2%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[既可以选择使用一年的AWS免费服务器，也可以选择购买GPU服务器。 注册AWS首先注册AWS，需要一张外币信用卡(Master卡可行)。 选择服务器地区及版本在Amazon EC2 定价查看各地区版本价格。一定要先确定服务器地区及版本，不同地区的相同版本价格是不一样的。以p2.xlarge为例，美国等地区基本是0.9$/hour，但是亚洲地区的是在1.1$起步。 更改上限在选择好地区及版本后，还需要去EC2 Service Limit report配置一下示例的运行上限，此步骤是不需要钱的。如果是先选择了购买，在最后一步可能会提示：123456Launch FailedYou have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit.Hide launch logInitiating launches FailureRetry 在Support Center提交修改limit申请即可，需要十分钟及4 5封邮件时间。 购买实例访问EC2 Management Console选择 Launch Instance。 在左侧选择AWS Marketplace，输入deep learning ubuntu进行搜索，选择Deep Learning AMI(Ubuntu)。 在价格详细页面选择Continue。 选择已经决定的服务器配置类型。 Configure Instance(配置实例)、Add Storage(添加存储)、Add Tags(添加标签)这几步俊保留默认配置，在Configure Security Group(配置安全组)需要自定义配置。创建一个自定义的TCP规则来允许8888端口，并选择Anywhere来允许任何IP访问。 在创建示例过程最后，系统会询问你想要创建新的连接密钥还是重复使用现有的，如果没有使用过创建一个新的就可以了。 运行实例Window可以使用Git的终端进行操作。 使用SSH连接实例,请自行替换部分内容：12345cd /Users/your_username/Downloads/chmod 0400 &lt;your .pem filename&gt; ssh -L localhost:8888:localhost:8888 -i &lt;your .pem filename&gt; ubuntu@&lt;your instance DNS&gt; 在终端中，使用jupyter notebook启动程序，然后复制打开链接。 终止实例返回到AWS Management Console ，选择实例，选择Action，找到Instance State,，单击Terminate。 参考 Launch an AWS Deep Learning AMI with Amazon EC2 Amazon EC2 定价 DeepLearning笔记：如何用亚马逊云服务 GPU 训练神经网络]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>AWS_EC2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[收藏夹]]></title>
    <url>%2F2019%2F03%2F20%2F%E6%94%B6%E8%97%8F%E5%A4%B9%2F</url>
    <content type="text"><![CDATA[机器学习算法整体 算法汇总 各种机器学习算法的应用场景分别是什么 贝叶斯 简单详细化 深入详细化 SVM Linear SVM 和 LR 有什么异同？ LInear SVM和LR都是线性分类器，都是判别模型 Linear SVM部直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别非常不平衡，则需要处理平衡 Linear SVM依赖数据表达的距离测度，所以需要先对数据normalization;LR不受影响 Linear SVM依赖惩罚的系数，实验中需要验证 Linear SVM和LR都会收到outlier的影响，但敏感程度没有明确结论 Linear 只输出类别，LR输出概率。 深度学习 Understanding LSTM Networks:英文,多图说明LSTM机制。来源cs224n推荐。 Attention and Augmented Recurrent Neural Networks:讲解Attention,关于NMT query的部分理解还不够。 强化学习 强化学习_讲的比较完整 强化学习_代码 其它 数据集汇总 NLP数据汇总 BERT使用 27 个Jupyter Notebook的小提示与技巧 matplotlib添加中文英文为宋体及Times New Roman 12import matplotlibmatplotlib.matplotlib_fname() #将会获得matplotlib文件路径 修改配置文件，取消两行注释，并在第二行添加Times New Roman, SimSun, 123font.family : sans-seriffont.sans-serif : Times New Roman, SimSun,...axes.unicode_minus : True # 显示负号 将SimSun.ttf字体文件放入到./fonts/ttf文件夹下 通过以下命令找到缓存文件并删除 12import matplotlibprint(matplotlib.get_configdir()) 重启kernel 如果需要修改公式字体则修改mathtext.fontset : custom. Times New Roman 偏黑，使用宋体SinSun很像没有加粗的TimesNewRoman 123456789101112131415161718&gt; import matplotlib.pyplot as plt&gt; import numpy as np&gt; &gt; from matplotlib.font_manager import _rebuild&gt; # _rebuild() #如果修改了matplotlibrc而不想手动删除缓存 则用代码reload一下&gt; &gt; csfont = &#123;'fontname':'SimSun', 'fontsize': 10&#125;&gt; &gt; x = [0.2,0.3,0.4,0.5,0.6]&gt; y = [36,21,15,12,10]&gt; &gt; plt.figure(dpi=180)&gt; plt.plot(x, y, label='$n_e(0)$')&gt; &gt; plt.xlabel(u'休眠参数'+r'$\theta$', **csfont)&gt; plt.ylabel(u'休眠状态下云用户最优接入阈值'+r'$G_&#123;soc&#125;$', **csfont)&gt; plt.legend()&gt; &gt; python matplotlib 中文显示参数设置 推荐软件 typora:强大的Markdown编辑器 Mathpix Snippiong:截图识别Latex公式，效果非常棒 下载工具FDM(Free Download Manager)：无广告，支持.torrent文件 f.lux 好像和win10兼容性差 AxMath: 强大的公式编辑工具，word中比MathType好用且便宜，已入正版 Github插件： Enhanced GitHub:添加文件大小及下载选项 Isometric Contributions:3D显示contributions Octotree:github项目树，好像比之前少了下载支持 MathJax Plugin for Github: github公式渲染，复杂点貌似不行 octolinker: 代码变量跳转 Sourcegraph: 完全将github项目变成本地IDE使用]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>松鼠</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention模型详解]]></title>
    <url>%2F2019%2F01%2F13%2FAttention%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Attention论文列表 Sequence to Sequence Learning with Neural Networks, 2014. 机器翻译(Machine Translation)领域论文，使用两个RNN进行机器翻译，亮点在于将source sentence做了逆向输入。未看 Neural Machine Translation by Jointly Learning to Align and Translation, 2015. 同样属于MT领域，作者认为将source sentence由Encoder映射成一个向量，无法突破翻译瓶颈，并提出了动态搜索relevant part of source sentence， 即Attention思想。未看 Effective Approaches to Attention-based Neural Machine Translation, 2015. 论文提出了global attention 及 local attention, 博文对其做了比较详细的解释，我也会在下面详细介绍。 Attention is All You Need, 2017. self-attention机制及Transformer模型。 Pointer Sentinel Mixture Models, 2016. cs224n课程曾提到过的，当时并没理解。李宏毅老师的课程视频也有讲解。以往的RNN模型虽然对输入序列长度适应，但输出的序列仍然属于固定的集合。Pointer模型能从输入序列中学习输出单词，从而解决模型无法输出未学习到的单词情况。 Listen, Attend and Spell, 2015. 李宏毅老师曾调侃Attenion模型一定要以三个动词开始。 Attention模型之一详解参考论文Effective Approaches to Attention-based Neural Machine Translation及完全解析RNN, Seq2Seq, Attention注意力机制(知乎专栏)。 “attention”产生于机器翻译领域，其含义是允许模型学习不同任务间的对齐(learn alignments between different modalities)，比如图片描述生成，语音识别。神经机器翻译(NMT)相比于传统MT,占用内存小，decoder实现简单以及end-to-end。 NMT系统是神经网络模型在条件概率$p(y|x)$下将source sentence，$x_1,…,x_n$ 翻译成target sentence, $y_1,…,y_n$,的过程。NMT的一般由Encoder和Decoder，前者学习source sentence的向量化表示，后者是每次生成一个目标单词，故条件概率为： \log p(y|x) = \sum_{j=1}^m \log p(y_j|y_{]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec详解]]></title>
    <url>%2F2018%2F12%2F29%2Fword2vec%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[词向量(词嵌入，Word Embedding)已经比较熟悉了，但还是整体梳理一下比较好。不再详细介绍它的背景和简介，而是将能想到的细节进行整理说明。 语言模型语言模型的任务定义:对于语言序列$w_1,w_2,w_3,…,w_n$,语言模型就是计算该序列的概率，即$P(w_1,w_2,w_3,….,w_n)$。从机器学习角度来看，语言模型就是对语句的概率分布进行建模。简单来说，可以用语言模型来比较两个语句哪个更加正常,如 $P(\text{I am Light}) &gt; P(\text{Light I am})$。由概率公式可得： P(w_1,w_2,w_3,....,w_n) = P(w_1)\cdot P(w_2 | w_1) \cdot P(w_3 |w_1, w_2)\cdot \ ... \ P(w_n|w_1,w_2,...w_{n-1})在统计学领域，语言模型通常是使用n-gram语言模型，即使用size为n的window对序列切分建模，这里不再展开。 2003年，Bengio发表了神经网络语言模型(NNLM)论文[1]，其网络结构如下图所示： NNLM包含输入层，隐层，输出层三层神经网络，首先来看输入层，将单词$wt$的上文(前n个单词)使用one-hot形式表示，并乘以矩阵$C$(跟据one-hot形式可发现是查表操作)。然后将得到的向量拼接在一起得到 $[C(w{t-n+1}), …C(W{t-2}), C(W{t-1})]$ ，其作为隐层的输入。隐层的激活函数为tanh。最后输出层使用Softmax进行词汇表 $V$ 上的分类。矩阵$C$初始是随机初始化的，通过NNLM任务发现，学习后的矩阵$C$是一个密集且包含语义的词向量矩阵。通过one-hot进行相乘可得到每个单词对应的词向量。 one-hot向量长度为词汇表$V$维向量，嵌入维度m可自行设定。在cs224n课程中，矩阵$C$及隐层权重$W$都用来表示词嵌入，后者被成为上下文向量。 在吴恩达课程中，嵌入矩阵即为矩阵$C$。 通过NNLM任务，作为副产品的词向量效果良好，进而将其发扬光大。 Word2vecWord Embedding的常用技术：word2vec主要任务目标就是为了训练得到词向量，而不再是NNLM的任务。word2vec包含两个模型CBOW，Skip-gram。word2vec是由两篇论文[2][3]完成的，第一篇论文提出了两个模型，第二篇论文提出了两个训练技巧，使训练更加有效可行。 模型细节CBOW模型是将中心词的上下文作为输入来进行预测，而Skip-gram是根据中心词来预测其上下文单词。二者其实差不多。现在以Skip-gram为例来介绍下细节[4]。 我们想要训练一个神经网络，抽取语句中的一个中间词来作为输入单词，然后随机选择’临近’的一个单词(‘临近’即超参数window size，通常选择5，即中心词前后各五个单词)。网络的输出概率应该为那些比较相关的单词。比如输入单词为”Soviet”,那输出结果 “Union” 和 “Russia”的概率应该高于其它不相关的单词。 网络应该能从单词组的共现次数上学习到统计信息。比如词组(“Soviet”, “Union”)出现是高于(“Soviet”, “Sasquatch”)，当网络在训练完成后，输入”Soviet”得到的结果中，”Union” 或 “Russia” 的概率应高于 “Sqsquatch”。 加入词汇表长度为10000，首先使用one-hot形式表示每一个单词，经过隐层300个神经元计算，最后使用Softmax层对单词概率输出。每一对单词组，前者作为x输入，后者作为y标签。 隐层细节假如我们想要学习的词向量维度为300，则需要将隐层的神经元个数设置为300(300是Google在其发布的训练模型中使用的维度，可调)。 隐层的权重矩阵就是词向量，我们模型学习到的就是隐层的权重矩阵。 之所以这样，来看一下one-hot输入后与隐层的计算就明白了。 当使用One-hot去乘以矩阵的时候，会将某一行选择出来，即查表操作，所以权重矩阵是所有词向量组成的列表。 输出层细节Softmax细节不再阐述，其能将输出进行概率归一化。注意，神经网络并没有学习到输出相对于输入的偏移量。比如，在我们的语料库中，’York’每次前面都有’New’这个单词，所以，从训练数据角度来讲，’New’与’York’附近的概率为100%，但在’York’附近随机取10个单词，那概率就不再是100%了。 问题假如使用词向量维度为300，词汇量为10000个单词，那么神经网络输入层与隐层，隐层与输出层的参数量会达到惊人的300x10000=300万！训练如词庞大的神经网络需要庞大的数据量，还要避免过拟合。因此，Google在其第二篇论文中说明了训练的trick，其创新点如下： 将常用词对或短语视为模型中的单个”word”。 对频繁的词进行子采样以减少训练样例的数量。 在损失函数中使用”负采样(Negative Sampling)”的技术，使每个训练样本仅更新模型权重的一小部分。 子采样和负采样技术不仅降低了计算量，还提升了词向量的效果。 对频繁词子采样在以上例子中，可以看到频繁单词’the’的两个问题: 对于单词对(‘fox’,’the’)，其对单词’fox’的语义表达并没有什么有效帮助，’the’在每个单词的上下文中出现都非常频繁。 预料中有很多单词对(‘the’,…)，我们应更好的学习单词’the’ Word2vec使用子采样技术来解决以上问题，根据单词的频次来削减该单词的采样率。以window size为10为例子，我们删除’the’： 当我们训练其余单词时候，’the’不会出现在他们的上下文中。 当中心词为’the’时，训练样本数量少于10。 采样率(Sampling rate)使用 $w_i$来表示单词，$z(w_i)$来表示单词的频次。采样率是一个参数，默认值为0.001。$P(w_i)$ 表示单词保留的概率： P(w_i)=(\sqrt{\frac{z(w_i)}{0.001}} +1) \cdot \frac{0.001}{z(w_i)}该函数的图像为： 可以发现随着频次x的增加，其保留的概率越低。关于此函数有三个点需要注意： $P(w_i) = 1.0$(100%保留)，对应的 $z(w_i)&lt;=0.0026$。(表示词频大于0.0026的单词才会进行子采样) $P(w_i) = 0.5$(50%保留)，对应的 $z(w_i)=0.00746$。 $P(w_i) = 0.033$(3.3%保留)，对应的 $z(w_i)=1.0$。(不可能) 负采样(Negative Sampling)训练一个网络是说，计算训练样本然后轻微调整所有的神经元权重来提高准确率。换句话说，每一个训练样本都需要更新所有神经网络的权重。 就像如上所说，当词汇表特别大的时候，如此多的神经网络参数在如此大的数据量下，每次都要进行权重更新，负担很大。 在每个样本训练时，只修改部分的网络参数，负采样是通过这种方式来解决这个问题的。 当我们的神经网络训练到单词组(‘fox’, ‘quick’)时候，得到的输出或label都是一个one-hot向量，也就是说，在表示’quick’的位置数值为1，其它全为0。 负采样是随机选择较小数量的’负(Negative)’单词(比如5个)，来做参数更新。这里的’负’表示的是网络输出向量种位置为0表示的单词。当然，’正(Positive)’(即正确单词’quick’)权重也会更新。 论文中表述，小数量级上采用5-20，大数据集使用2-5个单词。 我们的模型权重矩阵为300x10000，更新的单词为5个’负’词和一个’正’词，共计1800个参数，这是输出层全部3M参数的0.06%！！ 负采样的选取是和频次相关的，频次越高，负采样的概率越大： P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^n(f(w_j)^{3/4})}论文选择0.75作为指数是因为实验效果好。C语言实现的代码很有意思：首先用索引值填充多次填充词汇表中的每个单词，单词索引出现的次数为$P(w_i) * \text{table_size}$。然后负采样只需要生成一个1到100M的整数，并用于索引表中数据。由于概率高的单词在表中出现的次数多，很可能会选择这些词。 分层SoftmaxSkip-gram 代码实现Skip-gram的实现有多种方式，比如很多软件包(gsim)已经收录了，可以直接调用。Skip-gram模型并不复杂，网络仅有一个隐层，使用Keras实现也非常简单，但是自己手写实现更更好理解一些。这里讲下自己的实现。主要参考Tensorflow word2vec模型,该实现封装数据集及推理(analogy)等内容，但对神经网络的定义、负采样、NCE损失函数等内容的，虽然并不都是完全底层实现，但对深入理解还是很有帮助的。数据集采用的是20newsgroups，这个数据集有个博士处理后的版本(去除了停用词the等)。20newsgroups数据还是比较粗糙的，整个实现比较随意，源jupyter及训练数据也就没有上传github,但对模型理解还是有帮助的。 加载数据12345678910111213141516171819202122232425262728293031323334353637383940414243def load_data(): """加载数据""" newsgroup = fetch_20newsgroups(data_home='./ataset/20newgroups/', subset='all', remove=('headers', 'footers', 'quotes')) corpus = ''.join(newsgroup.data).lower().split() counter_corpus = Counter(corpus) words_count = sorted(counter_corpus.items(), key=lambda kv:kv[1], reverse=True) stop_words = '? ! @ # $ % ^ &amp; * ( ) [ ] &#123; &#125; &gt; &lt; = - + ~ ` --- (i (or / ; ;\' $1 |&gt; \ --------- -------------------------------------------------------------------------- \ ========================= \ 0 1 2 3 4 5 6 7 8 9 13 15 30 24 20 "a" tk&gt; 95 45' index = 0 vocab_words, vocab, reverse_vocab, vocab_count, vocab_freq,= [],&#123;&#125;,&#123;&#125;,&#123;&#125;,&#123;&#125; for (k,v) in words_count: if k in stop_words.split() or v &lt; 15: continue # 单词列表 vocab_words.append(k) # 单词:id vocab[k] = index # id:单词 reverse_vocab[index] = k # 单词:频次 vocab_count[k] = v # 单词:频率 vocab_freq[index] = v/len(corpus) index += 1 print('字典长度:', len(vocab.keys())) print(vocab_words[:10]) corpus_int = [vocab.get(word) for word in corpus] # 非字典词语占比 print('非字典词语占比%.2f' % (corpus_int.count(None)/len(corpus_int)*100)) samples = generate_samples(corpus_int, vocab, vocab_freq) # version2 实现子采样 传入原始语料(word list)而不是int print('样本数量:',len(samples)) return samples, list(vocab_count.values()), vocab 使用子采样方式生成训练数据1234567891011121314151617181920212223242526272829def generate_samples(corpus, vocab, vocab_freq): """使用子采样生成数据""" LEN = len(corpus) rate = 0.001 samples = [] for i,center_word in enumerate(corpus): # 非词汇表词过滤 if i-2&lt;0 or i+2&gt;LEN-1 \ or center_word is None \ or center_word == vocab['.']: continue else: condedate_words = [center_word, corpus[i-1], corpus[i-2], corpus[i+1], corpus[i+2]] condedate_words = [word for word in condedate_words if word is not None] freqs = np.array([vocab_freq[word] for word in condedate_words]) p_keeps = (np.sqrt(freqs/rate) + 1) * rate / freqs p_keeps[p_keeps&gt;1] = 1 if random.random() &gt; p_keeps[0]: # center_word 子采样 # print('center_word %d 舍弃' % center_word) continue else: # target_word 子采样 sampled_words = [(center_word, condedate_words[i+1]) for i,p in enumerate(p_keeps[1:]) if random.random()&lt;p] samples.extend(sampled_words) return samples 子采样需要注意，论文中使用的采样公式与cs224n课程中不一样，但都可以实现，别忘记对数值进行截断，概率最大为1。 skip-gram 网络定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143emb_dim = 300vocab_size = len(vocabulary)batch_size = 128num_samples = 5epoches = 1000top_n = 10val_data = ['geometric', 'monitor', 'mouse', 'linux', 'microsoft', 'engine', 'factory', 'storage', 'billion', 'article']def forward(vocab, counts): """Build the graph for the forward pass.""" examples = tf.placeholder(tf.int32, shape=[batch_size], name='input') labels = tf.placeholder(tf.int32, shape=[batch_size, 1], name='label') # Embedding Weight [vocab_size, emb_dim] init_width = 0.5 / emb_dim emb = tf.Variable( tf.random_uniform( [vocab_size, emb_dim], -init_width), name='emb') # Softmax Weight [vocab_size, emb_dim].Transposed. sm_w_t = tf.Variable( tf.zeros([vocab_size, emb_dim]), name='sm_w_t') # Softmax bias [vocab_size] sm_b = tf.Variable(tf.zeros([vocab_size]), name='sm_b') # NCE loss labels_matrix = tf.reshape( tf.cast(labels, dtype=tf.int64), [batch_size, 1]) # Negative sampling sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler( true_classes=labels_matrix, num_true=1, num_sampled=num_samples, unique=True, range_max=vocab_size, distortion=0.75, unigrams=counts)) # Embeddings for examples: [batch_size, emb_dim] example_emb = tf.nn.embedding_lookup(emb, examples) # Weights for labels: [batch_size, emb_dim] # 不需要计算所有的logistic 仅计算正确单词的logistic进行更新 true_w = tf.nn.embedding_lookup(sm_w_t, labels) true_b = tf.nn.embedding_lookup(sm_b, labels) # Weights for sampled ids: [num_sampled, emb_dim] sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids) # Biases for sampled ids: [num_sampled, 1] sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids) # True logits: [batch_size, 1] true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w), 1) + true_b # Sampled logits: [batch_size, numsampled] sampled_b_vec = tf.reshape(sampled_b, [num_samples]) sampled_logits = tf.matmul(example_emb, sampled_w, transpose_b=True) + sampled_b_vec # Similarity option norm = tf.sqrt(tf.reduce_sum(tf.square(emb), axis=1, keepdims=True)) norm_emb = emb / norm val_int = tf.constant([vocab[word] for word in val_data], dtype=tf.int32) val_emb = tf.nn.embedding_lookup(norm_emb, val_int) similarity = tf.matmul(val_emb, tf.transpose(norm_emb)) return examples, labels, true_logits, sampled_logits, similaritydef nec_loss(true_logits, sampled_logits): """Build the graph for the NCE loss.""" # cross-entropy (logits, labels) true_xent = tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.ones_like(true_logits), logits=true_logits) sampled_xent = tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.zeros_like(sampled_logits), logits=sampled_logits) # NCE-loss is sum of the true and noise (sampled words) # contributions, average over the batch. nce_loss_tensor = (tf.reduce_sum(true_xent) + tf.reduce_sum(sampled_xent)) / batch_size return nce_loss_tensordef optimize(loss): """Build the graph to optimize the loss function.""" optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss) return optimizerdef nearby(): passdef train(center_words, target_words, vocab, reverse_vocab, counts): """Build the graph for the full model.""" tf.reset_default_graph() examples, labels, true_logits, sampled_logits, similarity = forward(vocab, counts) loss = nec_loss(true_logits, sampled_logits) optimizer = optimize(loss) saver = tf.train.Saver() with tf.Session() as sess: sess.run(tf.global_variables_initializer()) batch_all = len(center_words) // batch_size print('Batch_all:', batch_all, 'Batch_size:', batch_size, 'Samples:', len(center_words)) for epoch in range(epoches): for num in range(batch_all): x_batch = center_words[num*batch_size: batch_size*(num+1)] y_batch = target_words[num*batch_size: batch_size*(num+1)] y_batch = np.array(y_batch).reshape(-1, 1) _, l = sess.run([optimizer, loss], feed_dict=&#123; examples : x_batch, labels : y_batch &#125;) if num % 100 == 0: print('Epoch:',epoch,' Iter', num, 'loss:', l) if num % 1000 == 0: sim = similarity.eval() for i,j in enumerate(val_data): nearest_n = (-sim[i, :]).argsort()[1:top_n+1] logg = 'Nearest to %s is :' % j for ind,ner_int_word in enumerate(nearest_n): nearest_word = reverse_vocab[ner_int_word] logg = '%s %s'%(logg,nearest_word) print(logg) save_path = saver.save(session, "./checkpoints/word2vec_model_20news.ckpt") 网络定义基本手写copy github官方模型，在这可以看到NCE loss是如何计算，一般理解负采样是在反向传播简化计算，其实也是直接在前向进行计算简化，这点比其它博文教程要好。 训练训练就比较简单，直接上就行。 不过即使使用了子采样，the的中心词数据也在30W+,调参不如直接删除~ 用博士处理后的数据训练效果会好一些，但那个数据的分词也挺烂的~123456samples,counts,vocabulary = load_data()reverse_vocab = &#123;v:k for k,v in vocabulary.items()&#125;center_words = [x for (x,y) in samples]target_words = [y for (x,y) in samples]train(center_words, target_words, vocabulary, reverse_vocab, list(counts)) 总之，skip-gram的效果还是可以看到的。 参考[1] A Neural Probabilistic Language Model[2] Distributed Representations of Words and Phrases and their Compositionality[3] Efficient Estimation of Word Representations in Vector Space[4] cs224n: Word2Vec Tutorial - The Skip-Gram Model[5] word2vec Parameter Learning Explained[6] [Tensorflow word2vec模型][7] 20newsgroups数据集]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>word embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文:Globally Normalized Transition-Based Neural Networks]]></title>
    <url>%2F2018%2F11%2F15%2F%E8%AE%BA%E6%96%87-Globally-Normalized-Transition-Based-Neural-Networks%2F</url>
    <content type="text"><![CDATA[SyntaxNet :Globally Normalized Transition-Based Neural NetworksDaniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn et.alGoogle Inc Abstract我们设计了一个全局正则化的基于转移的神经网络模型(a globally normalized transition-based neural network)，在POS,依赖解析，语句压缩达到了最好的效果。我们的模型是一个在特定trainsition任务上简单的前向传播神经网络，与其它模型相比，准确率更高。我们讨论了global normalization及local normalization：一个重要的概念是标签的偏差意味着globally normalized 模型比locally normalized模型更严格的表达。 IntroductionNN在NLP领域应用广泛，LSTM在POS,语法解析，语义角色标注应用广泛。一个观点是因为循环神经网络才得到这样的效果 我们验证了使用仅globally normalized，精度也能达到甚至超过使用RNN的模型。详见第二节。我们不使用任何循环结构，但用集束搜索验证多个假设，并使用条件随机场(CRF)目标引入globally normalized，以克服locally normalized所遇到的标签偏差问题(???读不懂)。由于我们使用集束推断，我们通过对集束中的元素求和来近似分割函数，并使用早期更新（Collins和Roark，2004; Zhou等，2015）。 我们基于该近似全局归一化来计算梯度，并基于CRF损失对所有神经网络参数执行完全反向传播训练。 章节3重新回顾了下标签偏差问题，说明globally normalized 模型比local normalized模型能更严格的表达。我们的模型在词性标注POS，依赖解析和语句压缩上取得最好的精度，特别是华尔街日报的依赖解析达到历史最好94.61%。 章节5中，我们模型也优于以前用于基于神经网络转换的解析的结构化训练方法。为了在实践过程中进一步阐述标签偏差问题，我们提供了一个语句压缩的例子，其中local 模型几乎完全失败。然后我们证明了没有任何lookahead features的globally normalized模型几乎与我们的最佳模型一样准确。 最后，我们提供了一个名为SyntaxNet的方法的开源实现，我们已将其集成到流行的TensorFlow框架中。 我们还提供了一个预先训练的，最先进的英语依赖解析器，名为“Parsey McParseface”，我们在速度，简单性和准确性方面进行了调整。 2 Model我们的模型是增量式基于转移的解析器。将它应用到不同的任务上只需要调整transition system和 input feature。 2.1 Transition system对于输入 $x$，通常是一个语句，我们定义： 状态集合$S(x)$ 特定的开始符号 $S^{\dagger} \in S(x)$ 允许的决策集合 $A(s,x)$, 所有的$s \in S(x)$ 转移函数 $t(s,d,x)$, 对任一决策$d \in A(s,x)$返回一个新的状态$s^\prime$ 我们会使用函数 $\rho(s,d,x;\theta)$ 来计算输入$x$在状态$s$做决策$d$的得分。向量 $\theta$ 包含模型的所有参数，我们认为 $\rho(s,d,x;\theta)$ 和 $\theta$ 是不同的表示。 本节中，我们省略$x$，函数记为 $S,A(s),t(s,d),\rho(s,d;\theta)$。 在整个工作中，我们使用transition system，相同输入$x$的完整结构，具有相同数量$n(x)$(或简单记为n)的决策。例如，在依赖解析中，arc-standard 和 arc-eager transition system 都是如此，对于长度为$m$的语句输入$x$，完整解析的决策为$n(x) = 2 \times m$。完整结构(complete structure) 是决策/状态序列 $(s1,d_1)…(s_n,d_n)$, $s_1= s^{\dagger}, d_i \in S(s_i)$, 每个$i = 1…n$, $s{i+1} = t(si,d_i)$。我们用 $d{1:j}$ 来表示决策序列$d_1…d_j$。 我们认为决策序列 $d{i:j}$ 和状态 $s_j$是一对一映射：也就是说，我们基本假设一个状态编码了整个决策历史。因此每个序列可以通过从 $S^{\dagger}$ 开始的唯一的决策序列得到。决策序列 $d{1:j-1}$ 和状态可替换表示，我们定义 $\rho(d{1:j-1},d;\theta)$ 与 $rho(s,d;\theta)$等同，其中s是 $d{1:j-1}$ 到达的状态。 得分函数的定义有很多种，我们通过前向传播神经网络来定义： \rho(s,d;\theta)=\phi(s;\theta^{(l)}) \cdot \theta^{(d)}.$\theta^{(l)}$ 是神经网络出去最后一层的所有参数。$\theta^{(d)}$ 是对决策$d$的最后一层参数。 $\phi(s;\theta^{(l)})$ 表示神经网络在参数 $\theta^{(l)}$ 计算得到的状态 $s$。注意，在参数 $\theta^{(d)}$下得分是线性的。我们之后会解释 softmax-style normalization 是如何应用在global和local级的。 2.2 Global vs. Local Normalization在Chen&amp;Manning(2014)的贪婪式神经网络解析中，在上下文 $d_{1:j-1}$ 下决策 $d_j$ 的条件概率分布定义如下： p(d_j | d_{1:j-1;\theta}) = \frac{\exp \rho(d_{1:j-1}, d_j; \theta)}{Z_L(d_{1:j-1}; \theta)} \tag{1}其中： Z_L(d_{1:j-1}; \theta) = \sum_{d^\prime \in A(d_{1:j-1})} \exp \rho(d_{1:j-1},d^\prime; \theta)每一个 $ZL(d{1:j-1}; \theta)$是一个 local normalization 项。决策序列 $d_{1:n}$ 的概率为： p_L(d^{1:n} = \prod_{j=1}^n p(d_j | d_{1:j-1; \theta})\\ =\frac{\exp \sum_{j=1}^n \rho(d_{1:j-1, d_j; \theta})}{\prod_{n=1}^{n}Z_L(d_{1:j-1}; \theta)}. \tag{2}集束搜索尝试找到Eq.(2)的最大化。集束搜索终得附加分是对每个决策使用logsoftmax，$\ln p(dj | d{1:j-1}; \theta)$,而不是原始分数 $\rho(d_{1:j-1}, d_j; \theta)$。 相反，条件随机场(CRF)定义的 $pG(d{1:n})$ 如下： p_G(d_{1:n}) = \frac{\exp \sum_{j=1}^n \rho(d_{1:j-1}, d_j; \theta)}{Z_G(\theta)} \tag{3}其中， Z_G(\theta) = \sum_{d_{1:n}^\prime \in D_n} \exp \sum_{j=1}^n \rho(d_{1:j-1}^\prime, d_j^\prime; \theta)$D_n$是所有的长度为n的有效决策序列集合， $Z_G(\theta)$是global normalization 项。现在推断问题成为找到： \text{argmax}_{d_{1:n} \in D_n} = \text{argmax}_{d_{1:n}\in D_n} \sum_{j=1}^n \rho(d_{1:j-1}, d_j; \theta).可以再次使用集束搜索来找到近似argmax。 2.3 Training训练数据是由输入 $x$ 和 gold decision 序列 $d_{1:n}^*$ 组合而成。我们使用随机梯度下降和对数似然估计(negative log-likehood)函数。在 locally normalized model中，negative log-likehood 函数为： L_{local}(d_{1:n}^*;\theta) = -\ln p_L(d_{1:n}^*;\theta) = \\ -\sum_{j=1}^n \rho(d_{1:n}^*, d_j^*; \theta) + \sum_{j=1}^n\ln Z_L(d_{1:n}^*; \theta) \tag{4}而globally normalized model 的似然估计函数为： L_{global}(d_{1:n}^*;\theta) = -\ln p_G(d_{1:n}^*;\theta) = \\ -\sum_{j=1}^n \rho(d_{1:n}^*, d_j^*; \theta) + \sum_{j=1}^n\ln Z_G(d_{1:n}^*; \theta) \tag{5}locally normalized 的一个显著优势是公式4中 $Z_L$及其导数计算快速有效。而Eq.5中 $Z_G$ 项在很多情况下是难以处理的。 为了使globally normalized 模型更易学习，我们使用集束搜索和early updates。随着训练序列解码，我们记录集束中的gold path，如果gold path落在 $j$ 布集束之外，将对以下目标进行随机梯度步骤： L_{global-beam}(d_{1:n}^*:\theta) = \\ -\sum_{i=1}^j \rho(d_{1:i-1}^*;\theta) + \ln \sum_{d_{1:j} \in \mit{B_j} } \exp \sum_{i=1}^j \rho(d_{1:n}^\prime, d_i^\prime; \theta) . \tag{6}其中， $\mit{B}j$ 是 $j$ 步集束的所有路径, 前缀 $d{i:j}^*$。可以直接计算Eq.6 中loss的梯度，并反向传播。如果gold path 在整个解码过程中保留，则使用解码结束后的集束 $\mit{b_n}$执行梯度步骤。 3 The Label Bias Problem直观来讲，我们希望模型能够在发现错误后，能够修改之前的决策错误。乍一看，使用集束搜索或其它确定搜索结合的locally normalized模型能够修改之前的决策。但是label bias问题(see Bottou (1991), Collins (1999) pages 222-226, Lafferty et al. (2001), Bottou and LeCun (2005), Smith and Johnson (2007))说明其在这方面能力非常弱。 本节通过证明全局规范化模型比局部规范化模型更具表现力，证明了标签偏差问题的正式观点。 Global Models can be Strictly More Expressive than Local Models来考虑将输入序列 $x{1:n}$ 映射成决策序列 $d{1:n}$任务中的 tagging problem。首先，考虑到locally normalized 模型， 在对决策 $di$ 评分时，我们将限制评分函数取到前 $i$ 个输入符号 $x{1:i}$。 评分函数 $rho$ 可以是元组 $\langle d{1:i-1}, d_i, x{1:i}\rangle$ 的其他任意函数: p_L(d_{1:n} | x_{1:n}) = \prod_{i=1}^n p_L(d_i|d_{1:i-1}, x_{1:i}) \\ = \frac{\exp \sum_{i=1}^n \rho(d_{1:i-1}, d_i, x_{1:i})}{\prod_{i=1}^n Z_L(d_{1:i-1}, x_{1:i})}其次，考虑 globally normalized模型： p_G(d_{1:n} | x_{1:n}) = \frac{\exp \sum_{i=1}^n \rho(d_{1:i-1}, d_i, x_{1:i}) }{Z_G(x_{1:n})}这个模型同样的得分函数，在对决策 $d_i$ 评分时，限制于前 $i$ 个输入。 定义 $\mit{PL}$ 为locallly normalized 模型下得分 $\rho$ 所有可能的分布集合 $P_L(d{1:n} | x{1:n})$。同理，可以定义 globally normalized 模型$P_G$。这里的“分布”是一个从 $(x{1:n}, d{1:n})$ 到概率 $p(d{1:n}|x_{1:n}$。我们的主要结果如下(证明可参考论文)。 4 Experiments我们在POS，依赖解析，语句压缩三个任务上进行实验。 4.1 POS数据集： the English Wall Street Journal (WSJ) part of the Penn Treebank the English “Treebank Union” multi-domain corpus containing data from the OntoNotes corpus version 5 the CoNLL ’09 multi-lingual shared task Model ConﬁgurationResult 4.2 依赖解析数据集与POS任务数据集一样，此外，使用WSJ的标准依赖切分。Model Conﬁguration与 Chen and Manning (2014)一致。Result 4.3 语句压缩数据集Filippova(2015)数据集。Model Conﬁguration与transition system相似，将左右替换成keep和drop。Result 5 Discussion略 6 Conclusion我们提出了一种简单但功能强大的模型体系结构，可为POS标记，依赖性解析和句子压缩生成最好的结果。 我们的模型结合了 transition-based 的算法的灵活性和神经网络的建模能力。 我们的研究结果表明，没有循环的前馈网络在通过全局规范化训练时可以胜过LSTM等RNN模型。 我们进一步支持我们的实证结果，证明全局标准化有助于模型克服局部标准化模型所遭受的标签偏差问题。]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>自然语言处理</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文:A Fast and Accurate Dependency Parserusing Neural Networks]]></title>
    <url>%2F2018%2F11%2F13%2F%E8%AE%BA%E6%96%87-A-Fast-and-Accurate-Dependency-Parserusing-Neural-Networks%2F</url>
    <content type="text"><![CDATA[A Fast and Accurate Dependency Parser using Neural NetworksDanqi Chen, Christopher D. ManningStanford University Abstract现在几乎所有的依赖解析器(dependency parsers)都是基于数百万个稀疏指示器特征(sparse indicator feature)进行分类。这些特征泛化能力差，计算成本高。本文提出了一种新的贪婪式(greedy),基于转移(trasition-based)依赖解析器(dependency parser)来实现神经网络学习。由于分类器学习和使用少量的dense feature，所以它运行快，不论是在中文还是英文上有LAS或UAS提高了2%。具体来说，我们的解析器能在92%的UAS数据上每秒处理1000个句子。 Introduction现在，基于特征的判别式依赖解析器(feature-based discriminative dependency parser)(K¨ubler et al., 2009)。应用中基于转换的及其子类解析器都比较快速。但这些解析器仍不完美。首先，从统计学角度来讲，他们使用数百万并不是很好的特征权重，虽然整体上词汇特征和高级交互特征在提升算法，但没有足够数据来给这些权重正确赋值。其次，所有的解析器都需要手工设计特征模板，这需要专业性，通常并不完整。再而，许多特征模板在研究很少的问题：现在解析器，最大的耗时不是算法，而是特征提取。Bohnet说他的算法虽然有效，但99%的时候都在特征提取。 受词向量成功的启发(POS任务)，我们想使用dense feature来解决问题。低维，密集的特征是不错的良好的开始。 然而，如何对配置信息中所有有用信息进行编码，如何建模基于密集表示的高层特征都是挑战。我们训练了一个神经网络分类器，来在trainsition-based 依赖解析器中做解析决策。神经网络能生成词向量，POS，和依赖标签。我们仅使用200个特征在中文和英文的两个不同雨来任务中取得快速而准确的效果。本文贡献： 展示解析任务中学习到密集特征(dense representation)的效果。 提出了新的神经网络结构，速度与精度兼有 介绍一种新的， 能更好得到高层交互特征的激活函数 2 Transition-based Dependency Parsing基于转移的依赖解析旨在预测从初始配置到终端配置的转移序列，结果为依赖树。其效果如图所示：在本文中，我们使用贪婪解析，即使用分类器在从配置提取的他反正基础上对transition进行预测。这类解析器很要意思，他快速，虽有由于错误传播，准确率略低于基于搜索(search-based)的解析器. 本文采用arc-standard系统(Nivre 2004)。 configuration 的含义；$c = (s, b, A)$,分别是Stack, Buffer, Dependency Arc。初始化时候，$s = [ROOT], b = [w_1,w_2…w_n],A = \emptyset$。三个操作定义(可参考之前博文)： LEFT-ARC(l):添加边$s1 \rightarrow s2$及label (l)，并移除$s2$。约束 $|s&gt;2|$ RIGHT-ARC(l):添加边$s1 \leftarrow s2$,并将s1出栈。约束 $|s&gt;2|$ SHIFT将元素进栈。约束 $|b|&gt;1$ Transitions共有 $|T|=2N_l+1$个。图一解释了Trasition序列例子，从初始配置到终端配置。 贪婪解析的本质目标是在给定configuration下，从$T$预测正确的transition。 从一个configuration可以得到以下信息：1. 所有单词及其词性POS标签 2. 每个单词的头及标签 3.单词在栈/缓冲区的位置，不管是否已出栈。 传统特征模板的弊端： 稀疏。特征稀疏，尤其是词汇化特征稀疏是NLP任务常见问题。我们对English Penn Treebank(表1)进行了特征分析。结果如表2.结果说明：1）词汇特征是必不可少的。2）单词对很重要，三词组也很重要。 不完整。不完整性是现有特征模板的不可避免的问题。因为设计手工处理，无法包含每个有用单词对。 计算成本高。 indicator features的特征抽取代价很高。我们必须连接一些单词，POS标签，边标签来生成特征字符串，并在庞大的特征中来寻找。我们的实验中，95%的时间是特征计算。 到目前为止，我们已经讨论了基于转换的依赖解析的初步和稀疏指标特征的现有问题。 在接下来的部分中，我们将详细阐述我们的神经网络模型，以便通过实验评估来证明其效率。 3 Neural Network Based Parser本节介绍神经网络模型和其细节，包括训练，加速解析。 3.1 Model图2描述了网络结构，首先，词向量是一个$d$维向量，用$e_i^w \in \mathbb{R}^d$表示,整个嵌入矩阵用 $E^w \in \mathbb{R}^{d \times N_w}$ 表示, 其中 $N_w$ 为字典大小。同时我们还将POS标签和和边标签影射成$d$维度向量，$e_i^t,e_j^l$ 表示第$i$个POS标签，第$j$个标签。相应的，$E^t \in \mathbb{R}^{d\times N_t}$ 表示全部的POS标签，$N_t$ 为标签的数量。同理$E^l \in \mathbb{R}^{d\times N_l}$。 我们根据每种类型信息（字，POS或标签）的堆栈/缓冲区位置选择一组元素，这可能对我们的预测有效，分别用 $S^w, S^t, S^l$表示。如图2中，$S^t = {lc1(s2).t,s2.t,rc1(s2).t,s1.t}$ , 我们会依次抽取 $PRP, VBZ, NULL, JJ$。我们使用NULL来表示不存在元素。 我们建立了单隐层标准神经网络，我们从$S^w, S^t, S^l$插曲的元素将会输入到神经网络，用 $nw, n_t, n_l$表示每种类型元素的数量。我们将 $x_w = [ e{w1}^w; e{w2}^w;…e{wnw}^w ]$添加到输入层， 其中 $S^w = {w_1,…,w{n_w}}$。同样，我们添加$x^t$ , $x^l$到输入层。 隐层使用立方激活函数(cube activation function): h = (W_1^wx^w + W_1^tx^t + w_1^lx^l + b1)^3$W_1^w \in \mathbb{R}^{d_h \times (d \cdot n_w)}$ , $W_1^t \in \mathbb{R}^{d_h \times (d\cdot n_t)}$ , $W_1^w \in \mathbb{R}^{d_h \times (d \cdot n_l)}$ , $b1$是偏置项。 POS and label embeddings据我们所知，这是首次尝试引入POS标签和圆弧标签嵌入而不是离散表示。尽管POS标签 $\mathbb{P} = {\text{NN, NNP, NNS, DT, JJ…}}$(英语)和边标签 $\mathbb{L} = {\text{amod,tmod,nsubj,…}}$(英语标准依赖)是相对比较小的离散集合，但它们像单词一样，仍然包含很多语义信息。比如NN(singularnoun)比DT(DETERMINER)和NNS(pluralnoun)更接近。我们想要有效抽取更加密集的表示。 Cube activation functioncube $g(x) = x^3$ 来替代sigmoid函数。cube函数能对三个来自不同嵌入维度的$x_ix_jx_k$建模，能更好的适用于依赖解析。Cube函数仍需要理论分析。 The choice of $S^w, S^t, S^l$根据(Zhang and Nivre, 2011)工作，我们选择rich set of elements。 细节上来说，$S^w$包含 $n_w$ = 18个元素：（1）stack及buffer的top 3元素：$s1,s2,s3,b1,b2,b3$;（2）stack top 2元素的第一和第二最可能的左/右孩子：$lc_1(s_i),rc_1(s_i),lc_2(s_i),rc_2(s_i), i=1,2$. （3）stack top 2元素的最可能的左/右元素的左/右元素。 POS标签数据与之对应(18)。边标签数据与之相应$S_l(n_l=12)$。我们的解析器胜在可以很简单的添加一组特征，省去手工繁琐步骤。 3.2 Training我们首先根据训练语句创建样本 ${(ci,t_i)}{i=1}^m$,$c_i$是配置Configuration, $t_i$是预测transition。 模型采用交叉熵损失函数，并使用l2正则化： L(\theta) = - \sum_i \log p_{t_i} + \frac{\lambda}{2}||\theta||^2$\theta$是所有的参数${W_1^w,W_1^t,W_1^l,b1,W_2,E_w,E_t,E_l}$,最后使用softmax来做预测。 对于参数的初始化，我们使用预训练词嵌入l来初始化$E^w$，使用(-0.01,0.01)来初始化$E^t$, $E^l$。英语的预训练词嵌入来自于(Collobert et al., 2011)(#dictionary = 130,000, coverage = 72.7%)，和我们的基于维基的50维度word2vec词嵌入 (Mikolov et al., 2013) ，以及中文语料库的Gigaword (#dictionary =285,791, coverage = 79.0%)。我们也做了随机初始化$E^w$对比试验，详见Section4。模型使用反向传播算法来训练嵌入。 模型使用 mini-batch AdaGrad作为优化器，并使用Dropout(0.5)技术。在开发集上上无标签成就分数(unlabeled attachment score)最高的参数模型作为最终选择。 3.3 Parsing在解析过程中我们选择贪婪解码。在每一步，我们从当前配置Configuration $c$中抽取所有对应的单词，POS和标签嵌入，然后计算$h(c) \in \mathbb{R}^{dh}$，选择最高分 $t=\text{arg max}{t \text{ is feasible}}W_2(t,\cdot)h(c)$，然后执行 $c \rightarrow t(c)$。 与indicator features相比，我们的解析不需要计算连接特征和在巨大特征表中检索，因此特征生成时间大幅减少。其包含大量矩阵加法和乘法操作。为了进一步提高速度，我们采用pre-computation技巧。对 $S^w$，我们预先计算每个位置的最常见10000个单词。因此，隐层计算只需要检索，然后添加 $d_h$维向量。同样，可以计算POS和边标签每个位置的10000个单词。我们只在神经网络解析器中使用这个优化方法，能加速8-10倍。 4 Experiments4.1 Details数据集：English Penn Treebank(PTB) 和 Chinese Penn Treebank(CTB)。图三是三个数据集的统计情况。 4.2 Results超参数设置：嵌入维度$d$=50，隐层大小$h$=200,正则化率 $\lambda = 10^{-8}$,AdaGrad学习率 $\alpha = 0.01$。 同时，我们自己实现了arc-eager算法和arc-standard算法作为对比实验。此外，还与最新的两个解析器进行比较，MaltPaser - 基于转移的贪婪式依赖解析(a greedy transition-based dependency parser)(Nivre et al., 2006)， MSTParser - 基于图的先序解析器(a first-order graph-based parser)(McDonald 2006)。 评估参数：LAS(labeled attachment score) 和 UAS(unlabeled attachment score)。测试环境i7 2.7Ghz 16G RAM 在三个数据集的结果：准确率基本第一梯度，速度基本碾压。 4.3 Effects of Parser ComponentsCube activation function简而言之，Cube比其它激活函数激活高0.8~1.2% Initialization of pre-trained word embeddings预训练在PTB上有0.7%的提升，CBT上1.7%的提升 POS tag and arc label embeddings使用三者结合准确率最高。 4.4 Model Analysis最后，我们研究了参数学习到了什么，特征捕获了什么。 What do $E^t$, $E^l$ capture?使用t-SNE算法对特征进行可视化，如图5。这些嵌入有效地展示了POS标签或弧标签之间的相似性。POS中JJ,JJR,JJS非常相近。acomp,ccomp,xcomp分到了一起。我们有理由相信，POS嵌入对NLP其他任务也会有帮助。 What do $W_1^w,W_1^t,W_1^l$ capture?在了解$E^t$,$E^l$像$E^w$一样能学习到语义信息后，我们想研究每个特征在隐藏中学习到什么。 我们想知道隐层大小$h=200$是否学习到足够的信息。对每个隐层单元$k$，将其权重reshape成 $d \times n_t, d \times n_w, d \times n_l$。这样每个元素和词嵌入矩阵是保持一致的。 我们选取绝对值大于0.2的权重，然后对每个特征可视化它们。图6是三个采样特征，可以看到很有趣的现象： 不同的特征权重分布式多样的。 然而，大多数判别权重来自$W_1^t$（图6中的中间区域），并且这进一步证明了POS标记在依赖性解析中的重要性。 我们仔细研究了$h=200$个特征发现，它们编码了不同的信息。在图6中三个采样点，最大的权重是由以下确定的： Feature 1:$s_1.t, s_2.t, lc(s_1).t$ Feature 2:$rc(s1).t, s1.t, b1.t$ Feature 3:s_1.t, s1.w, lc(s1).t, lc(s1).l通过在indicator feature 实验上观察，这些特征非常合理。我们的模型自动识别最有用的信息来进行预测，而不用手工创建。 我们可以轻松提取三个以上元素特征，包括indicator feature没有的特征。 5 Related Work6 Conclusion我们使用神经网络提出了一种新颖的依赖解析器。 实验评估表明，我们的解析器在精度和速度方面都优于使用sparse indicator features 的其他贪心解析器。 这是通过将所有单词，POS标签和圆弧标签表示为密集向量，并通过新颖的立方体激活函数对其交互进行建模来实现的。 我们的模型仅依赖于密集的特征，并且能够自动学习最有用的特征连接以进行预测。接下来工作是结合神经网络分类器和search-based 模型来进一步提高精度。当然，网络模型还有提升的空间。]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>自然语言处理</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文:Incrementality in Deterministic Dependency Parsing]]></title>
    <url>%2F2018%2F11%2F08%2F%E8%AE%BA%E6%96%87-Incrementality-in-Deterministic-Dependency-Parsing%2F</url>
    <content type="text"><![CDATA[Incrementality in Deterministic Dependency ParsingJoakim NivreVaxjo University Abstract确定性依赖解析(Deterministic dependency parsing)是一个鲁棒和高效的非限制性自然语言文本句法解析方法。本文分析了其增量处理的可能性，并得出结论：在此框架内无法严格实现增量。但是我们还证明，通过选择最优解析算法，可以最小化需要非增量处理结构的数量。这一说法得到了实验证据的证实，该证据表明，当在瑞典文本的随机样本上进行测试时，该算法实现了对输入的68.9％的增量解析。 当限于解析器接受的句子时，增量程度增加到87.9％。 Introduction增量解析被提倡是出于至少两个不同的原因。第一从实用性上讲，语音识别等实时任务需要对当前输入进行不断的更新的分析。第二从理论上来说，增量解析将解析和认知建模(congnitive modeling)联系在一起。然而，由于不同的原因，目前大多数最先进的解析方法都不遵循增量性原则。尝试完全消除输入歧义的解析器 - 即完全解析 - 通常首先使用某种动态编程算法来派生包解析林(packed parse forest)，然后应用概率自上而下模型以选择最可能的分析(Collins, 1997; Charniak, 2000)。由于第一步是基本不确定的，这似乎至少在严格意义上排除了增量性。与之相反，只对输入消除部分歧义的解析器-即部分解析(partial parsing)-通常是确定的，通过一次输入构建最终分析(Abney, 1991; Daelemans et al., 1999)。但由于它们通常会输出一系列未连接的短语或块，因此不能满足增量性的限制。最近提出的确定性依赖解析(Deterministic dependency parsing)作为用于语法分析非受限自然语言文本的方法，具有鲁棒性和高效性(Yamada and Matsumoto, 2003; Nivre, 2003)。在某些方面，这种方法可以看作是传统的完整解析和部分解析之间的折衷。本质上，它是一种全解析，其目标是构建一个对输入字符的完整的语法分析，而不是仅仅是确定主要成分。但它和部分解析类似，是具有鲁棒，高效和确定性的。总而言之，这些属性似乎是依赖解析适应于增量处理，尽管有些实现并不满足这些约束。例如，Yamada和Matsumoto（2003）使用多通道自下而上算法，结合支持向量机，其方式不会导致增量处理。在本文中，我们分析了确定性依赖解析中增量性的约束，并认为严格的增量是不可实现的。 然后，我们分析了Nivre（2003）中提出的算法，并且表明，鉴于先前的结果，该算法从增量性的角度来看是最优的。 最后，我们实验性地评估了算法在实际解析中实现的增量程度。 2 Dependency Parsing在依赖结构中，每个word token是依赖于最多一个的其它word token，通常称之为它的head或regent。也就是说，依赖结构可以使用有向图(directed graph)来表示。节点node表示word token，边arcs表示依赖关系。此外，边arcs可以标记特定依赖类型。图1为瑞典语句的标记依赖图。在下文中，我们将注意力限制在未标记的依赖图上，即没有标记弧的图，但结果也将适用于标记的依赖图。 我们也将自己局限于投影依赖图（Mel’cuk，1988）。 在形式上，我们通过以下方式定义这些结构： 一个依赖图的单词集合$W = w_1…w_n$,边集合$D = (W, A)$。A是由边的集合组成，$(w_i, w_j), (w_i, w_j \in W)$。$w_i &lt; w_j$表示$w_i$先于$w_j$在字符串$W$出现。$w_i \rightarrow w_j$ 表示一条边，$\rightarrow^$ 表示弧关系的重新闭合和传递闭合。 $\leftrightarrow$ 和表示 $\leftrightarrow^$ 对应的无向关系。 依赖图 $D = {W, A}$ 的约束如图2。 将字符串$W = w_1…w_n$在满足约束的条件下映射到依赖图的任务，我们称之为依赖解析(dependency parsing)。更多的细节参考Nivre (2003)。 3 Incrementtality in Dependency Parsing在定义完依赖图后o，我们可以考虑在多大程度上可以逐步构件图。从最严格的意义上讲，我们采用的增量表示，在解析过程中的任何一个时候，都存在一个连接结构，表示目前为止，对输入所做的分析。对我们的依赖图而言，是说在解析过程中图始终是连接的。我们将提高在一分钟内的精度，但首先讨论下增量性(incrementality)和确定性(determinism)间的关系。 至少在从不撤销之前做出的决定这一方面讲，增量性似乎本身并不代表确定性。 因此，涉及回溯的解析方法可以是递增的，只要回溯以这样的方式实现，即我们总是可以维持表示直到回溯点处理的输入的单个结构。 在依赖解析的上下文中，一个典型的例子是Kromann（Kromann，2002）提出的解析方法，它将启发式搜索与不同的修复机制相结合。 在本文中，我们将把注意力限制在依赖性解析的确定性方法上，因为我们认为在更严格的框架内更容易确定基本约束。 我们将以一种方式形式化确定性依赖解析，该方式受传统的基于无上下文语法的shift-reduce解析的启发，使用输入令牌的缓冲区和用于存储先前处理的输入的栈。 但是，由于依赖性解析中不涉及非终结符号，我们还需要维护在处理期间构造的依赖图的表示。 我们使用 $\langle S,I,A \rangle$ 表示解析器配置。$S$是由一个列表表示的栈，$I$ 是输入token列表。$A$ 是依赖图的当前边关系。(由于依赖图的节点由输入字符串给出，因此只需要明确表示弧关系)。给定输入字符串$W$后， 解析器初始化为 $\langle \textbf{nil}, W, \emptyset \rangle$，并在达到 $\langle S, \textbf{nil},A \rangle$条件时终止。如果在结束时图$D=(W,A)$是结构良好的，则接收accept字符串$W$,否则拒绝reject. 为了理解依赖性解析中增量性的约束，我们将首先考虑最直接的解析策略，即从左到右的自下而上解析，在这种情况下，它基本上等同于使用上下文的shift-reduce解析。 乔姆斯基正常形式的自由语法。 解析器以转换系统的形式定义，如图3所示（其中$w_i$和$w_j$是任意字标记）： Left-Reduce转换是结合了栈里的两个token，$w_i$和$w_j$，通过左向相连：$w_j \rightarrow w_i$，头是$w_j$ Right-Reduce转换是结合了栈里的两个token，$wi$和$w_j$，通过右向相连：$w \rightarrow wj$，头是$w$ Shift操作是将下一个token进栈。 Left-Reduce和Right-Reduce transitions的约束是保证满足单头Single head条件。Shift操作的约束是列表非空。 可以看到，由于多个transitions能应用相同的配置，整个transition system是不确定的。因此，为了得到具有确定性的解析器，我们需要引入一种解决过渡冲突的机制。不管使用哪个机制，在给定长度为n的字符串下，要保证解析器能在2n transitions内完成。此外，保证解析器生成非循环和映射的依赖图（并且满足单头约束）。 这意味着当且仅当连接时，终止时给出的依赖图是格式良好的。 我们现代可以定义这个框架中解析器的增量的含义。理想情况下，我们希望图表 $\langle W-I，A\rangle$始终连接。 然而，考虑到Left-Reduce和Right-Reduce的定义，不可能在没有将其移动到堆栈的情况下连接新单词，因此似乎更合理的条件是堆栈的大小不应超过2。 通过这种方式，我们要求每个单词一旦被移动到堆栈上就被附加在依赖图中的某处。 我们现在可能会问，是否有可能通过从左到右的自下而上依赖性解析器实现递增性，并且在一般情况下答案结果为“否”。 这可以通过考虑仅包含三个节点的所有可能的投影依赖图并且检查哪些可以递增地解析来证明。 图4显示了相关结构，其中共有七个结构。 2-5都可以实现结构化增量，首先将两个token进栈，2-3使用Right Reduce, 4-5使用Left Reduce， 之后再都进栈，最后2，4再使用Right Reduce, 3,5 使用Left Reduce。 以(2)为例子，进栈2个token, statck: [a,b]使用Right Reduce, 删除b, 产生 $a \rightarrow b$进栈1个token, stack: [a,c]使用Right Reduce, 删除c, 产生 $a \rightarrow c$PS 一般会有个虚拟根节点[ROOT, a, b] 参后 相反，剩下的三个需要先全部进栈才能执行reduction。然而，1和6-7例子解析器不能构成增量解析的原因是不同的。 6-7中前两个节点在最终依赖图中并不是由一条单边连接的。6中前两个全部依赖第三个token.7中巴啦啦拉你懂的。无论使用何种算法，这都必然存在，并且这就是为什么在这里定义的依赖性解析中不可能实现严格的递增性的原因。需要注意的是，作为6-7的镜像2-3，虽然它们包含三个未通过单边相连的相邻token,但它们仍然可以增量解析。原因在于，前两个token的reduction使得第一个与第三个相邻。因此，有问题的结构定义特征是恰好最左的token不直接相连。 1的情况不同之处在于，这是由严格的自下而上的策略引起的，这需要每个token在与其头部结合时要找到其所有的依赖。对于左依赖来说，这没有问题，如5中可以使用Shift和Left-Reduce来交替处理。但在1中，必须从右到左开始。这派出了严格的增量。然而6-7在当前框架中永远不能处理，而1可以修改策略来达到解析。下一节中说明。 在这一点上，与基于扩展分类语法的增量解析进行比较是有益的，其中（6-7）中的结构通常由某种级联（或产品）处理，这与任何真正的语义都不对应。 组成部分的组合（Steedman，2000; Morrill，2000）。 相反，（1）中的结构通常由函数组合处理，其对应于良好定义的组合语义操作。 因此，可能有人认为（6-7）的处理只是伪增量，即使在其他框架中也是如此。 在我们采用严格的自下而上方法之前，可以注意到本节中描述的算法本质上是Yamada和Matsumoto（2003）与支持向量机结合使用的算法，不同之处在于它们允许解析以多个方式执行 传递，其中一次传递产生的图形作为下一次传递的输入.1它们为多次传递解析的主要动机正是自下而上策略要求每个令牌在组合之前找到所有依赖项的事实 用它的头，这也是阻止增量解析结构的原因，如（1）。 4 Arc-Eager Dependency Parsing为了增加确定性依赖解析的递增性，我们需要结合自下而上和自上而下的处理。 更确切地说，我们需要自上而下处理左依赖者自下而上和右依赖者。 通过这种方式，只要相应的头部和从属关系可用，就会将弧添加到依赖关系图中，即使依赖关系对于其自己的依赖关系是不完整的。 在Abney和Johnson（1991）之后，我们将这种热切的解析称为与前一节中讨论的标准自下而上策略区别开来。 使用与以前相同的解析器配置表示，可以通过图5中给出的转换来定义arc-eager算法，其中wi和wj是任意字标记（Nivre，2003）： Left-Arc transition：$w_j \overset{r}{\rightarrow} w_i$，含义是，将下一个输入token $w_j$指向栈顶元素$w_i$并出栈(pop stack) Right-Arc transition：$w_i \overset{r}{\rightarrow} w_j$，含义是，将栈顶元素$w_i$并出栈(pop stack)指向下一个输入token $w_j$,并将$w_j$入栈 Reduce transition 出栈(pop stack) Shift(SH) 将下一个输入token进栈 可以看出，Left-Arc 和 Right-Arc 类似于 Right-Reduce 和 Right-Reduce。它们确保满足单头(Single head)约束，而只当栈顶token有头时候才能使用Reduce trainsition. Shift与之前一样，列表不为空就可以执行。 比较两个算法，Left-Arc 和 Left-Reduce是对应的，但不同在于，出于对称原因，前者适用于栈顶元素token和下一个输入元素token而不是栈顶的两个元素token。但 Right-Reduce 和 Right-Reduce相比，前者并不减少(Reduce)元素而是简单的将新的右依赖项转移(shift)到栈顶，从而使依赖项可以有自己的右依赖项。但是为了允许多个多个右依赖项，必须还有一种机制来弹出栈中的依赖项。这就是 Reduce transition. 因此，我们可以说，标准自下而上算法中右Reduce transitions所执行的动作是通过Right-Arc结合弧激发(arc-eager)算法中的后续Reduce转换来执行的。由于Right-Arc和Reduce可以由任意数量的转换分隔，因此允许对任意长的右依赖链进行增量解析。对于Arc-eager算法而言，定义增量对于标准自下而上算法来说不那么简单。 简单地考虑堆栈的大小将不再起作用，因为堆栈现在可以包含形成依赖图的连接组件的token序列。另外，由于不再必须转移两个tokens组合到栈，并且因为出栈的任意token都可连接到栈的某个token,我们需要保证图 $(S,As)$ 是一直连接的。$As$表示$A$到$S$的约束，如： $As = {(w_i,w_j) \in A | w_i,w_j \in S }$ 给定增量性的定义后，图4中的2-5可以很好使用arc-eager算法的逐步解析，和标准的自下而上算法一样。 \langle \textbf{nil}, abc, \emptyset \rangle \\ \downarrow \textbf{Shift} \\ \langle a,bc, \emptyset \rangle \\ \downarrow \textbf{Right-Arc} \\ \langle bc,a,\{(a,b)\} \rangle \\ \downarrow \textbf{Right-Arc} \\ \langle cba,\textbf{nil}, \{(a,b), (b,c)\} \rangle我们得出结论，相对于依赖性解析中的递增性，arc-eager算法是最优的，即使它仍然适用于图4中的结构（6-7）不能以递增方式解析。接下来的问题是，在实际的解析过程中，出现这样结构的情况有多频繁。这个问题相当于说，Arc-eager算法对严格的增量处理偏离有多少。答案显然取决于选取的语言和理论框架，我们将在下一节来至少谈一谈这个问题。在此之前，我们希望结果能与之前的上下文(context-free)解析联系起来。 首先可以知道，相比于其它context-free解析的标准应用，自下而上和自上而下的术语在依赖解析的上下文中有一点不同的含义。由于依赖图中没有终止节点，自上而下结构表示头节点是在其依赖被绑定前进行绑定的。然而，依赖图中的自上而下结构并不包含从高层节点prediction底层节点，因为所有的节点是输入字符串给定的。因此，就驱动解析过程的因素而言，这里讨论的所有算法都对应于无上下文解析中的自下而上算法。 有趣的是，如果我们将依赖解析的问题重新定义为使用CNF语法的无上下文解析，那么图4中的有问题的结构（1），（6-7）都对应于右分支结构，并且很好 - 已知自下而上的解析器可能需要无限量的内存来处理右分支结构（Miller和Chomsky，1963; Abney和Johnson，1991）。 此外，如果我们在Abney和Johnson（1991）的框架下分析这里讨论的两种算法，它们对于枚举节点的顺序完全不同，而只是对枚举的顺序有所不同;第一种算法是电弧标准，而第二种算法是弧形标准。 Abney和Johnson（1991）提出的观察之一是，无弧上传解析的弧形策略有时可能需要比弧标准策略更少的空间，尽管它们可能会导致局部模糊性的增加。看起来关于图4中结构（1）的依赖关系解析的arc-eager策略的优点可以沿着相同的行解释，尽管依赖图中缺少非终结节点意味着本地没有相应的增加歧义。虽然详细讨论无上下文解析和依赖解析之间的关系超出了本文的范围，但我们推测这可能是依赖表示在解析中的真正优势。 5 Experimental Evaluation结果可以在表1中找到，我们看到在解析613个句子（平均长度为14.0个单词）中使用的16545个配置中，68.9％在堆栈上有零个或一个连接组件，这是我们要求的 一个严格的增量解析器。 我们还发现，大多数违反增量的行为相当温和，因为超过90％的配置在堆栈上的连接组件不超过三个。 许多违反增量的行为是由无法解析为良好的依赖图的句子引起的，即单个映射依赖树，但解析器的输出是一组内部连接的组件。 为了测试不完全解析对增量统计的影响，我们进行了第二个实验，其中我们将测试数据限制为444个句子（613个中），本文生成了一个格式良好的依赖图。 结果可以在表2中看到。在这种情况下，87.1％的配置实际上满足增量性的约束，并且堆栈中具有不超过三个连接组件的配置的比例高达99.5％。 似乎可以得出结论，尽管在确定性依赖性解析中不可能采用严格的逐字增量，但实际上，Arc-eager算法可以被视为增量解析的近似近似。 6 Conclusion本文分析了确定性依赖解析的增量性实现的可能。我们的第一个结果是否定的，因为我们已经证明在这里考虑的限制性解析框架中无法实现严格的增量。然而，我们证明对于增量依赖解析器，考虑到所有的框架约束，arc-eager解析算法是最优的。此外，我们已经表明，在实际解析中，算法对大多数输入结构执行增量处理。如果我们考虑测试数据中的所有句子，则占比大约为三分之二，但如果我们将注意力限制在格式良好的输出上，则几乎为90％。 由于先前已经证明确定性依赖性解析在解析准确性方面具有竞争力（Yamada和Matsumoto，2003; Nivre等，2004），我们认为这个有前景的方法是适应于具有鲁棒性，高效性及(近乎)增量性的需求的。]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>自然语言处理</tag>
        <tag>cs224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO]]></title>
    <url>%2F2018%2F11%2F06%2FTODO%2F</url>
    <content type="text"><![CDATA[[x] Google收录[x] NexT 6.0 公式解析问题[] - Latex公式说明 NLP数据集备份Yelp评论：Yelp，就是美国的“大众点评”，这是他们发布的一个开放数据集，包含超过500万条评论。 Blogger Corpus：收集了来自http://blogger.com的681,288篇博文，每篇博文至少包含200个常用英语单词。 20 Newsgroups从20个网络新闻组中收集的20000条文本数据，可用于文本分析、分类等。文件大小：61.6 M。ps:数据有点少不到2W。 Billion Words:大型，有统一目标的语言建模数据集。常被用来训练诸如word2vec或Gove的词嵌入表征。 Stanford Sentiment Treebank:标准的情感数据集，在每个句子解析树的结点上带有细腻的情感注解。 亚马逊评论：包含18年来亚马逊上的大约3500万条评论，数据包括产品和用户信息，评级和文本审核。 Netflix PrizeNetflix 发布了他们的电影评级数据集的匿名版；包含 480,000 名用户对 17,770 部电影的 1 亿个评分。首个主要的 Kaggle 风格数据挑战。随着隐私问题的出现，只能提供非正式版。 SogouCS 来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息。相关教程(搜索关键字：SogouCS 分类): 基于 Tensorflow 的 TextCNN 在搜狗新闻数据的文本分类实践 搜狗新闻语料文本分类实践_CSDN Text Classification Datasets：一个文本分类数据集，包含8个可用于文本分类的子数据集，样本大小从120K到3.6M，问题范围从2级到14级，数据来源于 DBPedia、Amazon、Yelp、Yahoo!、Sogou 和 THUCNews中文文本数据集: 1.56GB 2016-01-25 数据集下载 腾讯词向量：Tencent AI Lab Embedding Corpus for Chinese Words and Phrases]]></content>
  </entry>
  <entry>
    <title><![CDATA[Springboot使用说明]]></title>
    <url>%2F2018%2F10%2F25%2FSpringboot%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[1. 新建Spring Boot项目 打开Myeclipse,在左侧项目区域右键，选择1new-&gt; web project。填写项目名称并勾选 Add Maven support，选择next，在最后勾选 Standard Maven JEE project structure，选择完成 将sport项目中的pom.xml内容复制覆盖新建项目的pom.xml，修改第一行的项目名称和版本，以及spring-boot-maven-plugin/Application信息。 将Application.java,application.properties文件复制到相应位置。 进行项目包分层。bean,dao,service,contol等 编写测试方法。12345678@RestControllerpublic class Default&#123; @RequestMapping(value="/") public String defaultMethod()&#123; return "&#123;'name':'Jack', 'gender':'female'&#125;"; &#125;&#125; 2. Mybatis generator使用 将mabatis-generator.xml复制到对应位置，并修改其内容，填写JDBC驱动包、待生成路径、数据库对应表等信息。 修改pom.xml，取消mybatis-generator的注释。 右键项目，选择 Run As - Maven build, 在Goals填写mabatis-generator:generate,选择执行。 Maven自动下载包并执行。 3. Spring Boot Jar包生成选择Run As-Maven install会自动生成可运行Jar包。]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用命令]]></title>
    <url>%2F2018%2F10%2F25%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[git diffgit diff 比较工作区和暂存区的文件git diff —cached 比较暂存区和版本库的文件git diff —staged 比较暂存区和版本库的文件git diff HEAD 比较工作区和版本库的文件patch 恢复操作 git clonegit clone 克隆项目git colne -b 远程分支 拉取特定分支 git branchgit branch new_branch 创建新的分支git branch -l 显示本地分支git branch -v 显示分支最后commit信息git branch -a 显示远程分支git branch -d branch 删除分支git branch -D 强制删除未合并的分支 git statusgit status -s 简要显示状态信息 git addgit add . 提交所有到暂存区git add -u 将被追踪的文件全部提交到暂存区git add -A 将改动和新加文件提交到暂存区git add -f 强制提交git add -i 交互提交 git checkoutgit checkout 汇总显示工作区、暂存区 与HEAD的差异[显示与origin/master一致与否]git checkout [] 切换分支git checout 目标是工作区的文件git checkout [-q] [] [—] … 使用版本覆盖工作区 如果省略则指定暂存区进行覆盖git checkout 撤销git .add 命令[使用暂存区文件覆盖工作区修改]git checkout branch 将branch所指向的提交中的filename替换暂存区和工作区中相应的文件。git checkout . 重置工作区，使用暂存区进行覆盖git checkout HEAD . 重置工作区，使用版本库进行覆盖git checkout -b / 拉取远程分支并创建新分支 git reset 有：重置指定路径文件 无：重置引用git reset [HEAD] 重置暂存区git reset HEAD —hard 使用版本库重置暂存区和工作区git reset HEAD —soft 仅重置引用，不重置暂存区和工作区git reset [-mixed] HEAD^ 回退一次引用，并使用其覆盖暂存区 git commitgit commit -a 不推荐使用 将追踪文件直接提交git commit —amend 修补命令git commit -c 修改重用提交信息 -C直接使用不修改git commit -F 从文件读取 git stashgit stash list 显示所有git stash save message 保存git stash pop pop Stashgit stash apply 恢复不删除git stash drop 删除git stash clear 清空Stash git taggit tag 显示所有标签git tag -a tagtName -m tagMessage 创建taggit show tagNmae 显示tag详细信息git tag -s GPG密钥签署git tag -v GPG验证git push origin tagName 推送标签到远程版本库git push : 删除远程版本库tag 里程碑共享，必须显式的推送。即在推送命令的参数中，标明要推送哪个里程碑。 执行获取或拉回操作，自动从远程版本库获取新里程碑，并在本地版本库中创建。 如果本地已有同名的里程碑，默认不会从上游同步里程碑，即使两者里程碑的指向是不同的。 git pushgit push 创建远程分支git push : 删除远程分支git remote -v 显示remote信息git remote set-url remoteName 修改remote信息git remote add 添加remote git pullgit pull —rebase 设置变基而不是合并git config branch..rebase true 设置pull默认采用rebase git loggit log —graph 图显日志git log —pretty=online 查看文件提交历史 一种格式化日志输出1git config alias.lg log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative core.quotepath=false Tipsgit rm —cache 取消文件追踪git reflog show master | head -5 显示HEAD^ HEAD~3 master@{n} 引用表示git config —global —list 显示git global 配置信息 Markdown 的使用# 一级标题 ##二级标题两个空格换行.+-序列使用*斜体* _斜体_**粗体**，__粗体__]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-序列模型]]></title>
    <url>%2F2018%2F10%2F25%2F05-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[05.序列模型 (Sequence Models)第一周 循环序列模型 (Recurrent Neural networks)1.1 为什么选择序列模型？ (Why Sequence Models)循环神经网络在语音识别、自然语言处理等领域有很大应用。 语音识别 音乐生成 情感分类 DNA序列 机器翻译 视频行为识别 命名实体识别 1.2 数学符号 (Notation)给出下面语句： Harry Potter and Herminoe Granger invented a new spell. $x^{}$ 来索引序列中的元素 $y^{}$ 来表示输出数据中的序列元素 $T{x}$来表示输入序列的长度，使用 $T{y}$来表示输出序列的长度 在以前，用$x^{(i)}$来表示第$i$个训练样本，训练样本i的序列中第$t$个元素用$x^{\left(i \right) }$这个符号来表示，$T_{x}^{(i)}$就代表第$i$个训练样本的输入序列长度。 在NLP中，建立Vocabulary，一般规模的商业应用来说30000到50000单词大小的词典比较常见。每个单词可以使用one-hot表示。 还有一点是 如果遇到不在词表中的单词时候，使用一个新的标记Unknow word，用来作为标记 1.3 循环神经网络模型 (Recurrent Neural Network Model)使用标准的神经网络结构来构建学习序列$x$到序列$y$的映射，会遇到下面问题： 不同的样本的输入和输出长度是不相同的 并没有共享从文本的不同位置上学到的特征(如CNN在图像小块学习到的内容可以应用到整个图像) 循环神经网络模型如图：首先构造激活值$a^{}$，这通常是零向量。$x^{}$经过神经网络得到预测值$\hat{y}^{}$。而当第二个单词$x^{}$输入时候，来自时间步(Time-step)1的信息也会输入。这样一直到最后一个时间步，输入$x^{}$，然后输出${\hat{y}}^{&lt; T{y} &gt;}$。至少在这个例子中$T{x} =T{y}$，同时如果$T{x}$和$T_{y}$不相同，这个结构会需要作出一些改变。可以发现，RNN是使用了当前输入的以前信息来综合预测。 He said, “Teddy Roosevelt was a great President.”He said, “Teddy bears are on sale!” 这个例子中，仅仅使用前置单词信息，Teddy并不能被很好的识别为是否为人名，因此出现了BRNN，即双向RNN公式化：$a^{} = g{1}(Wa^{&lt; 0 >} + Wx^{&lt; 1 >} + b{a})$$\hat y^{&lt; 1 >} = g{2}(Wa^{&lt; 1 >} + b{y})$经过整理后：$a^{} =g(W{a}\left\lbrack a^{&lt; t-1 >},x^{t} \right\rbrack +b{a})$$\hat y^{} = g(W{ya}a^{} +b_{y})$ 1.4 通过时间的反向传播 (Backpropagation through time)讲的不够清楚，仅提示是前向传播的反过程。损失函数：$L^{}( \hat y^{},y^{}) = - y^{}\log\hat y^{}-( 1-\hat y^{})log(1-\hat y^{})$将每一个时间步的的损失函数加起来，得到整体损失函数。$L(\hat y,y) = \ \sum{t = 1}^{T{x}}{L^{&lt; t >}(\hat y^{&lt; t >},y^{&lt; t >})}$ 详细需要细看反向传播实现 1.5 不同类型的循环神经网络 (Different types of RNNs) $T{x}=T{y}$不适用的情况是存在的。many-to-one的例子如电影情感分类one-to-many的例子如音乐生成many-to-many的例子如机器翻译，输入与输出的序列是不等长的。 1.6 语言模型和序列生成 (Language model and sequence generation)在自然语言处理中，构建语言模型是最基础的也是最重要的工作之一，并且能用RNN很好地实现。语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少。语言模型做的最基本工作就是输入一个句子，准确地说是一个文本序列，$y^{}​$，$y^{}​$一直到$y^{}​$。对于语言模型来说，用$y​$来表示这些序列比用$x​$来表示要更好，然后语言模型会估计某个句子序列中各个单词出现的可能性。如何建立一个语言模型呢？首先需要语料库corpus，然后将语句进行标记化，如使用one-hot形式。一般句尾会增加一个额外标记EOS，表示句尾。再添加一个UNK标记表示不在字典中的字。 $x^{}$设为0向量，这一步其实就是通过一个softmax层来预测字典中的任意单词会是第一个词的概率，$y^{} = x^{}$（上图编号2所示）损失函数： $L\left( \hat y^{},y^{}&gt;\right) = - \sum{i}^{}{y{i}^{}\log\hat y{i}^{}}$ 可记为 $L = \sum{t}^{}{L^{&lt; t >}\left( \hat y^{},y^{} \right)}$ 1.7 对新序列采样 (Sampling novel sequences)论文：（Chung J, Gulcehre C, Cho K H, et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling[J]. Eprint Arxiv, 2014. Cho K, Merrienboer B V, Bahdanau D, et al. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches[J]. Computer Science, 2014.） 在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样，来看看到底应该怎么做。 首先输入$x^{} =0$，$a^{} =0$，然后第一个时间步会通过softmax概率采样，得到第一个单词$\hat y^{}$，然后将其输入到第二个时间步。一直采样得到EOS标识。有时候采样到UNK，可以忽略或重采样。 实际中可能用到基于字符的RNN结构。优点是不会遇到UNK标识。但缺点序列太多太长，计算成本高。 1.8 循环神经网络的梯度消失 (Vanishing gradients with RNNs) “The cat, which already ate ……, was full.”“The cats, which ate ……, were full.” 这些句子中，cat/cats和was/were有长期依赖性质，但中间的词可以无限长。神经网络可能会遇到梯度消失，所以RNN很难学习这种依赖。梯度爆炸虽然也会出现在RNN中，但梯度消失更难处理。以后课程会详细讲解。 1.9 GRU单元 (Gated Recurrent Unit(GRU))$a^{&lt; t >} = g(W{a}\left\lbrack a^{&lt; t - 1 >},x^{&lt; t >}\right\rbrack +b{a})$，通过上图可以发现，首先将上一个时间步的$a^{}$输入，然后再输入$x^{}$，并在一起后经过激活函数tanh计算得到$a^{}$，然后$a^{}$经过softmax单元可以产生输出$y^{}$。这是最简单的RNN单元。其规则图像如下： The cat, which already ate…, was full. 许多GRU的想法都来分别自于Yu Young Chang, Kagawa，Gaza Hera, Chang Hung Chu和 Jose Banjo的两篇论文。 首先，GRU添加了新的变量$c$(cell)，代表记忆细胞(图中编号1)。GRU实际上输出了激活值$a^{}​$，$c^{} = a^{}​$（图中编号2所示）。仍使用$c^{} $是因为LSTMs会是两个不同值，GRU是一样的。然后经过计算可以得到候选值${\tilde{c}}^{}$，使用tanh函数计算。重点来了，GRU中的门，用$\Gamma{u}$表示：其计算公式为：$\Gamma{u}= \sigma(W{u}\left\lbrack c^{},x^{} \right\rbrack +b{u})$$\Gamma{u}$的值在大多数情况家非常接近0或1。$\Gamma{u}$的作用就是决定什么时候你会更新这个值。更新公式为：$c^{} = \Gamma{u}*{\tilde{c}}^{} +\left( 1- \Gamma{u} \right)*c^{}$ (* 为元素相乘)当$\Gamma_{u}$=1时候，就会更新，为0时候会保持原来的值。 以例句为例子，当到单词cat时候，门值更新，后面一直不需要更新，到了was单词的时候更新。这时候GRU记住了cat是单数。同样的，$\Gamma_{u}$在多个步骤下都=0，即经过几层后，$c^{}$几乎就等于$c^{}$，这就是GRU缓解梯度消失问题的关键。 细节：$c^{}$是一个向量，比如100维，那么$c^{}$也是100维。${\tilde{c}}^{}$也是相同的维度。$\Gamma_{u}$也就100维的向量，里面的值几乎都是0或者1，就是说这100维的记忆细胞$c^{}$（$c^{}=a^{}$上图编号1所示）就是你要更新的比特。 当然在实际应用中$\Gamma_{u}$不会真的等于0或者1，有时候它是0到1的一个中间值（上图编号5所示），但是这对于直观思考是很方便的，就把它当成确切的0，完全确切的0或者就是确切的1。元素对应的乘积做的就是告诉GRU单元哪个记忆细胞的向量维度在每个时间步要做更新，所以你可以选择保存一些比特不变，而去更新其他的比特。比如说你可能需要一个比特来记忆猫是单数还是复数，其他比特来理解你正在谈论食物，因为你在谈论吃饭或者食物，然后你稍后可能就会谈论“The cat was full.”，你可以每个时间点只改变一些比特。 完整的GRU：完整的添加了一个新的门$\Gamma{r}$，$r$代表相关性（relevance）。$\Gamma{r}$门告诉你计算出的下一个$c^{}$的候选值${\tilde{c}}^{}$跟$c^{}$有多大的相关性。计算这个门$\Gamma{r}$需要参数，正如你看到的这个，一个新的参数矩阵$W{r}$，$\Gamma{r}= \sigma(W{r}\left\lbrack c^{},x^{} \right\rbrack + b{r})$。$\Gamma{r}$是研究生试验后选择的，非常健硕和实用。 GRU，即门控循环单元，这是RNN的其中之一。这个结构可以更好捕捉非常长范围的依赖，让RNN更加有效。然后我简单提一下其他常用的神经网络，比较经典的是这个叫做LSTM，即长短时记忆网络，我们在下节视频中讲解。 1.10 长短期记忆(LSTM (long short term memory) unit)论文：Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780. (比较难) GRU中，$a^{} = x^{}$，并且有两个门： 更新门$\Gamma_{u}$（the update gate） 相关门$\Gamma_{r}$（the relevance gate） LSTM中，有三个门：更新(u)、遗忘(f)、输出(o)LSTM中不再有$a^{} = c^{}$的情况更新与遗忘不再是二选一操作 由红色10号线可以看出，只要设置了正确的遗忘门和更新门，LSTM是相当容易把$c^{}$的值（上图编号11所示）一直往下传递到右边，比如$c^{} = c^{}$（上图编号12所示）。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。 “偷窥孔连接”其实意思就是门值不仅取决于$a^{}$和$x^{}$，也取决于上一个记忆细胞的值（$c^{}$），然后“偷窥孔连接”就可以结合这三个门（$\Gamma{u}$、$\Gamma{f}$、$\Gamma_{o}$）来计算了。如果你读过论文，见人讨论“偷窥孔连接”，那就是在说$c^{}$也能影响门值。 LSTM反向传播计算：门求偏导：$d \Gammao^{\langle t \rangle} = da{next}\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}(1-\Gamma_o^{\langle t \rangle})\tag{1}$ $d\tilde c^{\langle t \rangle} = dc{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c{next})^2)*it*da{next}*\tilde c^{\langle t \rangle}*(1-\tanh(\tilde c)^2) \tag{2}​$ $d\Gammau^{\langle t \rangle} = dc{next}\tilde c^{\langle t \rangle} + \Gammao^{\langle t \rangle} (1-\tanh(c{next})^2) * \tilde c^{\langle t \rangle} * da_{next}\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{3}$ d\Gamma_f^{\langle t \rangle} = dc_{next}\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) \* c_{prev} \* da_{next}\Gamma_f^{\langle t \rangle}\*(1-\Gamma_f^{\langle t \rangle})\tag{4}参数求偏导 ： $ dWf = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a{prev} \ xt\end{pmatrix}^T \tag{5} $ $ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a{prev} \ xt\end{pmatrix}^T \tag{6} $ $ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a{prev} \ xt\end{pmatrix}^T \tag{7} $ $ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a{prev} \ x_t\end{pmatrix}^T \tag{8}$ 为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。 最后，计算隐藏状态、记忆状态和输入的偏导数： $ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{9}​$ $ dc{prev} = dc{next}\Gammaf^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c{next})^2)*\Gammaf^{\langle t \rangle}da{next} \tag{10}$ $ dx^{\langle t \rangle} = W_f^Td\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11} $ LSTM和GRU的选择没有准则，GRU模型更加简单，容易创建更大网络，只有两个门，计算也快。LSTM更强大和灵活，拥有三个门。 1.11 双向循环神经网络 (Bidirectional RNN)在这个示例中，仅使用单向RNN不能很好的识别Teddy是否为人名。以四个单词为例子，使用${\overrightarrow{a}}^{}$，${\overrightarrow{a}}^{}$，${\overrightarrow{a}}^{}$还有${\overrightarrow{a}}^{}$，表示前向循环单元，然后添加了${\overleftarrow{a}}^{}$，左箭头代表反向连接，${\overleftarrow{a}}^{}$反向连接，${\overleftarrow{a}}^{}$反向连接，${\overleftarrow{a}}^{}$反向连接，这里的左箭头代表反向连接。最后，这各网络构成了无环图。举个例子，以时间步3来说，$x^{}$经过前向的${\overrightarrow{a}}^{}$到前向的${\overrightarrow{a}}^{}$到前向的${\overrightarrow{a}}^{}$再到$\hat y^{}$。同理$x^{}$可以到达。而对于$x^{}$，可以经过反向的${\overleftarrow{a}}^{}$，到反向的${\overleftarrow{a}}^{}$再到$\hat y^{}$。所以时间3，不仅使用了过去的信息，还有现在的信息。 这就是双向循环神经网络，并且这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，对于大量有自然语言处理问题的文本，有LSTM单元的双向RNN模型是用的最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，一个有LSTM单元的双向RNN模型，有前向和反向过程是一个不错的首选。 如果你总是可以获取整个句子，这个标准的双向RNN算法实际上很高效。 1.12 深层循环神经网络 (Deep RNNs)Deep RNN有两种类型：网状、串状 网状：以$a^{\lbrack 2\rbrack }$，上图编号5所示为例子，激活值$a^{\lbrack 2\rbrack }$有两个输入，一个是从下面过来的输入（上图编号6所示），还有一个是从左边过来的输入（上图编号7所示），$a^{\lbrack 2\rbrack &lt; 3 >} = g(W{a}^{\left\lbrack 2 \right\rbrack}\left\lbrack a^{\left\lbrack 2 \right\rbrack &lt; 2 &gt;},a^{\left\lbrack 1 \right\rbrack &lt; 3 >} \right\rbrack + b{a}^{\left\lbrack 2 \right\rbrack})$，这就是这个激活值的计算方法。参数$W{a}^{\left\lbrack 2 \right\rbrack}$和$b{a}^{\left\lbrack 2 \right\rbrack}$在这一层的计算里都一样，相对应地第一层也有自己的参数$W{a}^{\left\lbrack 1 \right\rbrack}$和$b{a}^{\left\lbrack 1 \right\rbrack}$。 串状：没有横向的连接，只有纵向的连接。可以使用简单的RNN单元，也可以使用GRU，LSTM，甚至双向RNN。 第二周 自然语言处理与词嵌入 (Natural Language Processing and Word Embeddings)2.1 词汇表征 (Word Representation)使用one-hot形式表示词汇，每个单词都是独立没有联系的(任何两个单词的内积为零)。因此可以使用词嵌入。这种高维表示，词向量间可以通过计算内积来表示相似度。为了可视化，可以使用t-SNE算法将词向量高维空间映射到二维空间来观察。 2.2 使用词嵌入 (Using Word Embeddings)如何将词嵌入表示应用到NLP中？这是如何用词嵌入做迁移学习的步骤。 第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。 第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量。尽管one-hot向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。 第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。 词嵌入技术广泛应用于NLP中的命名实体识别，文本摘要，指代消除等，在语言模型和机器翻译领域使用的少一些。 人脸识别中的术语编码（encoding）和嵌入（embedding）可以互换，差别不是因为术语不一样，这个差别就是，人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，而像一些没有出现过的单词我们就记为未知单词。 2.3 词嵌入的特性 (Properties of Word Embeddings)论文：Mikolov T, Yih W T, Zweig G. Linguistic regularities in continuous space word representations[J]. In HLT-NAACL, 2013. 词嵌入能够实现类比推理，准确率在30%~75%左右。对于Man-Woman这种关系，King对应的是那个单词？这时候我们使用Man的词嵌入向量$e{man}$减去Woman的词嵌入向量$e{woman}$，在第一位度表示Gender项可以得到大概-2值，其余项为0。同样，对于King来说，Queen与King的差值向量与之相似。 在向量$u$和$v$之间定义相似度:$\text{sim}\left( u,v \right) = \frac{u^{T}v}{\left| \left| u \right| \right|{2}\left| \left| v \right| \right|{2}}$距离用平方距离或者欧氏距离来表示:$\left| \left| u - v \right|\right|^{2}$测量两个词的两个嵌入向量之间的相似程度。 给定两个向量$u$和$v$，余弦相似度定义如下： ${CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$ 其中 $u.v$ 是两个向量的点积（或内积），$||u||_2$是向量$u$的范数（或长度），并且 $\theta$ 是向量$u$和$v$之间的角度。角度越小，两个向量越相似。 2.4 嵌入矩阵 (Embedding Matrix)应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵。 例如，假如词汇表长度为1000，词嵌入维度为300。则所有词汇组成的矩阵为3001000。假如6527个单词代表orange，使用符号$O{6527}$来表示这个one-hot向量，这个向量除了6527位置为1，其余全为0。假设嵌入矩阵为$E$，它的第6527列为$e{6527}$表示单词orange的嵌入向量。这时候把矩阵$E$和*one-hot向量相乘，则可以得到$e_{6527}$。在实际中，通常使用一个专门的函数来单独查找矩阵$E$的某列，而不是使用通常的矩阵乘法来做。 2.5 学习词嵌入 (Learning Word Embeddings)假如构建一个语言模型，并且使用神经网络来实现。如预测“I wang a glass of range —”,然后预测下一个词。对于每一个单词使用嵌入向量，然后输入到神经网络中，则可以很好实现词嵌入。如每个单词是300维度，使用前四个单词则是1200的维度输入到神经网络，再经过softmax预测下一个单词。同时你还可以使用6个单词，所以是6x300的维度。这个固定的窗口是算法的超参数。这中算法能很好的学习词嵌入。 其它方式的更加简单的方法是使用不用类型的上下文，使用 目标词的前后各两个词作为上下文，如果目标不是学习语言模型本身的话，可以选择其它上下文也可以使用上下文一个单词，这就是一种Skip-Gram模型的思想，也能得到很好的效果。 2.6 Word2Vec论文：Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013. 在Skip-gram模型中，上下文不一定总是目标最近的$n$个单词，而是随机选择。比如在上下文词前后5个词内或10个词内选择目标词。构建监督学习问题，给定上下文词，预测正负10个词距内随机选择的目标词。我们的目标不是解决监督学习问题本身，而是想要使用它来学习一个好的词嵌入模型。对于一个单词，先构建one-hont向量，然后使用嵌入矩阵得到词向量，$e{c}=EO{c}$。然后将向量$e{c}$喂入一个softmax。$Softmax:p\left( t \middle| c \right) = \frac{e^{\theta{t}^{T}e{c}}}{\sum{j = 1}^{10,000}e^{\theta{j}^{T}e{c}}}$$\theta{t}$是一个与输出$t$有关的参数，即某个词$t$和标签相符的概率是多少。损失函数：$L\left( \hat y,y \right) = - \sum{i = 1}^{10,000}{y{i}\log \hat y{i}}$ 对于词汇过大，softmax计算过慢有两种解决办法，1.分层softmax分类器2.负采样。 模型中对于上下文单词的采样并不是随机抽取，否则 the、of、a、and等词出现相当频繁，而是采用不同的分级平衡。论文中提到了两种模型：Skip-Gram和CBOW，Skip-Gram使用更多些。 2.7 负采样 (Negative Sampling)论文： Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119. 不使用分层softmax，而是使用负采样方式。即构建新的监督学习，给定一堆单词，比如orange和juice，预测是否为一对上下文词-目标词(context-target)。在上个例子中，orange和juice就是一个正样本。再选择k个负样本，即使抽取的词再目标词的范围内也无所谓。k的取值：数据集越大k越小，选择5-20比较好。 当输入词orange，即词6527，得到嵌入向量$e_{6527}$，就得到了10000个可能的逻辑回归分类任务。但我们不在使用上面提到的10000类分类器，而是选择训练其中的$k+1$个，这样计算成本更低。 还有一个细节是负样本的采样过程，既不选择随机抽取，也不通过词频才采样。而是选择下面方式：$P\left( w{i} \right) = \frac{f\left( w{i} \right)^{\frac{3}{4}}}{\sum{j = 1}^{10,000}{f\left( w{j} \right)^{\frac{3}{4}}}}$ 2.8 GloVe 词向量 (GloVe Word Vectors)NLP领域的Glove算法，使用不多，但研究的人页不少。GloVe代表用词表示的全局变量（global vectors for word representation），如果使用$X{\{ij\}}$来表示单词$i$在单词$j$的上下文出现的次数，那么这里的$i$和$j$功能是和$t$ $c$是一样的。如果将上下文定义为左右范围的话，很明显会得到对称关系，但如果规定是单侧方向，就不对称了。不过对于GloVe算法，我们可以定义上下文和目标词为任意两个位置相近的单词，假设是左右各10词的距离，那么$X{\{ij\}}$就是一个能够获取单词$i$和单词$j$出现位置相近时或是彼此接近的频率的计数器。 GloVe模型做的就是进行优化，我们将他们之间的差距进行最小化处理：$\text{mini}\text{mize}\sum{i = 1}^{10,000}{\sum{j = 1}^{10,000}{f\left( X{\{ij\}} \right)\left( \theta{i}^{T}e{j} + b{i} + b{j}^{‘} - logX{\{ij\}} \right)^{2}}}$ 最后，一件有关这个算法有趣的事是$\theta$和$e$现在是完全对称的，所以那里的$\theta{i}$和$e{j}$就是对称的。如果你只看数学式的话，他们（$\theta{i}$和$e{j}$）的功能其实很相近，你可以将它们颠倒或者将它们进行排序，实际上他们都输出了最佳结果。因此一种训练算法的方法是一致地初始化$\theta$和$e$，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值，所以，给定一个词$w$，你就会有$e{w}^{(final)}= \frac{e{w} +\theta_{w}}{2}$。因为$\theta$和$e$在这个特定的公式里是对称的，而不像之前视频里我们了解的模型，$\theta$和$e$功能不一样，因此也不能像那样取平均。 最后，词嵌入维度的可解释性很差 2.9 情感分类 (Sentiment Classification)情感分类是NLP的重要模块之一，最大的挑战可能是标记的训练集可能没有那么多。比如对一个餐馆的评价，输入$x$是一段文本，输出$y$是要预测的相应情感，分为五颗星。对于情感分类任务，训练集从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集时。 算法一：将语句用one-hot表示，并转换成词向量，然后将语句的词向量进行求和或求平均，最后将这些特征输入到一个softmax分类器，谭厚输出$\hat{y}$。根据输出结果来进行星级的确定。但是这个算法有个一缺点：”Completely lacking in good taste, good service, and good ambiance.”，但是good这个词出现了很多次，有3个good，如果你用的算法跟这个一样，忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，你最后的特征向量会有很多good的表示，你的分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价。 算法二：我们首先取这条评论，”Completely lacking in good taste, good service, and good ambiance.”。找到每一个one-hot，向量，乘以词嵌入矩阵$E$，得到嵌入表达$e$，然后输入到RNN。RNN能够考虑词序问题。这样的算法，会得到一个很合适的情感分类算法。如果词嵌入是一个在更大数据集训练的，这样效果会更好。 2.10 词嵌入除偏 (Debiasing Word Embeddings)论文：Man is to computer programmer as woman is to homemaker?Debiasing word embeddings(Bolukbasi et.al, 2016)这里的bias指的是学习到的知识上的偏见，比如性别歧视，种族歧视等。比如下面这种情况：Man : Woman as King : QueenMan : Computer_programmer as Woman : Homemaker Wrong!Father : Doctor as Mother : Nurse Wrong! 以性别歧视为例子，首先看如何辨别与这个偏见相似的趋势： 对于性别歧视，首先得到$e{\text{he}}-e{\text{she}}$，然后将$e{\text{male}}-e{\text{female}}$，然后将这些值取平均（上图编号2所示），将这些差简单地求平均。这个趋势（上图编号3所示）看起来就是性别趋势或说是偏见趋势，然后这个趋势（上图编号4所示）与我们想要尝试处理的特定偏见并不相关，因此这就是个无偏见趋势。然后偏见趋势看成1D子空间，无偏见趋势就会使299D子空间。当然可以使用更复杂的算法SUV(奇异值分解)来描述偏见趋势的维度大于1. 中和步骤。像grandmother、grandfather、girl、boy、she、he，他们的定义中本就含有性别的内容，不过也有一些词像doctor和babysitter我们想使之在性别方面是中立的。像doctor和babysitter这种单词我们就可以将它们在这个轴（上图编号1所示）上进行处理，来减少或是消除他们的性别歧视趋势的成分，也就是说减少他们在这个水平方向上的距离（上图编号2方框内所示的投影），所以这就是第二个中和步。 均衡步。我们想要确保的是像grandmother和grandfather这样的词都能够有一致的相似度，或者说是相等的距离，和babysitter或是doctor这样性别中立的词一样。 参考资料：针对性别特定词汇的均衡算法 第三周 序列模型和注意力机制3.1 基础模型 (Basic Models)在这一周，你将会学习seq2seq（sequence to sequence）模型，从机器翻译到语音识别，它们都能起到很大的作用，从最基本的模型开始。之后你还会学习集束搜索（Beam search）和注意力模型（Attention Model），一直到最后的音频模型，比如语音。 如何训练一个网络来输入序列$x$和输出序列$y$呢？首先建立一个编码网络(encoder network)，是可以是GRU或LSTM组成的RNN网络。每次只向网络输入一个法语单词，序列接收完成后，得到一个表示序列X的向量。然后建立解码网络(decoder network)，用来输出翻译后的英文单词，一直输出到序列结尾或句子结尾标记。类似语言模型合成文本。 同样的可以应用到图片描述问题，将一张图片输入到AlexNet网络，去掉最后的softmax层，接一个RNN网络输出文本序列。 这两种模型都非常有效，细节之后的课程继续讲。 3.2 选择最可能的句子(Picking the most likely sentence)seq2seq机器翻译模型和之前的语言模型有很多相似之处，但也有重要的区别。 可以把机器翻译模型想象成建立一个条件语言模型。语言模型是估计一个句子的可能性，可以理解$x^{}$是一个全为0的向量，然后$x^{}$、$x^{}$等都等于之前所生成的输出，这就是所说的语言模型。 机器翻译模型中，用绿色标识encoder，紫色标识decoder。与语言模型不同在于，encoder网络会计算一系列向量，并输入到decoder中。decoder并不是以零向量开始，所以把它叫做条件语言模型(conditional language model)。换句话说，你将估计一个英文翻译的概率，所以它是一个语言模型。 现在，假如通过模型来讲法语翻译成英文,通过模型会得到各中英文翻译的可能性。显然，你不想得到随机的输出，所以需要一个算法来找到合适的输出$y$，使得条件概率最大。常用的算法便是束搜索(Beam Search)。 这里需要主义的是和贪心算法作比较，当使用贪心算法的时候，是每次选择概率最大的单词，以下面翻译句子为例：Jane is visiting Africa in September.Jane is gong to be visiting Africa in September.英语中 going更常见， 但第二个句子并没有第一个句子翻译的好。所以$p(y|x)$模型并不是最好的选择。应该以句子为整体来考虑。 另外一点，假设你的字典有10000个单词，翻译的句子假如有10个单词长度。那么组合10,000的10次方这么多，数量非常大。所以近似的算法是尽力去挑选出句子$y$使得条件概率最大化，不一定成功，但已经足够了。 最后总结一下，在本视频中，你看到了机器翻译是如何用来解决条件语言模型问题的，这个模型和之前的语言模型一个主要的区别就是，相比之前的模型随机地生成句子，在该模型中你要找到最有可能的英语句子，最可能的英语翻译，但是可能的句子组合数量过于巨大，无法一一列举，所以我们需要一种合适的搜索算法，让我们在下节课中学习集束搜索。 3.3 集束搜索 (Beam Search)语音识别，机器翻译等，我们想要的都是最好的而不是随机选择一个结果。集束搜索就是为了解决这个问题。 “Jane visite l’Afrique en Septembre.”（法语句子），我们希望翻译成英语，”Jane is visiting Africa in September”.（英语句子）。假如词汇表有10000个单词，然后利用编码网络(绿色)和解码网络(蓝色)来评估di第一个单词的概率。即给定$x$，第一个输出$y$的概率是多少$P(y^{}|x)$贪婪算法只选取最可能的一个，而集束算法有一个参数B，叫做集束宽(beam width)。以3为例，选取最大概率的三个单词并记录概率，假如是in, jane, september。 第二步:针对每一个单词，继续求解词汇中每个单词出现在第二个位置的概率。以in为例，绿色编码，蓝色解码。同时$y^{}$设为单词in，来计算$y^{}$。我们在第二步更关心的出找到最可能的第一个单词和第二个单词对，而不是仅仅第二个单词最大的概率。编号8表示第一个单词的概率，乘以第二个单词的概率(编号9，可以从网络编号10中得到)，最后就得到了第一个和第二个单词对的概率(编号7)。接下来对第二个单词jane继续求解与字典中各单词组成单词对的概率。同样对第三个三次september进行相同的操作。针对第二个单词有10000中不同的选择，集束宽度为3，所以最终会有30000个可能的结果，然后对这30000个结果进行评估，选取前三个结果。这时候可能选取的是in september、jane is、jane visits，这时候可以看到，第一个单词的选择只有两种可能，去掉了september 。接下的步骤与第二步一样，重复这个过程最终得到Jane visits africa in september这个句子，终止在句尾符号(编号8)。集束算法每次只考虑3个可能的结果，当集束宽为1时候就成了贪婪算法了。集束算法会比贪婪算法搜索更好的输出结果。 3.4 改进集束搜索(Refinements to Beam Search)前面讲到束搜索就是最大化这个概率，这个乘积就是$P(y^{&lt; 1 &gt;}\ldots y^{&lt; T{y}}|X)$，可以表示成:$P(y^{}|X)$ $P(y^{&lt; 2 &gt;}|X,y^{&lt; 1 &gt;})$ $P(y^{&lt; 3 &gt;}|X,y^{&lt; 1 &gt;},y^{&lt; 2&gt;})$…$P(y^{&lt; T{y} &gt;}|X,y^{},y^{}\ldots y^{&lt; T_{y} - 1 &gt;})$这些概率都小于1，很多乘在一起就可能造成数值下溢(numerical underflow)，因此，我们在实践中会取$log$值，这样数值会更加稳定，所以在实际工作中，我们记录的是概率的对数和，而不是概率的乘积。 还有一点就是，哪怕采取$log$操作，每一项都是小于1的，因此模型会对短句有偏好，因为越长相乘目标函数的值越小。因此要选择长度归一化：$\alpha$是一个超参数，为0时候完全没有归一化，为1的时候完全归一化，在实践中通常选取0.7，虽然没有理论支持，但效果还不错，你自己以后的项目可以来调节这一参数。有时候也叫这个目标函数为归一化的对数似然目标函数（a normalized log likelihood objective）。 其次，超参数集束宽B也需要调节，同样，B越大，计算成本越大，而且收益由大到小。对于不同的应用和特定的领域来说，10-3000都是可能的。 3.5 集束搜索的误差分析 (Error analysis in beam search)集束搜索是一种近似搜索算法(an approximate algorithm )，也叫启发式搜索(a heuristic search algorithm)。将误差分析和集束搜索结合起来。法语：Jane visite l’Afrique en septembre人工翻译：Jane visits Africa in September 记为$(y^*)$算法翻译：Jane visited Africa last September 记为$(\hat{y})$这个例子中，算法翻译很差，偏离了含义。你的模型有两个主要部分，一个是神经网络或序列到序列模型(seq2seq)，它实际是编码器和解码器。另一个是技术算法，以某个集束宽B运行。如何判断是那个部分出错呢？RNN是计算$P(y|x)$。我们来计算$P(y^*|x)$和$P(\hat y|x)$，然后比较这两个哪个更大，所以就会有两种情况。 $P(y^|x)$ 大于$P(\hat y|x)$。这说明集束算法并没有搜索到正确的结果，所以是集束算法出错了。 $P(y^|x)$ 大于$P(\hat y|x)$。是RNN出错了，这里没有涉及长度归一化(length normalization)，如果使用需要比较的是归一化的最优目标函数。 所以误差分析过程是，首先遍历开发集，然后找出错误并标记各个错误是属于RNN还是集束搜索问题。最后根据分析结果对模型算法进行改进。 3.6 Bleu得分(选修)论文(看)：Bleu:A method for automatic evaluation of machine translation. Papineni et.al., 2002 机器翻译(machine translation)的一个难题是一个法语句子可以有多种英文翻译，并且同样好。这样改如何评价一个机器翻译系统？常用的解决办法就是BLEU得分。BLEU代表bilingual evaluation understudy (双语评估替补)。它背后的理念是观察机器生成的翻译，然后看生成的词是否出现再至少一个人工翻译参考中。 例子：法语：Le hcat est sur le tapis.参考(Reference)1: The cat is on the mat.参考(Reference)2: There is a cat on the mat.机器翻译(MT): the the the the the the the. 这时候，精度(Precision)为$\frac{7}{7}$，因为MT的输出共有7个单词(分母)，其中7个单词出现在两个参考(Reference)中。而修改后的精度(Modified precision)为$\frac{2}{7}$，对于单词the,在两个参考(Reference)中出现的次数分别为2，1。而MT中出现次数为7，超过了最大参考中出现，因此进行截断。所以改良后的精确度评估（the modified precision measure）得分为$\frac{2}{7}$。 目前为止，我们关注的都是单独的单词。我们定义以下二元词组(bigrams)，是指相近的两个单词。同样，可能会考虑一元词组(unigram)，三元词组(trigrams)等。我们首先列出所有的二元词组，然后统计次数，以及截断统计次数可以得到下图：所以二元词组的改良精度为$\frac{4}{6} = \frac{2}{3}$. 现在我们将他们公式化： P_1 = \frac{\sum_{unigram\in y} count_{clip}(unigram)}{\sum_{unigram\in \hat y} count(unigram)}P_n = \frac{\sum_{ngram\in y} count_{clip}(ngram)}{\sum_{ngram\in \hat y} count(ngram)}如果翻译完全一致的话，那么$P_1$、$P_n$的值为1。 细节：我们来组合以下构成最终的BLUE得分。$Pn$就是$n$元词组这一项的BLEU得分，也是计算出的$n$元词组改良后的精确度。首先计算得到$P_1$，$P_2$， $P_3$，$P_4$。然后求和取平均。BLEU定义为$exp (\frac{1}{4}\sum\limits{n=1}^{4}{P_n})$，它是严格单调递增的。我们还会使用一个额外的BP(brevity penalty)惩罚项，用于调整输出偏向短句翻译。 拥有单一实数评估指标(a single real number evaluation metric)是非常重要的。BLEU对机器翻译来说具有革命性的意义。它的开源算法有很多。BLEU得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似。不过它并没有用于语音识别（speech recognition）。因为在语音识别当中，通常只有一个答案。 3.7 注意力模型直观理解 (Attention Model Intuition)论文：Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate[J]. Computer Science,2014.本节主要讲述Attention model的直观理解，细节在下节。注意力思想已经成为深度学习中最重要的思想之一。 假如翻译的句子非常长，绿色编码器的任务是读取整个句子，然后记忆。紫色神经网络(解码器)将生成英文。而人工翻译是看一点，翻译一点。这个编码器中，它对短语句非常好，BLUE得分会很高，但当句子长度达到30，40时候，表现会非常差。神经网络记忆非常长的句子也是很困难的。所以采用注意力模型，每次只翻译一部分。不会出现长句子得分严重下滑情况。以”Jane visite l’Afrique en septembre”为例子，使用双向RNN可以得到$\hat y^{},\hat y^{},\hat y^{}$等。现在我们构建一个新的注意力机制的单向RNN，首先去掉所有的$Y$输出。然后为了区分感知器(activations)$a^{}$，我们使用$S^{}$来标识RNN的隐藏状态。来看生成第一个单词，我们需要看句子的那一部分呢？应该先看第一个或它附近的单词，因此，用$a{}$来标识生成第一个单词时，原句中第一个单词的注意力权重。$a^{}$它告诉我们当你尝试去计算第一个词Jane时，我们应该花多少注意力在输入的第二个词上面。同理这里是$a^{}$，接下去也同理。然后RNN向前进一次生成一个词，知道最终生成EOS。注意力权重，即$a^{}$告诉你，当你尝试生成第$t$个英文词，它应该花多少注意力在第$t$个法语词上面。当生成一个特定的英文词时，这允许它在每个时间步去看周围词距内的法语词要花多少注意力。 个人理解：双向RNN用来对法语单词进行特征标识，新的RNN中，$S$与$A$效果一致，但输入处的$X$不再是一个单词，而是上下文$C$，$C$是注意力权重与双向RNN的输出(不同时间步的特征值)结合的。细节见下一节 3.8 注意力模型 (Attention Model)上节提到注意力模型如何让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译。 首先有一个双向RNN来进行计算每个词的特征。每个单元既可以是GRU，也可以是LSTM，LSTM可能会更常见。一共有五个时间步，前向激活值和后向激活值都有六个。我们使用$t^\prime$来索引法语单词。用$a^{}$来标识时间步上的特征向量。然后设置注意力权重$a{&lt;1,t’&gt;}$。注意力权值和为1。之后计算上下位$C$，其中$C^{} = \sum_{t’}\alpha^{&lt;1,t’&gt;} a^{t’}$。每一个时间步的输入为上下文$C$与上个时间步的输出。 接下来的问题就是该如何定义注意力权重了。用一个小的神经网络来输出各项权重，softmax可以保证权重和为1。相信反向传播，相信梯度下降。 这个算法有一个缺点是它需要花费三次方时间，即$O(n^3)$。如果你有$T_x$个输入单词和$T_y$个输出单词，于是注意力参数的总数就会是$T_x\times T_y$，所以这个算法有着三次方的消耗。但是在机器翻译的应用上，输入和输出的句子一般不会太长，可能三次方的消耗是可以接受，但也有很多研究工作，尝试去减少这样的消耗。 3.9 语音识别 (Speech recognition)将seq2seq应用到语音识别，了解就好。 Attention Model for recognition CTC cost for speech recognition 3.10 触发字检测 (Trigger word detection)可以将触发后的多个label 0改为1，缓解不平衡。 3.11 Thank you! Andrew Ng参考深度学习笔记-黄海广]]></content>
      <categories>
        <category>深度学习[吴恩达]</category>
      </categories>
      <tags>
        <tag>Andrew Ng</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-卷积神经网络]]></title>
    <url>%2F2018%2F10%2F25%2F04-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[04.卷积神经网络00? 动态性策略如何与DL结合(预测之上的决策系统)？第一周 卷积神经网络1.1 计算机视觉 (Computer vision)将CV知识应用到新的领域，催生新的网络结构目标检测 风格迁移一个64x64的图像，会有12288个维度，一个1000x1000的图像，会有3M个维度，假如第一层神经元有1000个，则$w^{[1]}$有3B个参数，这太大了！故需要卷积 1.2 边缘检测示例 (Edge detection example)*是卷积操作的标准符号对卷积不了解可详细看这个视频参考论文 1.3 更多边缘检测内容 (More edge detection)固定的卷积核：Sober过滤器： - - - 1 0 -1 2 0 -2 1 0 -1 Scharr过滤器： - - - 10 0 -3 10 0 -10 3 0 -3 另一种思想：将9个数字作为参数进行学习，是CV的有效思想之一。 1.4 Padding普通的卷积操作 输出减少，$output: (n-k)+1$，输出图片变小 丢失了图像边缘的大部分信息 对原始图片进行填充，如用0填充，则输出(n+2p-k)+1=6，图像保持不变。Valid convolution:不使用填充，$n\times n * f\times f \rightarrow (n-f+1) \times (n-f+1)$Same convolution:使用填充，保持图片大小不变。$n+2p-f+1 \times n+2p-f+1$ 继续求解可得$p = \frac{f-1}{2}$卷积核一般为奇数 1.5 卷积步长 (Strided convolution)输出图片维度(向下取整)：$\lfloor \frac{n+2p-f}{s}+1 \rfloor$按机器学习惯例，不使用翻转操作，技术上讲，可能叫互相关更好，深度学习中叫做卷积操作。 1.6 三维卷积 (Convolutions over volumes) 输出：$n\times n \times n_c * f\times f \times n_c \rightarrow n-f+1 \times n-f+1 \times n_c’$ $n_c$是channel， $n_c’$是卷积核的数量 1.7 单卷积层网络 (One layer of convolution network) 假如有10个3x3x3的卷积核，则该层的参数为280个。而无论你的输入的图片大小是多少，参数个数是不变的，即使用10个特征提取。卷积之后得到feature_map，与偏置项$b$相加，得到$z^{[l]}$，然后再应用激活函数得到$a^{[l]}$ $f^{[l]}$是卷积核的大小，$n_c^{[l]}$是卷积核的个数$p^{[l]}$是padding, $s^{[l]}$是步长输入为上一层的输出，所以为$n_H^{[l-1]} \times n_w^{[l-1]} \times n_c^{[l-1]}$卷积计算：$n_H^{[l]} = \frac{n_H^{[l-1]}+2p^{[l]}-f^{l}}{s^{[l]}} + 1$，同理$n_w^{[l]}的计算$每一个卷积核(过滤器)维度：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} $，$n_c^{[l-1]}$是上一层输出的channel。激活函数：$a^{[l]} = n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$权重Weights：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$，$n_c^{[l]}$是卷积核的个数偏置项bias：$n_c^{[l]}\rightarrow(1,1,1,n_c^{[l]})$ 1.8 简单卷积网络示例 (A simple convolution network example)假设输入图片大小：$n{H}^{[0]} = n{W}^{[0]}=39$，$n_{c}^{[0]} =3$第一层卷积：10个$f^{[1]} = 3$，$s^{[1]} = 1$，$p^{[1]} =0$，则$a^{[1]}=$37×37×10第二层卷积：20个$f^{\left\lbrack 2 \right\rbrack}=5$，$s^{\left\lbrack 2 \right\rbrack}=2$，$p^{\left\lbrack 2 \right\rbrack} = 0$，则$a^{\left\lbrack 2 \right\rbrack}=$17x17x20第三层卷积：40个$f^{\left\lbrack 3 \right\rbrack}=5$，$s^{\left\lbrack 3 \right\rbrack}=2$，$p^{\left\lbrack 3 \right\rbrack} = 0$，则$a^{\left\lbrack 3 \right\rbrack}=$7x7x40=1960最后处理成向量，接softmax或logistic回归函数 典型的卷积神经网络层： Convolution (Conv) Pooling (POOL) Fully connected (FC) 1.9 池化层 (Pooling layers)Max pooling:如果在卷积核中提取到某个特征，则保留其最大值，如果没有提取到，最大值也很小。仅是直观理解，但实验效果良好。步长和大小不需要学习。p一般为0Average pooling:不太常用。 1.10 卷积神经网络示例（Convolutional neural network example）类似于Le-Net-5逐步讲解各卷积层的输出维度有的文献将卷积和池化作为一层神经网络。随着网络的加深，高度$n{H}$和宽度$n{W}$通常都会减少，而通道数量会增加。 在神经网络中，另一种常见模式就是一个或多个卷积后面跟随一个池化层，然后一个或多个卷积层后面再跟一个池化层，然后是几个全连接层，最后是一个softmax。这是神经网络的另一种常见模式。 有几点要注意，第一，池化层和最大池化层没有参数；第二卷积层的参数相对较少，前面课上我们提到过，其实许多参数都存在于神经网络的全连接层。观察可发现，随着神经网络的加深，激活值尺寸会逐渐变小，如果激活值尺寸下降太快，也会影响神经网络性能。示例中，激活值尺寸在第一层为6000，然后减少到1600，慢慢减少到84，最后输出softmax结果。我们发现，许多卷积网络都具有这些属性，模式上也相似。 1.11 为什么使用卷积神经网络 (Why convolutions?)卷积神经网络的优点： 参数共享：特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，共享特征选择器。 稀疏连接：如32x32x3 = 3072 使用6个5核卷积得到28x28x6=4704，如果用传统的全连接则需要3072x4704=14M参数，而卷积只需要(5x5+1)x6=156个参数。同时，映射后的feature_map某一像素点只和整张图片中的25个像素点有关联，所以是稀疏连接。 Parameter sharing: A feature detector (such as a vertical edge detector) that’s useful on one part of the image is probably useful in another part of the image.Sparsity of connections: In each layer, each output value depends only on a small number of inputs. 卷积网络可以使用任何的代价函数$J$，以及其它梯度下降算法(Momentum,RMSprop,Adam) 第一周作业卷积神经网络的反向传播 第二周 深度卷积网络：实例探究 (Deep convolutional models: case studies)2.1 为什么要进行实例探究 (Why look at case studies?)通过他人案例学习建立卷积神经网络的直觉与技巧。尝试读计算机视觉(CV)论文提纲： 经典网络 LeNet-5 1980 AlexNet VGG ResNet 训练了深达152层的网络 Inception 2.2 经典网络 (Classic networks) LeNet-5 离线阅读论文发表于1998年，当时使用的平均池化，也没有采用padding。LeNet-5在全连接最后一层使用的不是softmax，而是另一种，现在很少用到的分类器。此外，当时使用的激活函数为Sigmoid和tanh，而不是ReLu。PPT内容大部分来自于论文II和III，精读第II段，泛读第III段。 AlexNet 离线阅读AlexNet使用227x227x3(原文使用224x224x3)图片作为输入，部分卷积层使用了padding，还是用了复杂的GPU计算，激活函数选取的ReLu，最后一层使用softmax，同时使用了局部响应归一化层”（Local Response Normalization），即LRN层,LRN效果没多大作用。 VGG-16VGG-16 虽然包含16个看似很多的的网络层，但结构并不复杂。首先卷积核(过滤器)的数量，64-128-256-512-512。 如果你对这些论文感兴趣，我建议从介绍AlexNet的论文开始，然后就是VGG的论文，最后是LeNet的论文。虽然有些晦涩难懂，但对于了解这些网络结构很有帮助。 2.3 残差网络 (Residual Networks (ResNets))ResNet论文中将逐层传播网络定义为plain network，而ResNet的不同就是添加了”short cut/skip connection”，形成了Residual block。示意图：在第二层线性变化后，非线性激活前，添加一样$a^{[l]}$，即$a^{[l+1]} = g(z^{[l+1]} + a^{[l]})$ 整个网络示意图：残差网络解决了梯度消失或爆炸，允许网络结构更加深层。 残差网络为什么有效 (Why ResNet work)设想一个神经网络的输出为$a^{\left\lbrack l \right\rbrack}$， 我们在它后面添加带“residual block”的两层网络。此时，$a^{\left\lbrack l + 2\right\rbrack} = g(z^{\left\lbrack l + 2 \right\rbrack} + a^{\left\lbrack l\right\rbrack})$。如果使用L2正则化，那么权重$W^{[l+1]}$会被压缩，$b$偶尔也会被压缩，注意$W$，如果$W^{\left\lbrack l + 2 \right\rbrack} = 0$，为方便起见，假设$b^{\left\lbrack l + 2 \right\rbrack} = 0$，这几项就没有了，因为它们（$W^{\left\lbrack l + 2 \right\rbrack}a^{\left\lbrack l + 1 \right\rbrack} + b^{\left\lbrack l + 2\right\rbrack}$）的值为0。最后$ a^{\left\lbrack l + 2 \right\rbrack} = \ g\left( a^{[l]} \right) = a^{\left\lbrack l\right\rbrack}$，因为我们假定使用ReLU激活函数，并且所有激活值都是非负的，$g\left(a^{[l]} \right)$是应用于非负数的ReLU函数，所以$a^{[l+2]} =a^{[l]}$。 事实上，残差块学习这个恒等式函数并不难。从上面可以看出，即使网络添加了两层，但它的效率比没有降低，所以将残差块放到网络中间还是末尾，都不影响网络。 此外，如果这些隐层能学到一些拥有东西，比恒等式表现还好，那网络可以提升效果，或者说至少不会降低。 一个细节：ResNets使用了需要SAME卷积，保证了$a^{\left\lbrack l\right\rbrack}$与残差块的维度一致，当不一致的时候，在残差块中添加一个科学系参数$W_s$，即$a^{\left\lbrack l + 2\right\rbrack} = g(z^{\left\lbrack l + 2 \right\rbrack} + W_s \cdot a^{\left\lbrack l\right\rbrack})$ 论文中，使用卷积-卷积-卷积-池化 -卷积-卷积-卷积-池化….最后使用softmax 2.5 网络中的网络以及1x1卷积 (Network in Network and 1x1 convolutions)对于通道为1的6x6图片，使用1x1卷积核似乎作用并不大，仅仅是对二维数组进行了扩大或缩放。 但对于通道为32的6x6输入来说就不一样了，这时候我们使用一个1x1的卷积核(实际维度为1x1x32，卷积核通道与输入通道一致)。这时候的卷积操作，是将输入中的一个1x1x32的切片，乘以1x1的卷积核中32个不同的权重再求和，最后应用ReLU激活函数。当然这是1个卷积核，如果是32个卷积核，则效果如下： 这种方法通常称为1×1卷积，有时也被称为Network in Network，在林敏、陈强和杨学成的论文中有详细描述。虽然论文中关于架构的详细内容并没有得到广泛应用，但是1×1卷积或Network in Network这种理念却很有影响力，很多神经网络架构都受到它的影响，包括下节课要讲的Inception网络。 这时候输入输出的通道数量保持一致，因此改变卷积核的数量可以进行通道的压缩与扩充(池化仅仅压缩图片的高和宽) 2.6 Google Inception network 简介Inception层或Inception网络可以用来代替人工来确定卷积层中的过滤器类型(1x1?3x2?5x5?)。 从图中可以发现会设计大量的计算，我们先来看一下5x5的卷积计算成本。对于28x28x192的输入，采用32个5x5(x192)的卷积进行操作，计算量为 28x28x32 x 5x5x192 = 120M(120422400) 32个核 每个核参数为 5x5x192 做一次卷积需要的stride为 28x28 所以计算成本为 32x5x5x192x28x28 而考虑下面的结构： 对于28x28x192的输入，我们想要得到和之前一样的输出维度28x28x32，先使用16个1x1(x192)的核卷积，再使用32个5x5(x16)的核卷积。其计算量为 28x28x16 x 192 = 2.4M 28x28x32 x 5x5x16=10.0M,总计为12.4M，与上面的结果相比，缩小了10倍。 所以$ 1\times1 $卷积核作为“bottleneck layer”的过渡层能够有效减小卷积神经网的计算成本。事实证明，只要合理地设置“bottleneck layer”，既可以显著减小上层的规模，同时又能降低计算成本，从而不会影响网络的性能。 2.7 Inception 网络通过计算成本的对比，可以发现使用1x1的卷积核能够简化计算量，因此可以构建Inception module： 有了Inception module，则Google的gooLeNet网络就很好理解了： 该模型使用多个Inception Module，此外模型多两个softmax输出，能对网络进行调整，并防止过拟合。Inception网络还有很多新版本，如Inception V2、V3以及V4，还有一个版本引入了跳跃连接(skip connection)的方法，有时也会有特别好的效果。 2.8 使用开源的实现方案 (Using open-source implementations)熟练使用GitHub，如REstNets实现 2.9 卷积网络的迁移学习 (Transfer Learning)如果建立自己的CV检测器，可以下载神经网络的开源实现，不仅包括代码，还包括权重。这样，修改最后一层的softmax层，freeze前面的神经网络，然后在你的数据集上进行训练。 一个经验是，你的数据量越大，你需要freeze的层数越小，甚至仅仅把它们来当作初始化参数。CV中，迁移学习是很值得考虑去做的。 2.10 数据增强 (Data argumentation)数据扩充方式： 镜像(Mirroring) 随机裁剪(Random Cropping) 其它实现比较复杂的方式：旋转，扭曲， 色彩转换，使用PCA颜色增强(AlexNet有细节，也有其它开源实现) CPU并行：几个线程或进程做数据增强，其它CPU或GPU训练网络。 2.11 计算机视觉现状 (The state of computer vision)语音识别、图像识别、目标检测任务现所有的数据一个比一个少。数据越少的任务可能越需要手工工程。 在机器学习应用时，学习算法有两种知识来源。 Labeled data Hand engineered features / network architecture / other components 在缺乏数据的情况下，获取良好的表现方式还是花更多时间进行架构设计，或者在网络架构上花费更多时间。 Benchmark 基准测试，Benchmark是一个评价方式，在整个计算机领域有着长期的应用。维基百科上解释：“As computer architecture advanced, it became more difficult to compare the performance of various computer systems simply by looking at their specifications.Therefore, tests were developed that allowed comparison of different architectures.”Benchmark在计算机领域应用最成功的就是性能测试，主要测试负载的执行时间、传输速度、吞吐量、资源占用率等。 在基准/竞赛中提升效果 集成 Ensembling Train several networks independently and average their outputs 多折验证 Muti-crop at test time Run classifier on multiple versions of test images and average results 集成大概可以提高1%或2%，但消耗时间。Muti-crop 是说将你的测试图片进行多次裁剪，每张都进行预测，然后综合考虑结果。这两种方式在实际生产情况下，很少考虑，在基准测试和竞赛上做得很好。 最后，其他人可能已经在几路GPU上花了几个星期的时间来训练一个模型，训练超过一百万张图片，所以通过使用其他人的预先训练得模型，然后在数据集上进行微调，你可以在应用程序上运行得更快。当然如果你有电脑资源并且有意愿，我不会阻止你从头开始训练你自己的网络。事实上，如果你想发明你自己的计算机视觉算法，这可能是你必须要做的。 第三周 目标检测3.1 目标定位识别一张图片中是否有车的分类问题已经很熟悉了，现在还要输出车在图片中的位置，即定位分类问题Classification with Location。进一步，如果图片中包含多个物体需要定位，就是目标检测。 例：识别一张图片是1 行人 2 汽车 3 自行车 4 背景 类别。首先记图像左上角为(0,0)，右下角记(1,1)。图像的边框用bx,by,bh,bw来表示中心坐标点和宽高。这时候神经网络的输出向量可以定义为$[P_c,bx,by,bh,bw,c1,c2,c3]$。$pc$表示是前三类与否，如果为1，则$bx,by,bh,bw$来表示坐标，$c1,c2,c3$来表示具体的某一类。此外，损失函数也需要修改： L(\hat{y},y)=\begin{cases} \sum_i(\hat{y}_i-y_i)^2& \text{if} \ y_1=1\\\\ (\hat{y}_1-y_1)^2& \text{if} \ y_1=0\\ \end{cases}实际中，可以不对$c{1}$、$c{2}$、$c{3}$和softmax激活函数应用对数损失函数，并输出其中一个元素值，通常做法是对边界框坐标应用平方差或类似方法，对$p{c}$应用逻辑回归函数，甚至采用平方预测误差也是可以的。 3.2 特征点检测 (Landmark detection)如果需要检测64个人脸关键点，则可以使卷积神经网络输出为129个，第一个代表有无人脸，剩余的表示各个特征的坐标。 3.3 目标检测 (Object detection)使用卷积神经网络进行对象检测，采用是基于滑动窗口的目标检测算法。选用不同的窗口和步长会有一定的影响，太小计算量大，太大影响效果。不过计算成本得到了解决，见下。 3.4 卷积的滑动窗口实现 (Convolutional implementation of sliding windows)之前讲的滑动检测计算效率太低，其中一个原始是因为重复计算问题。而现在，不需要将图片每次切割单独放入神经网络去计算，可是将卷积神经网络的最后全连接层也改用为卷积层。输入也只需要输入原图片完整一次。 改写全连接层：假如输入图片是14x14x3，经过16的5x5卷积得到10x10x16，再经过池化得到5x5x16。连接两个400个节点的全连接层，输出四分类向量。改写成卷积层：前面一致，得到5x5x16特征后使用400个5x5的窗口可以得到1x1x400向量，再使用400个1x1的窗口，可以得到1x1x400向量，最后使用1x1的窗口可以得到1x1x4的向量。 一次计算假如训练数据为14x14x3，而测试图片数据为16x16x3，按切分的话，可以将16x16x3切分成四部分，然后每部分输入网络进行计算。但这样需要计算四次，且重复计算区域较大。可以将16x16x3图片直接输入网络，得到的2x2x4分别代表切割的四部分结果。 3.5 Bounding Box预测 (Bounding box predictions)参考论文：YOLO You Only Look Once: Unified real-time object detection 论文比较难懂。 上节讲到的滑动窗口卷积实现算法效率很高，但仍有一个问题，不能输出最精准的边界框。 YOLO(you only look once)算法： 假设输出维度为8：{px, bx, by, bh, bw, c1, c2, c3}。将原始100x100的图片划分为3x3的格子，对每个格子使用分配y标签(8维)。然后训练输出各个格子的y。最后整个输出为3x3x8的维度。 当然，你可以切分更细，使用19x19边框，这个单个边框内包含多个目标的可能性更小。 其中一个细节：bx,by,bw,bh都是[0,1]的，以每个边框的左上角为(0,0)坐标，右下角为(1,1)坐标。 3.6 交并集 (Intersection over union)目标检测算法的评估参数：交并集(lou)：$\frac{交集面积}{并集面积}$。一般lou&gt;0.5。这个阈值可以人为设置，很少小于0.5。lou衡量了两个边界框的重叠相对大小。 3.7 非极大值抑制(Non-max suppression)先假设对象检测中只有一种对象，但算法通常会检测出多次。如在19x19=361个格子检测，会得到很多格子检测包含目标，但很多检测的是同一个目标。非极大值抑制是说，对于同一个检测物体的多个检测边界，仅输出最大概率的，抑制其它边界输出。对于多目标类别检测，正确的做法是独立进行多次非极大值抑制。 3.8 Anchor Boxes当两个识别目标居于同一个格子时候，该如何处理呢？之前的目标检测都只能在一个格子里检测一个对象。以一个格子最多两个对象为例子，首先根据目标的特性人为规定两个Anchor Boxes，比如竖着的为人，横着的为车。然后格子的类别标签y不在是8维度，而是16维度。前8维度为Anchor Boxes1的标签，后八个维度为Anchor Boxes2的标签。 如何选择Anchor Boxes呢？可以使用K-Means对对象进行聚类，然后得到形状。 3.9 YOLO 算法将训练集图片分成3x3格子，需要检测三类对象：行人，汽车，摩托车。使用两个anchor box。第一个格子和第八个格子的标签如图。 在预测的时候，一个目标的情况很容易理解。而对于下图： 对于每一个grid call，得到两个预测边界框 去掉概率低的预测 对每个类别(行人，汽车，摩托车)使用非极值抑制算法。最后只得到最后一个预测。3.10 R-CNN首先得到候选区域，再进行CNN卷积识别。 R-CNN 使用图像分割算法，选取候选区域，然后使用滑动窗口方式卷积 Fast R-CNN 类似于第四节，不再单个框输入而是一次全部，但速度还是比较慢。 Faster R-CNN 使用CNN来选择候选区域。 第4周4.1 什么是人脸识别 (What is face recognition?) 人脸验证(Verification) 输入照片和姓名(ID) 输出图像是否为本人 人脸识别(Recognition) 拥有$K$个人脸的数据库 输入图像 输出人物ID如果他是$K$个人中之一 很显然，在人脸验证的任务中，准确率达到99%是可以接收的，但放到识别任务中，100个人则代表1%的失误状况，所以人脸识别需要较高的准确率。 人脸验证之所以困难，原因之一是要解决”一次学习(one-shot learning problem)“问题。 4.2 One-Shot学习 (One-Shot learning)如果识别人有四个人，使用ont-hot来表示输出，这种方式不太好，如果新加入一个人脸，则ont-hot维度需要改变。同时，一半人物只有一张照片，使用一张照片不能有效训练完成一个稳健的神经网络。 为了解决One-Shot问题，可以使用”similarity“ function。使神经网络学习一个$d$表示的函数，$d(img1, img2) = degreed\ of \ difference\ between\ images$。它以两张图片作为输入，然受输出两张图片的差异值。差异值小于某个阈值$\tau$，它是一个超参数，表明是同一个人。 4.3 Siamese网络 (Siamese network)论文：DeepFace closing the gap to human level performance Siamese网络，学习函数$d$来计算两张人脸的相似度。思想是，将人脸编码(映射)成固定维度向量(非one-hot)将$x^{(1)}$和$x^{(2)}$的距离定义为这两幅图片的编码之差的范数，$d( x^{( 1)},x^{( 2)}) =|| f( x^{( 1)}) - f( x^{( 2)})||_{2}^{2}$。如果$x^{(i)}$和$x^{(j)}$是同一个人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很小，相反很大。使用反向传播来学习网络参数。如果定义真正的目标函数呢？使用三元组损失函数 4.4 Triplet损失 (Triplet loss)论文：FaceNet: A Unified Embedding for Face Recognition and Clustering Anchor Positive Negative用三元组术语来说，对于一个Anchor图片，Positive图片和其是同一个人，距离更近，Negative非同一个人，距离更远。简写成$A$,$P$,$N$三元组损失，我们的目标是想要$|| f(A) - f(P) ||^{2}$，你希望这个数值很小，准确地说，你想让它小于等$f(A)$和$f(N)$之间的距离，或者说是它们的范数的平方（即：$|| f(A) - f(P)||^{2} \leq ||f(A) - f(N)||^{2}$）。（$|| f(A) - f(P) ||^{2}$）当然这就是$d(A,P)$，（$|| f(A) - f(N) ||^{2}$）这是$d(A,N)$，你可以把$d$ 看作是距离(distance)函数，这也是为什么我们把它命名为$d$。网络中的参数全为0的话，也是可以满足条件的，为了避免这个问题，添加一个超参数$\alpha$，代表间隔margin。最后我们可以定义损失函数： L(A,P,N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0)不难发现，当$|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha \ge 0$，则得到一个正的损失值。 相反，网络不会关心负值大小。 对于训练集，至少保证一个人有多张图片。训练的时候，可以随机选取照片来构成APN三元组，但使用相近图像的AP才能更好地训练网络。人脸识别模型可以选择其它公司或研究机构训练好的网络模型。 4.5 面部验证与二分类 (Face verification and binary classification)论文：DeepFace:Closing the gap to human-level performance in face verification 除了Triplet loss方法外，还可以把人脸识别看作二分类问题。选取一堆Siamese网络，将两章图片进行编码(映射)，然后输入逻辑回归单元如Sigmoid，进行输出1或0。此时损失函数为： \hat{y} = \sigma ( \sum_{k=1}^{128} \omega_i |f(x^{(i)})_k - f(x^{(j)})_k| + b) $f(x^{(i)})$是图片$x^{(i)}$的编码，$k$代表第$k$个元素。 这样就可以将其转化成二分类问题。 4.6 什么是神经风格转换 (What is neural style transfer)我将使用$C$来表示内容图像，$S$表示风格图像，$G$表示生成的图像。 4.7 什么是深度卷积网络？(What are deep ConvNets learning?)论文：Visualizing and Understanding Convolutional Networks 通过可视化卷积神经网络分析可以发现，层数越高，网络学习到的内容越复杂。 4.8 代价函数 (Cost function)代价函数包含两部分： $J_{\text{content}}(C,G)$，内容代价函数，用来度量生成图片$G$的内容与内容图片$C$的内容有多相似。 $J_{\text{style}}(S,G)$，风格代价函数，用来度量图片$G$的风格和图片$S$的风格的相似度。 最终的代价函数为($\alpha$ 是权重超参数)： J( G) = a J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)4.9 内容代价函数 (Content cost function)$J( G) = \alpha J{\text{content}}( C,G) + \beta J{\text{style}}(S,G)$内容代价函数： 选取隐层$l$来计算内容代价， use hidden layer l to compute content cost. 使用预训练的卷积神经网络，如VGG 使用$a^{[l]\lbrack C\rbrack}$和$a^{[l]\lbrack G\rbrack}$来表示两个图片$C$和$G$的$l$层的激活函数值。 如果这两个激活值相似，那么就意味着两个图片的内容相似。 内容代价函数求得就是两个图片之间$l$层激活值差值的平方和。 4.10 风格代价函数 (Style cost function)论文：A neural algorithm of artistic style 风格的数据表达：相关性(correlation)计算$l$层的输出，通道间的相关性。风格矩阵(style matrix)使用$a{i,\ j,\ k}^{[l]}$来表示隐层$l$中$(i,j,k)$位置的激活项，$i$，$j$，$k$分别代表该位置的高度、宽度以及对应的通道数。计算风格矩阵$G^{l}$，它是一个$n{c} \times n_{c}$的矩阵，同样地，我们也对生成的图像进行这个操作。 $G{kk^{‘}}^{[l][S]} = \sum{i = 1}^{n{H}^{[l]}}{\sum{j = 1}^{n{W}^{[l]}}{a{i,\ j,\ k}^{[l][S]}a_{i,\ j,\ k^{‘}}^{[l][S]}}}$ 用符号$i$，$j$表示下界，对$i$，$j$，$k$位置的激活项$a{i,\ j,\ k}^{[l]}$，乘以同样位置的激活项，也就是$i$,$ j$,$k’$位置的激活项，即$a{i,j,k^{‘}}^{[l]}$，将它们两个相乘。然后$i$和$j$分别加到l层的高度和宽度，即$n{H}^{[l]}$和$n{W}^{[l]}$，将这些不同位置的激活项都加起来。$(i,j,k)$和$(i,j,k’)$中$x$坐标和$y$坐标分别对应高度和宽度，将$k$通道和$k’$通道上这些位置的激活项都进行相乘。我一直以来用的这个公式，严格来说，它是一种非标准的互相关函数，因为我们没有减去平均数，而是将它们直接相乘。 同样计算生成图像的的风格矩阵：$G{kk^{‘}}^{[l][G]} = \sum{i = 1}^{n{H}^{[l]}}{\sum{j = 1}^{n{W}^{[l]}}{a{i,\ j,\ k}^{[l][G]}a_{i,\ j,\ k^{‘}}^{[l][G]}}}$ Gram matrix，所以用$G$来表示。 最后，整体的风格代价函数为两个风格矩阵的F范数的平方，可以乘以一个归一化常数，或者乘以一个超参数$\beta$就好了。 $J{style}^{[l]}(S,G) = \frac{1}{(2n_H^{[l]}n_W^{[l]}n_C^{[l]})^2} \sum_k \sum{k’}(G{KK’}^{[l][S]} - G{KK’}^{[l][G]})$ 最终，整体的代价函数为：$J(G) = a J{\text{content}( C,G)} + \beta J(S,G)$ 4.11 一维到三维推广 (1D and 3D generalizations of models)卷积操作可以应用到1D和3D上，1D可以使用序列模型，会在下次课程中讲解并对比。应用到3D，channel层是额外的。 参考深度学习笔记-黄海广]]></content>
      <categories>
        <category>深度学习[吴恩达]</category>
      </categories>
      <tags>
        <tag>Andrew Ng</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-结构化机器学习项目]]></title>
    <url>%2F2018%2F10%2F25%2F03-%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[03.结构化机器学习项目第一周1.1 为什么是ML策略经验：如何进一步优化模型 1.2 正交化正交化是指，在模型的调参过程中，尽量选择影响单一化的方法。 第一组按钮用来调节宽度，方法包括 - bigger network - Adam算法第二组按钮用来调节高度，方法包括 - 正则化 - 增加数据集第三组 - 增加数据集第二组 - 改变损失函数 - 改变开发数据集 一般不使用early stopping,它会同时影响前两步,违反正交化,使模型不好分析。 1.3 单一数字评估指标使用单一数字(实数)指标进行模型的评估。如F1值：$\frac{2}{\frac{1}{P}+\frac{1}{R}}$重申：idea&lt;-&gt;code&lt;-&gt;experiment 1.4 满足和优化指标当有多个指标去衡量目标的话，可以选择一个作为优化指标(optimizing metric)，其它的设置为满足指标(satisficing metric) 如猫学习器准确率作为optimizing metric，识别速度小于100ms作为satisficing metric 如语音唤醒设备，准确率作为optimizing metric，24小时内只能一次假阳性唤醒作为satisficing metric 1.5 训练/开发/测试集的划分开发集/保留交叉验证集 和 测试集 应保持数据同分布新收集的数据也应同时分布在开发集和测试集上 1.6 开发集和测试集的大小传统的8:2或6:2:2方式不再适合大数据下的当代深度学习对训练数据量的要求非常大，因此，当数据大于百万时候，使用98:1:1是没有问题的。一般强调设置 训练集、开发集、测试集有时候忽略测试集不推荐 1.7 什么时间开始改变开发/测试集和指标如果无法正确评估好算法的排名，则需要定义一个新的评估指标。 算法a在识别猫图时可能会推送成人图片，则需要修改error指标： Error: \frac{1}{m_{dev}} \sum_{i=1}^{m_{dev}}L({\hat y}^{(i)},y^{(i)}) \tag{1}Error: \frac{1}{\sum \omega^{(i)}} \sum_{i=1}^{M_{dev}}\omega^{(i)}L({\hat y}^{(i)},y^{(i)}) \tag{2}\omega^{(i)}= \begin{cases} 1& \text{if} \ x^{i}\ \text{is non-porn}\\\\ 10& \text{if} \ x^{i}\ \text{is porn} \end{cases} 在当前开发集和测试集表现很好，但在实际应用中表现不好时，需要修改指标换切换数据集。 1.8 为什么是人的表现贝叶斯最优误差(Bayes optimal error)是理论上可能达到的最优误差，如果你的模型比人类水平低，说明可以使用某些工具来提高性能。模型效果低于人类水平，你可以 人为标注更多类别数据 人工误差分析：为什么人类做的对？ 更好的偏差/方差分析 1.9 可避免偏差将人类水平看作是贝叶斯误差，贝叶斯误差和训练集的错误率记为可避免偏差(Avoidable bias)， 当可避免偏差过大，要先降低训练集的错误率。如情况A 当可避免偏差很小，解决方差。如情况B 1.10 理解人的表现Human-level error定义:替代贝叶斯误差。 一般人对放射科的识别误差为3% 一般医生的误差为1% 专家医生的误差为0.7% 专家团队的误差为0.5% 可以得知 bayes error &lt;= 0.5%比较Avoidable bias 和 Variance大小可以分析应解决偏差还是方差问题。 1.11 超过人的表现 Scenario Human-level training set error development error A 0.5% 0.6% 0.8% B 0.5% 0.3% 0.4% 在情况A下Avoidable Bias为0.1，Variance为0.2，故应该着重降低方差。但在情况B下，模型超过人类，很难用现有的工具和方法去判断应降低偏差还是方差。在结构化数据中，机器学习算法很容易超越人的表现。但在自然感知领域，人类比较擅长，但机器学习能够一定程度上超过人类。 1.12 改善你模型的表现 减小可避免偏差 更大的模型 训练更久，优化算法(momentum, RMS prop, Adam, activation function) 更换网络结构(RNN,CNN)，超参数搜索 减小方差 更多数据 正则化(L2, Dropout, data augmentation) 更换网络结构(RNN,CNN)，超参数搜索 第一周作业选择题 可参考网上相关资料 第二周2.1 进行误差分析误差分析：通过人工检查分析错误样本点，来进行下一步的优化。 一个猫分类器错误地将狗识别成了猫，目前误差是$10\%$是否应该专门去处理狗？误差分析： 随机抽取开发集中100个错误样本 统计狗的数量结果： $5\%$是狗，则花费很多时间处理狗，误差会从$10\%$降低到$9.5\%$，即性能提升的Ceiling为$9.5\%$。 $50\%$是狗，那么误差会从$10\%$降低到$5\%$，值得尝试。 多个想法的并行评估猫分类器的优化idea: 修复狗识别成猫问题 修复狮子等大猫被错误分类 更好地图片模糊处理 Image Dog Great Cats Blurry 分析中新的类别 Comments 1 ✔ 比特犬 2 ✔ 3 ✔ ✔ 动物园下雨 … … … … %of total 8% 43% 61% 12% - 然后通过人为分析，比如解决Great Cats问题，或同时解决图像模糊问题。这个统计步骤大概需要几小时，但是值得。 2.2 清除标注错误的数据深度学习对随机误差(少量的数据标记出错)鲁棒性(robust)，但对系统误差没有。在误差分析的时候添加一列：数据标记类别出错，进行分析。是否进行样本类别修正? 如果这些标记错误严重影响了模型在开发集的评估能力，需要修复。 没有，则不需要花费宝贵时间修正。 第一种情况下，其它错误占9.4%远大于标记错误问题，所以应多处理其它错误。第二种情况下，标记错误类别比较大，需要处理。 修正开发集/测试集错误样本 在开发集，测试集上使用相同的处理，保证数据同分布 同时检查算法中正确和错位的样例 训练集和开发测试集处于轻微的不同分布。之前提到训练集对随机误差有鲁棒性，且训练集比开发测试集大很多，不做标记修正也是可以的，之后细谈这个问题。 深度学习不仅是使用模型，还更多在模型优化，调参，分析误差上。 2.3 快速搭建你的第一个系统，并进行迭代快速建立原型系统，然后进行迭代。 快速设立开发测试集和评估指标 建立机器学习系统原型 使用偏差/方差分析和误差分析决定下一步 如果研究的问题有大量的文献参考资料，可以先了解然后一开始就建立比较复杂的模型 2.4 在不同的划分上进行训练和测试训练集和开发测试集要保持同分布不一定完全对 一、 想要训练识别APP用户上传的猫图分类器，有200K网页抓取猫图(清晰)数据，和10KApp用户上传数据(真实应用，模糊不专业)。这种情况下去做数据的划分有两种选择： 将两个数据集合并打乱，划分如下。虽然保证了数据的同分布，但在开发集2.5K数据中，仅有119张是App数据，靶心更多地指向了网页抓取猫图，因此，这种方式是不可行地。 | 训练集 | 开发集 | 测试集 | | 205k | 2.5k | 2.5k | 将App的10K数据分为2.5k,2,5k,5k。两个2.5k分别作为开发集和测试集，5k和网页抓取200K数据一起作为训练集，虽然训练集和开发测试集数据分布不同，但靶心是正确的。 二、想要建立一个汽车后视镜语音识别模型，有买的语音识别数据500k，和与后视镜交互的真实应用数据20k数据集的划分应该为，将真实数据中的5K分别做开发集和测试集，剩余的进行训练集 2.5 不匹配数据划分的偏差和方差偏差和方差分析可以帮助你进行下一步的优化，训练集和开发测试集的数据分布不同时候，偏差和方差分析方法需要调整。需要设立训练-开发集(train-dev set)进行分析。训练-开发集(train-dev set)：和训练集同分布，但没有用于训练。 误差分析情况示例： - Scenario A Scenario B Scenario C Scenario D Human-error - - 0% - Train error 1% 1% 10% 10% Train-dev error 9% 1.5% 11% 11% Dev error 10% 10% 12% 20% Problem Variance data mismatch Bias Bias &amp; data mismatch avoidable error:Human-error - Train errorvariance:train error - train-dev errordata mismatch: train-dev error - dev errordegree of overfit:dev error - test error 考虑下面的情况： 编号 - - 与上一行差值 1 Human-error 4% - 2 Train error 7% avoidable error 3 Train-dev error 10% variance 4 Dev error 6% data mismatch 5 Test error 6% degree of overfit 2,3是根据训练集分布评估的，4,5是根据开发集分布评估的，需要一个更通用的分析，以汽车后视镜语音为例： - General speech recognition Reaview mirror speech data Human level human level 4% 6% Error on examples on trained Traing error 7% 6% Error on examples on not trained Training-dev error 10% Dev/Test error 6% 首先分析红色四个数据，能给出优化方向把表格中右上两个数字得到，左右对比分析很有用最后一行说明实际任务对比训练任务更简单，但一行差值又表明不是那么简单data mismatch没有很系统的解决方法，但有一些尝试建议 原图如下： 2.6 定位数据不匹配数据不匹配问题进行误差分析，1）尝试了解两个数据分布的不同之处，2）收集等多的像开发集一样的数据，如通过人工合成数据。使用人工合成数据要警惕：从所有可能性的空间只选取了很好一部分去模拟数据，从而导致算法对这一部门过拟合。 以汽车后视镜语音系统为例子可以在清晰的数据集上人为与汽车噪声数据合成，但注意，汽车噪声不应该仅仅是一个小时，而应该是很大的，近似于清晰数据集。不然很容易对这一小时的汽车噪声过拟合。 2.7 迁移学习迁移学习的应用场景： 当你想从任务A并迁移一些知识到任务B，当任务A和任务B都有同样的输入X时，迁移学习时有意义的。 当你拥有任务A的数据大于任务B时候 任务A的低层特征对任务B是有效的 迁移学习将神经网络最后一层或多层进行重新初始化和训练，迁移学习又叫预训练。 2.8 多任务学习多任务学习使用频率低于迁移学习多任务学习比单任务学习效果要好，如果不好说明网络不够深多任务学习在计算机视觉，目标检测有很广应用。多任务学习最后一层不再使用softamx，单一样本的类别不再是onehot向量表示，$y$成为各个类别的0，1表示。损失函数：$Loss=\underset{(4,1)}{\hat{y}^{(i)}}$变为：$\frac{1}{m}\sum_i^m\sum_j^4L(\hat{y}_j^{(i)}, y_j^{(i)})$ 2.9 什么是端对端的深度学习以前的数据处理系统或学习系统，需要多个阶段的处理，end to end learning则是忽略所有这些不同的阶段，用单个神经网络来代替它。但端对端学习并不使用所有场景： 百度的人脸门禁系统，将其拆分成1.判断是否有人脸并进行图片切割2.判断人物身份 机器翻译，使用端对端学习相比传统的一步步收取特征再转换有效，且有大量的数据集 根据手骨判断儿童年龄，数据量太小。无法支撑端对端学习，而传统的切割手骨判断长度再判断年龄比较合适 2.10 是否使用端对端学习优势： 让数据说话(黑盒)。不要强迫机器学习C A T，以音位为单位去学习进行语音识别。(联想到用语法做文本分析，结果一沓糊涂) 降低手工设计的组件需要 劣势： 需要非常大量的数据 排除了可能有用的手工设计组件，精心设计的组件可能很有用，也可能限制学习器的学习思维。 使用端对端(end to end learning)的建议：关键：是否拥有足够的数据来学习从x到y足够复杂(complexity needed)的函数。无人驾驶技术，并不是简单的端对端学习：而是把DL用来学习构成部分组件。 深度学习做预测，决策是更上层的目标。-沈华伟 参考深度学习笔记-黄海广]]></content>
      <categories>
        <category>深度学习[吴恩达]</category>
      </categories>
      <tags>
        <tag>Andrew Ng</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-改善深层神经网络]]></title>
    <url>%2F2018%2F10%2F24%2F02-%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[02.改善深层神经网络第一周 深度学习的实用层面1-1 训练/开发/测试集以前一般使用7:3划分训练集测试集或6:2:2 。大数据下通常需要用验证集快速验证多种算法的优劣，所以可以去小一些。百万数据集98：1：1%确保验证集合测试集的数据来自同一分布 1-2 偏差/方差欠拟合：偏差过高 过拟合：方差过高高方差：训练集和验证集的错误率相差过大高偏差：与人类等实际分类效果对比太差如果可以确定最优方差 然后去和偏差方差作比较来评价模型 1-3 机器学习基础High bias? Bigger network Train longer NN architecture search (可选)拟合训练集后，再看 High variance? More data regularization NN architecture search (可选) 1-4 正则化L1 正则化 $\omega$稀疏，但没有压缩模型减少内存L2 正则化又称权重衰减(Weight Decay) 公式可见权重在减小 1-5 为什么正则化可以减少过拟合L2 正则化防止过拟合 直观经验是权重越多接近于0 模型越简单(存疑)以Sigmoid为例 当权重过小时候，Sigmoid是接近线性的，整个网络是线性的从而使模型变为简单来减少过拟合 1-6 Dropout(Inverted dropout)d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_proba3 /= keep_prob以Hinton论文中提出的Dropout在测试集上是将概率$p$应用到神经元的输出边上为了简化测试集的操作，在训练时候每个神经元的输出都降低为权重$1/p$ 这样就不需要在测试集做额外操作 BN与Dropout的冲突解决方式：1. 先使用BN后使用Dropout, 更甚仅在softmax前一层使用 2.使用Uout 1-7 理解DropoutDropout 有效性体现： 等效正则化，使网络更简单 神经元不在完全依赖特定的输入神经元 可以设置不同层$\omega$的dropout rate 设置为1表示不使用dropoutDropout主要同于计算机视觉(CV)，当过拟合的时候可以在使用缺点： 损失函数的定义不对明确 调试困难(先固定为1， J递减再开启) 1-8 其他正则化技术 Data Augmentation 数据增强 如 图片旋转、裁剪、扭曲 early stooping have a mid-size rate $||W||_{2F}$通常的训练步骤： 优化代价函数J(GD/ Momentum/ RMRprop /Adam) 避免过拟合(正则化) | 减少方差(正交化)early stooping 不能独立处理问题1 2 1-9 正则化输入 Normalization training sets0均值 1方差当特征范围不一致时候使用归一化，加速$J$下降 1-10 梯度消失和爆炸1-11 神经网络的权重初始化$z = W_1x_1+W_2x_2+…+W_nx_n$设置$var(Wi) = 1/n W^{[l]} $=np.random.rand(shape)*np.sqrt(1/n[l-1])最后一项tanh 使用Xavier 初始化，ReLU使用公式2 Xavier 初始化，公式： $\sqrt{\frac{1}{n^{[l-1]}}}$公式： $\sqrt{\frac{2}{n^{[l-1]}+n^{[i]}}}$ 此超参数的调节优先等级比较低 1-12 梯度的数值逼近双边误差法：$f(\theta) = \frac{f(\theta+\epsilon) - f(\theta-\epsilon)}{2\epsilon}$ 1-13 梯度检验$J(\theta) -&gt; d\theta$$10^{-7}$ 效果great $10^-3$效果wrong 1-14 梯度检验经验 Dont use in training - only to debug If algorithm fails grad check, look at components to try to identify bug Rember regularization Doesn’t work with dropout Run ai random initialization;perhaps again after some training 第一周第一次作业权重初始化作用： 加速梯度收敛 增加泛化能力 权重初始化对比 0 随机 HE三种方式结果逐渐变好 其中He Initialization (Xavier Initialization变种 参考) 权重为0 $cost$不下降模型无效权重过大，没有效果 第一周第二次作业正则化技术很有效 第一周第三次作业梯度检验： difference = \frac {\| grad - gradapprox \|_2}{\| grad \|_2 + \| gradapprox \|_2 } \tag{3}第二周 优化算法2.1 Mini-batch梯度下降法默认梯度下降是将所有数据进行计算实现一部梯度下降分批次 : $X = {X^{\{1\}}, X^{\{1\}}, …, X^{\{m\}} }$同样：$Y = \{Y^{\{1\}}, Y^{\{1\}}, …, Y^{\{m\}} \}$mini batch $t:X^{\{t\}}, Y^{\{t\}}$ $x^{(i)}$表示训练集中第$i$个训练样本$z^{[l]}$表示神经网络的层数$X^{\{i\}} \ Y^{\{i\}}$表示不同的mini batch $X$维度$(n_x, \text{batch size})$ 1234567891011121314# 遍历批次 每批次使用向量化for t=1,....5000:&#123; # 使用X^&#123;t&#125; Y^&#123;t&#125; 实现 一步 梯度下降 Forward prop on X^&#123;t&#125; Z^[1] = W^[1]X^&#123;t&#125; + b^[1] A^[1] = g[1](Z^[1]) . . . A^[l] = g^[l](Z^[l]) Compute cost J^&#123;t&#125; Back prop update W,B&#125; 2.2 理解mini-batch梯度下降法损失表现mini-batch 的成本函数是震荡下降的 选择mini-batch sizeTips: 将mini-batch size 设置为m : Batch Gradient Descend 将mini-batch size 设置为1 : stochastic gradient descent(随机梯度下降) batch gradient descend的cost轨迹，相对噪声低，幅度大stochastic gradient descent的cost轨迹，噪声大，徘徊在最小值 缺点：失去了向量化的加速处理 一般使用合适batch size的mini-batch 其优点： 向量化 不全部使用数据集 size 选取： 样本集小直接使用batch gradient descent ($m\leqslant 2000$) mini-batch size 一般为 [64，128，256，512] 一般为2的次方 保证$X^{\{t\}}, Y^{\{t\}}$ 与CPU/GPU内存相适应 2.3 指数加权平均指数加权平均(exponentially weighted averages)又称指数加权移动平均(exponentially weight moving averages)以温度为例,$V0 = 0 \\V_1 = 0.9 \cdot V_0 + 0.1 \cdot \theta_1 \\V_2 = 0.9 \cdot V_1 + 0.1 \cdot \theta_2 \\V_3 = 0.9 \cdot V_2 + 0.1 \cdot \theta_3 \\… \\V_t = 0.9 \cdot V{t-1} + 0.1 \cdot \theta_t$效果如图： $Vt = \beta V{t-1} + (1-\beta) \theta_t$当 $\beta = 0.9 ：\approx \frac{1}{1-\beta} = 10\ (dayTemperature)$当 $\beta = 0.98 ：\approx \frac{1}{1-\beta} = 50\ (dayTemperature)$当 $\beta = 0.5 ：\approx \frac{1}{1-\beta} = 2\ (dayTemperature)$对比效果图如下： 红色0.9(10天平均温度) 绿色0.98(50天平均温度) 黄色0.5(2天平均温度)平均天数越大图像越平滑，越偏移平均天数越小噪声越大 选定参数对模型有一定的影响 2.4 理解指数加权平均 $vt = \beta v{t-1}+(1-\beta)\theta_t$ $V{100} = 0.9v{99}+0.1\theta{100} \\V{99} = 0.9v{98}+0.1\theta{99} \\V{98} = 0.9v{97}+0.1\theta{98} \\… \\V{100}=0.1\cdot\theta{100}+0.1\cdot0.9\theta{99}+0.1\cdot(0.9)^2\theta{98}+0.1\cdot (0.9)^3\theta{97}…$ 0.9^{10} \approx 0.35 \approx \frac{1}{e} \label{1}(1-\epsilon)^{\frac{1}{\epsilon}} \approx \frac{1}{e}即，经过10天后，权重下降到原来的三分之一当$\beta = 0.98$时，需要指数50能达到$\frac{1}{\epsilon}$，即可以看作是平均了50天温度可以使用\frac{1}{1-\beta}来表示平均了多少数据(非正式数学证明) 2.5 指数加权平均的偏差修正(bias correction)当$t_0 = 0$时，$V_1 = \beta V_0+(1-\beta )\theta_1 = (1-\beta )\theta_1$，会发现前期数据值比较小 。 紫线是实际图形，对比绿线发现，前期数值偏小，后期重合 偏差修正$\frac{V_t}{1-\beta^t}$ 用来解决前期偏差过大问题，随着$t$的增加，偏差修正无效。在机器学习中，大家都不在乎使用偏差修正，大部分人喜欢熬过前期。如果关心初期的偏差，就需要使用它。 2.6 动量梯度下降法MomentumMomemtum 在梯度下降中，我们希望朝最优解进向横向速度越大，纵向震荡越小越好。 Momemtum对于优化碗形状损失函数比较适用 Implementation detailsOn iteration t: compute $dW$,$db$, on the current mini-batch $v{dW} = \beta v{dW}+(1-\beta)dW \\ v{db} = \beta v{dWb}+(1-\beta)db \\ W = W -\alpha v{dW}, b=b-\alpha v{db}$ Hyperparameters:$\alpha ,\beta$ $\beta = 0.9$(平均十次数据) 不使用偏差修正 $dW$ $db$ 可以理解为加速度$v{dW}, v{db}$ 可以理解为速度前项$\beta &lt; 1$可以理解为摩擦力 Tip: 有一些资料中删除了后项的$(1-\beta)$参数， 即$v{dW} = \beta v{dW}+dW$得到的$v_{dW}$缩小了$(1-\beta)$倍这要求学习率$\alpha$要与$(1-\beta)$相应变化，影响了$\alpha$的最优值此外，如果调整$\beta$ 则需要对应调整$\alpha$ 所以不选择此实现方式 2.7 RMSpropRMSprop(root mean square prop)可以消除梯度下降算法中的摆动，加速梯度下降，可以使用更大学习率我们希望在 $\text{w}$ 更新更快，在 $\text{b}$ 减小震荡从一个蓝色箭头中可以看出$dw$比较大，$db$比较小(即梯度)因此在更新时候加上相关项来处理 $\text{w}$ 和 $\text{b}b$ 表示高维中的参数 不仅代表w,b 实现： 在迭代$t$中计算 mini-batch 中的$dw$,$db$$S{dw} = \beta S{dw} + (1-\beta)(dw)^2$$S{db} = \beta S{db} +(1-\beta)(db)^2$$w = w-\alpha \frac{dw}{\sqrt{S{dw}}}$$b = b- \alpha \frac{db}{\sqrt{S{db}}}$$(dw)^2$ 变小 w更新越大 $(db)^2$ 变大 b更新越大特别的为了防止 $\sqrt{S{dw}}$ 过小，会添加一项 $\epsilon$ : $w = w-\alpha \frac{dw}{\sqrt{S{dw}+\epsilon}}$ $b = b- \alpha \frac{db}{\sqrt{S_{db}+\epsilon}}$ $\epsilon$ 一般取值 $10^{-8}$效果示意图为绿线 2.8 Adam 优化算法Adam优化算法是把Momentum和RMSprop算法结合在一起算法流程： $V{dw} = 0, S{dw} = 0, V{db} = 0, S{db} = 0$在迭代$t$：使用mini-batch 计算 $dw$, $db$$V{dw} = \beta_1V{dw}+(1-\beta1)dw, V{db} = \beta1v{db}+(1-\beta1)db $ &lt;- “momentum” $\beta_1$$S{dw} = \beta2S{dw}+(1-\beta2)(dw)^2, S{db} = \beta2S{db}+(1-\beta2)(db)^2 $ &lt;- “RMSprop” $\beta_2$# 要进行偏差修正$V{dw}^{corrected} = V{dw}/(1-\beta_1^t), V{db}^{corrected} = V{db}/(1-\beta_1^t)$$S{dw}^{corrected} = S{dw}/(1-\beta_2^t), S{db}^{corrected} = S{db}/(1-\beta_2^t)$# 更新系数$W=W-\alpha \cdot \frac{V{dw}^{corrected}}{\sqrt{S{dw}^{corrected}+\epsilon}}, b = b-\alpha\cdot\frac{V{db}^{corrected}}{\sqrt{S_{dw}^{corrected}+\epsilon}}$ 超参数的选择$\alpha : 需要调整$$\beta_1:0.9 -&gt;(dw)$$\beta_2:0.999 -&gt;(dw^2)$$\epsilon:10^{-8}$一般使用默认就好，不需要特别修改Adam : Adaption momentum Estimator 2.9 学习率衰减epoch:完整的使用一次全部数据集 $\alpha = \frac{1}{1+decay_rate*epoch_num}$ $\alpha = 0.95^{epoch_num}\cdot \alpha_0$ $\alpha = \frac{k}{\sqrt{epoch_num}}\cdot \alpha_0$ 离散化衰减 手动衰减 2.10 局部最优的问题鞍点(saddle point)问题：在各维度导数为0，但部分维度是凸函数顶点 unlikely to get stuck in a bad local optima(不太可能陷入局部最优的最佳点) Plateaus can make learning slow(平滑区域导致学习变慢，Adam能加速学习逃离) 第二周作业 Momentum 随机梯度下降和批量梯度下降的区别在于，在一次计算梯度时候所使用的训练集样本数量(The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.) 必须调整学习率(You have to tune a learning rate hyperparameter $\alpha$ .) 参数设置很好的批量梯度下降算法，比梯度下降和随机梯度下降算法要高效，特别在数据集大情况下(With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).) Adam \begin{cases} v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \\\\ v_{W^{[l]}}^{corrected} = \frac{v_{W^{[l]}}}{1 - (\beta_1)^t} \\\\ s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \\\\ s_{W^{[l]}}^{corrected} = \frac{s_{W^{[l]}}}{1 - (\beta_2)^t} \\\\ W^{[l]} = W^{[l]} - \alpha \frac{v_{W^{[l]}}^{corrected}}{\sqrt{s^{corrected}_{W^{[l]}}}+\varepsilon} \end{cases} 适合mini-batch 和 momentum 稍微调整超参数就能很有效($\alpha$除外) 第三周 超参数调试、Batch正则化和程序框架3.1 调试处理系统超参数调试技巧 $\alpha$ 第一级别重要$\beta$(默认0.9) mini-batch size 和 #hidden units 第二级别重要learning rate decay 和 #layers 第三级别重要$\beta_1(0.9),\beta_2(0.999), \epsilon(10^{-8})$基本不用修改 超参数搜索原则：先随机取值再精确搜索 随机取值:不是网格取值 精确搜索:缩小范围 3.2 为超参数选择合适的范围对参数选择良好的尺度(scale)，不是使用区间随机取值，而是将区间通过指数变换成整数值来随机抽取。如学习率$\alpha$范围为[0.0001, 1],直接随机抽取，搜索资源90%会落在[0.1,1]12r = -4 * np.random.rand() #[-4, 0]alpha = 10 ** r #[10^-4, 10^0] 3.3 超参数选取实战：Pandas vs. Caviar Pandas：在CPU/GPU资源较少的情况下，在一个模型上不同时间调整学习率，查看损失函数表现 Caviar：在计算资源充足条件下，并行跑多个模型，选择最好的模型参数 3.4 正则化网络的激活函数 Batch Normalization 吴恩达推荐的是在激活函数之前对$Z$进行归一化，与BN网络论文一致。参考论文：BN_PDFD和BN笔记BN与Dropout的冲突解决方式：1. 先使用BN后使用Dropout, 更甚仅在softmax前一层使用 2.使用Uout \mu = \frac{1}{m}\sum_i z^{(i)} \sigma^2 = \frac{1}{m}\sum_i(z^{(i)}-\mu)^2 z_{norm}^2=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}} \bar{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta $\gamma$和$\beta$是为了防止归一化限制输入特征的的表现。 当$\gamma = \sqrt{\sigma^2+\epsilon}$，$\beta=\mu$，只是一个恒等函数。 3.5 将Batch Normalization 拟合进神经网络 BN应用在每一个神经元激活函数之前的计算结果$z$，同时BN使参数偏置项$b$无效 [Appendix] 神经网络训练步骤 BN与Dropout的冲突解决方式：1. 先使用BN后使用Dropout, 更甚仅在softmax前一层使用 2.使用Uout1234567891011121314151617181920212223242526272829303132333435363738split x_train,y_train,x_test,y_testset hyper parameters: batch_size learning_rate learning_rate_decay beta1,beta2 keep_probeinit parameters(He Initialization): w, gamma,beta #？存疑for e in epoch: for i in iteration: # 1. forward propagation # 1.1 compute Z (dw)(db deprecated by BN) Z = W * a # 1.2 BN (dgamma,dbeta) Z = BN(Z) # 1.3 activation function a = g(Z) # ... iterate L neural layer # gradient parameters: dw,dgamma,dbeta # 1.4 droupout (applied on last hidden layer) a = a * d a /= keep_prob # 2. back paopagation dw, db, dbeta = computeGradient(Z) # 2.1 momentum vdw = beta1 * vdw + (1-beta1) * dw vdw_corrected = vdw/(1-beta1 ** i) # same as other gradient parameters # 2.2 RMSprop sdw = beta2 * sdw + (1-beta2) * dw ** 2 sdw_corrected = sdw/(1-beta2 ** i) # 2.3 update parameters w = w - learning_rate * ( vdw_corrected / math.sqrt(sdw_corrected + episilon) ) # same as other gradient parameters 3.6 Batch Norm 为什么奏效 归一化使输入均值为0，方差为1，能够加速学习。 固定了各神经层间的输入分布变化，因而使神经元能更好地学习 BN的额外功能：正则化 数据被缩放 添加噪声，使神经元不再特定依赖 轻微的正则化效果 3.7 测试时的 Batch Norm和BN论文说明不太相同。在训练BN网络地时候，训练数据集分为$X ^{\{1\}}$,$X^{\{2\}}$,$X^{\{3\}}$…$X^{\{m\}}$对于第一层地一个神经元可以得到$X^{\{1\}[1]}$,$X^{\{2\}[1]}$,$X^{\{3\}[1]}$……每一层的每一个神经元，都会得到$m$个$\mu$和$\sigma$以第l层第n个神经元为例，将其$m$个$\mu$使用指数加权平均方法进行处理$vi = \beta v{i-1} + (1-\beta)\mu^{\{i\}}$在测试集的时候BN仅仅是一个线性变化，利用指数加权平均计算各层神经元的$\mu$和$\sigma$参数 3.8 Softmax 回归 y(a^{[L]})= \frac{e^{z^{[L]}}}{\sum z^{[L]}}Softmax 用来归一化概率Softmax 激活函数的特殊：输入为向量，输出为向量，其它激活函数输入为单值无隐层的Softmax网络可以用于多分类，学习到的决策边界都是线性的。 3.9 训练一个Softmax分类器softmax是和hardmax相对应的。(hardmax形式[0,1,0,0]) Softmax regression generalizes logistic regression to C classes Softmax是logistic的多分类应用。Loss Function: L(\hat{y},y)=-\sum_{j=1}^C y_j log\hat{y}_j如$C=4$，$y=[0,1,0,0]$，$a^{[L]}=\hat{y}=[0.3,0.2,0.1,0.4]$,则: L(\hat{y},y)=0+y_2 \dot log\hat{y}_2+0+0=-log\hat{y}_2即损失函数定义是正确类别的概率越大(最大似然估计)训练集的代价函数J=(\omega^{[i]},b^{[i]}...)=\frac{1}{m}\sum_{i=1}^m L(\hat{y},y)实现梯度下降：\frac{\partial J}{\partial z^{[L]}} = dz^{[L]} = \hat{y}-y 深度学习框架选择指标： 易于编程 运行速度 真正开源 部分深度学习框架： Caffe/Caffe2 PyTorch Keras Tensorflow Theano mxnet 参考深度学习笔记-黄海广]]></content>
      <categories>
        <category>深度学习[吴恩达]</category>
      </categories>
      <tags>
        <tag>Andrew Ng</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Blog 教程]]></title>
    <url>%2F2018%2F10%2F24%2FHexoj%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[安装及部署初始化1$ hexo init [folder] 新建文章1$ hexo new [layout] &lt;title&gt; 如果没有设置layout，默认使用config.yml中的defaultlayout参数代替。如果标题包含空格，请使用引号括起来。 生成静态网页1$ hexo generate 可以简写成1$ hexo g 启动服务1$ hexo server 启动服务器。默认情况下，访问网址为： http://localhost:4000/。 部署首先，修改站点位置文件,文件位于项目根路径下_config.yml文件，1234deploy: type: git repo: "仓库路径" branch: master 这里注意:后一定要有一个空格，否则部署无反应。 然后，安装Hexo-git1$ npm install hexo-deployer-git --save 执行部署命令：1$ hexo deploy 该命令请在git中执行。 部署网站，可以简写为1$ hexo d 发布部署说明 hexo 分支:hexo笔记源代码 master 分支:hexo笔记访问分支 jekyll分支:之前博客文章备份源码文件夹一直处于hexo分支，直接修改博客，然后修改。源文件直接commit到hexo分支。部署直接git cmd使用123hexo cleanhexo ghexo d hexo已追踪文件及文件夹: .gitignore _config.yml source/ theme/next/_config.yml scaffolds/ 个性化配置更换主题 下载主题： 1$ git clone https://github.com/theme-next/hexo-theme-next themes/next 配置主题：修改站点默认主题 1theme: next 可以选择修改样式.打开主题配置文件(next/_config.yml)，选择以下即可：1234#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini 网站信息打开站点配置文件_config.yml，修改对应文字即可：1234567# Sitetitle: Hexosubtitle:description:author: John Doelanguage:timezone: Next的使用以后再调整 文章标签首先创建tag页面：1$ hexo new page tags 修改source/tags/index.md文件，添加type: &quot;tags&quot;：12345---title: tagsdate: 2018-10-24 21:25:58type: &quot;tags&quot;--- 修改模板文件scaffolds/post.md,添加一行’tags:’:1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:description:--- 给文章添加tags,在文章开头填写tags，格式如下：123tags: - Hexo - Github Page 文章分类同创建标签步骤基本一致，首先创建分类页面：1$ hexo new page categories 然后修改source/tags/index.md文件，添加type: &quot;tags&quot;：12345---title: tagsdate: 2018-10-24 21:25:58type: &quot;categories&quot;--- 最后在文章添加分类：123456789---title: Hexo教程date: 2018-10-24 18:14:18tags: - Hexo - Github Pagecategories: otherdescription: Hexo建站流程说明--- 阅读统计阅读次数统计（LeanCloud） 由 Doublemine 贡献请查看为NexT主题添加文章阅读量统计功能 文章搜索Hexo NexT 6.0+版本Algolia教程1.登陆注册Algolia,创建Index。详细步骤参考6.0以下或教程。2.安装1$ npm install --save hexo-algolia 3.修改站点配置文件: 12345algolia: applicationID: &apos;Application ID&apos; apiKey: &apos;Search-only API key&apos; indexName: &apos;indexName&apos; chunkSize: 5000 4.创建环境变量，Win下可直接手动创建。1234$ export HEXO_ALGOLIA_INDEXING_KEY=Search-Only API key # Use Git Bash# set HEXO_ALGOLIA_INDEXING_KEY=Search-Only API key # Use Windows command line$ hexo clean$ hexo algolia 完成后执行hexo algolia,得到结果：12345INFO [Algolia] Testing HEXO_ALGOLIA_INDEXING_KEY permissions.INFO Start processingINFO [Algolia] Identified 9 pages and posts to index.INFO [Algolia] Indexing chunk 1 of 1 (50 items each)INFO [Algolia] Indexing done. 5.安装依赖包。(可以选择修改CDN，参考教程)12$ cd themes/next$ git clone https://github.com/theme-next/theme-next-algolia-instant-search source/lib/algolia-instant-search 6.修改站点位置文件，启用搜索123456789# Algolia Searchalgolia_search: enable: true hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: "We didn't find any results for the search: $&#123;query&#125;" hits_stats: "$&#123;hits&#125; results found in $&#123;time&#125; ms" 7.配置URL修改站点配置文件，将url设置为/(或域名),防止出现搜索结果跳转链接域名为http;//yoursite:1234567# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;#url: http://yoursite.comurl: /root: /permalink: :year/:month/:day/:title/permalink_defaults: Hexo NexT 6.0以下版本1.安装hexo-generator-searchdb：1$ npm install hexo-generator-searchdb --save 2.编辑站点配置文件,新增以下内容到任意位置：12345search: path: search.xml field: post format: html limit: 10000 3.编辑主题配置文件，启用本地搜索及Algolia功能：1234567local_search: enable: true...algolia_search: enable: true 4.启用Algolia：4.1创建APIKeyHEXO_ALGOLIA_INDEXING_KEY 进入Algolia的API Keys页面ALL API KEYS选项卡 创建APIKey Description：HEXO_ALGOLIA_INDEXING_KEY Indices：&lt;此处选择之前创建的Index&gt; ACL：Add records，Delete records，List indices，Delete index4.2设置环境变量HEXO_ALGOLIA_INDEXING_KEY,可手动添加1$ export HEXO_ALGOLIA_INDEXING_KEY=&lt;此处为第1步创建的APIKey&gt; 4.3修改站点配置文件，添加以下内容：123456# Add manual - algolia:algolia: applicationID: &apos;你的Application ID&apos; apiKey: &apos;你的Search-Only API Key&apos; indexName: &apos;输入刚才创建index name&apos; chunkSize: 5000 官方教程中未添加apikey列，导致可能失败。参考Github上教程 4.4安装模块12$ cd themes/next$ git clone https://github.com/theme-next/theme-next-algolia-instant-search source/lib/algolia-instant-search 4.5配置URL修改站点配置文件，将url设置为/,防止出现搜索结果跳转链接域名为http;//yoursite:1234567# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;#url: http://yoursite.comurl: /root: /permalink: :year/:month/:day/:title/permalink_defaults: 4.6执行Algolia命令123456$ hexo algoliaINFO [Algolia] Testing HEXO_ALGOLIA_INDEXING_KEY permissions.INFO Start processingINFO [Algolia] Identified 9 pages and posts to index.INFO [Algolia] Indexing chunk 1 of 1 (50 items each)INFO [Algolia] Indexing done. 文章公式首先卸载默认渲染器，如遇到错误可忽略。 1$ npm un hexo-renderer-marked --save 然后安装新的渲染器 1$ npm i hexo-renderer-kramed --save 修改主题配置文件,开启公式支持12345math: enable: true ... engine: mathjax #engine: katex 最后注意，在文章顶部为当前文章开启渲染支持123456---title: 文章名称date: 2018-10-24 18:14:18mathjax: true...--- Hexo文章公式使用注意 先写下标，再写上标，否则可能无法编译。X_1^2-$X^2_1$-$X_1^2$ {}的转义不是\{\}，而是\\{\\}: $\{\}$ 公式换行\\转义为\\\\: \begin{cases} 1 \\\\ 2 \\\\ 3 \\\\ \end{cases} 表头前要有一行空白，否则编译失败 * 需要转义为\*：$*$ &lt;t&gt;需要转义&lt;t\&gt;: $x^{}$ 请注意：mathjax的编译，请选择不同cdn。123456789101112## 底部标签 ##将文章底部的标签由`#`改为&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 打开`/themes/next/layout/_macro/post.swig`，搜索`rel=&quot;tag&quot;&gt;#` 将`#`改为`&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;`## 谷歌收录 创建站点地图``` bash npm install hexo-generator-sitemap --save 在站点配置文件中添加以下内容： 12345# 自动生成sitemapsitemap: path: sitemap.xml #baidusitemap: #path: baidusitemap.xml 修改站点配置文件中的url： 1url: https://nocater.github.io 执行hexo d部署后，访问https://nocater.github.io/sitemap.xml，检查其中是否包含域名： 12345678&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt; &lt;url&gt; &lt;loc&gt; https://nocater.github.io/2018/10/24/Hexoj%E5%BB%BA%E7%AB%99%E6%95%99%E7%A8%8B/ &lt;/loc&gt; &lt;lastmod&gt;2018-10-26T06:46:34.000Z&lt;/lastmod&gt; &lt;/url&gt;&lt;url&gt; 验证通过后，就可以开始配置Google了： 在谷歌搜索引擎入口提交博客网址 选择文件验证，下载html文件后，在源码里添加123---layout: false--- 禁止Hexo的模板渲染。部署成功并访问成功后，就可以通过验证了。 在站点地图提交sitemap.xml，并查看状态。 百度收录提交网址与提交链接实现与Google类似。不过站点地图可选择： 主动推送 自动推送 可直接修改主题配置文件，将baidu_push: false设置为ture。 sitemap(Google中使用此方法) 参考$e=mc^2$Hexo官方文档NexT官方教程]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github Page</tag>
      </tags>
  </entry>
</search>
